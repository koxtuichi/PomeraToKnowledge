"""
blog_article_writer.py â€” ãƒãƒ¡ãƒ©è‰æ¡ˆ â†’ ãƒ–ãƒ­ã‚°è¨˜äº‹ç”Ÿæˆ

ãƒãƒ¡ãƒ©ã§æ›¸ã„ãŸãƒ¡ãƒ¢ã‚’ã‚‚ã¨ã«ã€Gemini APIã§èª­ã¿ã‚„ã™ã„ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ç”Ÿæˆã™ã‚‹ã€‚
ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰é–¢é€£ãƒ†ãƒ¼ãƒã‚’å¼•ç”¨ã—ã¦è¨˜äº‹ã«æ·±ã¿ã‚’æŒãŸã›ã‚‹ã€‚
ç”Ÿæˆå¾Œã€hatena_publisher.py ã‚’å‘¼ã³å‡ºã—ã¦ã¯ã¦ãªãƒ–ãƒ­ã‚°ã«ä¸‹æ›¸ãæŠ•ç¨¿ã™ã‚‹ã€‚
"""

import os
import json
import re
import argparse
import subprocess
from datetime import datetime
import requests
from typing import Dict, Any, Optional, Tuple

# è¨­å®š
API_KEY = os.getenv("GOOGLE_API_KEY")
MASTER_GRAPH_PATH = "knowledge_graph.jsonld"
BLOG_READY_DIR = "blog_ready"
HATENA_PUBLISHER_SCRIPT = "scripts/hatena_publisher.py"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ–ãƒ­ã‚°è¨˜äº‹ ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

BLOG_SYSTEM_PROMPT = """
# å½¹å‰²
ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ†ãƒƒã‚¯ãƒ–ãƒ­ã‚¬ãƒ¼ã§ã‚ã‚Šã€ã‚¨ãƒƒã‚»ã‚¤ã‚¹ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒãƒ¡ãƒ©ã§æ›¸ã„ãŸã€Œãƒ¡ãƒ¢ã€ã‚„ã€Œè€ƒãˆãŸã“ã¨ã€ã‚’ã‚‚ã¨ã«ã€
**èª­è€…ãŒæ¥½ã—ãèª­ã‚ã‚‹ãƒ–ãƒ­ã‚°è¨˜äº‹**ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚

# è¨˜äº‹ã®ã‚¹ã‚¿ã‚¤ãƒ«
- å€‹äººãƒ–ãƒ­ã‚°ã‚‰ã—ã„ã€è¦ªã—ã¿ã‚„ã™ã„èªã‚Šå£ã§æ›¸ã
- ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã§çµ±ä¸€ã™ã‚‹
- èª­è€…ã«èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ãªæ–‡ä½“ã«ã™ã‚‹
- å°‚é–€ç”¨èªã¯å™›ã¿ç •ã„ã¦èª¬æ˜ã™ã‚‹
- ä½“é¨“è«‡ã‚„å…·ä½“ä¾‹ã‚’äº¤ãˆã¦ã€èª¬å¾—åŠ›ã‚’æŒãŸã›ã‚‹
- é©åº¦ã«ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚’ç¹”ã‚Šäº¤ãœã‚‹
- èª­ã¿çµ‚ã‚ã£ãŸã‚ã¨ã«ã€Œãªã‚‹ã»ã©ã€ã¨æ€ãˆã‚‹å†…å®¹ã«ã™ã‚‹

# è¨˜äº‹ã®æ§‹æˆ
ä»¥ä¸‹ã®æ§‹æˆã§è¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„:

1. **å°å…¥**: èª­è€…ã®èˆˆå‘³ã‚’å¼•ããƒ•ãƒƒã‚¯ã‹ã‚‰å§‹ã‚ã‚‹ã€‚æ—¥å¸¸ã®é¢¨æ™¯ã‚„ç–‘å•ã‹ã‚‰å…¥ã‚‹ã¨è‰¯ã„ã€‚
2. **æœ¬è«–**: ãƒ¡ãƒ¢ã®æ ¸å¿ƒã‚’è†¨ã‚‰ã¾ã›ã‚‹ã€‚ä½“é¨“â†’è€ƒå¯Ÿâ†’ç™ºè¦‹ã®æµã‚Œã§å±•é–‹ã™ã‚‹ã€‚
   - è¦‹å‡ºã—ã‚’2ã€œ4å€‹ã¤ã‘ã¦èª­ã¿ã‚„ã™ãåŒºåˆ‡ã‚‹
   - å…·ä½“çš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨æŠ½è±¡çš„ãªè€ƒå¯Ÿã‚’äº¤äº’ã«é…ç½®ã™ã‚‹
   - ã€Œã€‡ã€‡ã ã¨æ€ã£ã¦ã„ãŸã€‚ã§ã‚‚å®Ÿã¯â–³â–³ã ã£ãŸã€ã®ã‚ˆã†ãªæ°—ã¥ãã®æ§‹é€ ã‚’å…¥ã‚Œã‚‹
3. **ã¾ã¨ã‚**: èª­è€…ã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ç· ã‚ã‚‹ã€‚æŠ¼ã—ã¤ã‘ãŒã¾ã—ããªãã€ä½™éŸ»ã‚’æ®‹ã™ã€‚

# çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«
1. ãƒ¡ãƒ¢ã®å†…å®¹ã‚’ãã®ã¾ã¾ã‚³ãƒ”ãƒšã—ãªã„ã€‚å¿…ãšè¨˜äº‹ã¨ã—ã¦å†æ§‹æˆã™ã‚‹
2. å€‹äººã‚’ç‰¹å®šã§ãã‚‹æƒ…å ±ã¯æ›¸ã‹ãªã„ã€‚å›ºæœ‰åè©ã¯å¿…è¦ã«å¿œã˜ã¦ä¸€èˆ¬åŒ–ã™ã‚‹
3. ã€Œä»¥ä¸Šã§ã™ã€ã€Œã„ã‹ãŒã§ã—ãŸã‹ï¼Ÿã€ã®ã‚ˆã†ãªå®šå‹æ–‡ã¯ä½¿ã‚ãªã„
4. SEOã‚’æ„è­˜ã—ã™ããŸä¸è‡ªç„¶ãªæ–‡ç« ã¯é¿ã‘ã‚‹
5. è¨˜äº‹ã®å†’é ­ã«ã‚¿ã‚¤ãƒˆãƒ«ã¯æ›¸ã‹ãªã„

# å“è³ªã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
- æœ€åˆã®3è¡Œã§èª­è€…ãŒç¶šãã‚’èª­ã¿ãŸããªã‚‹ã‹
- è¦‹å‡ºã—ã ã‘èª­ã‚“ã§ã‚‚è¨˜äº‹ã®æµã‚ŒãŒã‚ã‹ã‚‹ã‹
- ã€Œã ã‹ã‚‰ä½•ï¼Ÿã€ã¨æ€ã‚ã‚Œã‚‹ãƒã‚¤ãƒ³ãƒˆãŒãªã„ã‹
- èª­å¾Œã«ä½•ã‹ä¸€ã¤ã§ã‚‚æŒã¡å¸°ã‚Œã‚‹ã‚‚ã®ãŒã‚ã‚‹ã‹

# å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:

{
  "title": "ãƒ–ãƒ­ã‚°è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ30æ–‡å­—ä»¥å†…ã€èˆˆå‘³ã‚’å¼•ãã‚‚ã®ï¼‰",
  "body": "æœ¬æ–‡ï¼ˆã¯ã¦ãªãƒ–ãƒ­ã‚°ç”¨Markdownå½¢å¼ï¼‰",
  "description": "ãƒ¡ã‚¿ãƒ‡ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆ120æ–‡å­—ä»¥å†…ã®è¨˜äº‹æ¦‚è¦ï¼‰",
  "categories": ["ã‚«ãƒ†ã‚´ãƒª1", "ã‚«ãƒ†ã‚´ãƒª2"],
  "estimated_read_time": "â—‹åˆ†"
}

# é‡è¦ãªæ³¨æ„äº‹é …
- è¨˜äº‹ã®é•·ã•ã¯1500ã€œ3000æ–‡å­—ç¨‹åº¦ã€‚ã‚¹ãƒãƒ›ã§èª­ã¿åˆ‡ã‚Œã‚‹é•·ã•
- è¦‹å‡ºã—ã¯æœ¬æ–‡ä¸­ã«2ã€œ4å€‹ã€‚å¤šã™ããªã„
- ã¯ã¦ãªãƒ–ãƒ­ã‚°ã®Markdownå½¢å¼ã«å¾“ã†
- ã‚«ãƒ†ã‚´ãƒªã¯å†…å®¹ã«åˆã£ãŸã‚‚ã®ã‚’2ã€œ3å€‹ææ¡ˆã™ã‚‹

è¨€èª: æ—¥æœ¬èªã€‚
JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""

REVIEW_PROMPT = """
ã‚ãªãŸã¯ãƒ–ãƒ­ã‚°ã®ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’èª­ã¿ã€å“è³ªã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

## ãƒã‚§ãƒƒã‚¯é …ç›®
ä»¥ä¸‹ã®å„é …ç›®ã«ã¤ã„ã¦ã€å•é¡ŒãŒã‚ã‚Œã°å…·ä½“çš„ã«æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚

1. **èª­ã¿ã‚„ã™ã•**: æ–‡ç« ã®ãƒªã‚ºãƒ ã¯è‰¯ã„ã‹ã€‚ä¸€æ–‡ãŒé•·ã™ããªã„ã‹ã€‚
2. **æ§‹æˆ**: å°å…¥â†’æœ¬è«–â†’ã¾ã¨ã‚ã®æµã‚ŒãŒè‡ªç„¶ã‹ã€‚è¦‹å‡ºã—ã®é…ç½®ã¯é©åˆ‡ã‹ã€‚
3. **å…·ä½“æ€§**: æŠ½è±¡çš„ãªè©±ã°ã‹ã‚Šã§é€€å±ˆã«ãªã£ã¦ã„ãªã„ã‹ã€‚å…·ä½“ä¾‹ã¯ã‚ã‚‹ã‹ã€‚
4. **èª­è€…ç›®ç·š**: èª­è€…ãŒã€Œè‡ªåˆ†ã«ã‚‚é–¢ä¿‚ã‚ã‚‹ã€ã¨æ„Ÿã˜ã‚‰ã‚Œã‚‹ã‹ã€‚æŠ¼ã—ã¤ã‘ãŒã¾ã—ããªã„ã‹ã€‚
5. **ã‚¿ã‚¤ãƒˆãƒ«**: è¨˜äº‹ã®å†…å®¹ã‚’åæ˜ ã—ã¤ã¤ã€èˆˆå‘³ã‚’å¼•ãã‚¿ã‚¤ãƒˆãƒ«ã‹ã€‚

## å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:

{
  "passed": true/false,
  "issues": [
    {
      "type": "èª­ã¿ã‚„ã™ã•",
      "detail": "å…·ä½“çš„ãªå•é¡Œç®‡æ‰€ã¨ç†ç”±",
      "suggestion": "æ”¹å–„æ¡ˆ"
    }
  ]
}

å•é¡ŒãŒãªã‘ã‚Œã° passed ã‚’ true ã«ã—ã€issues ã¯ç©ºé…åˆ—ã«ã—ã¦ãã ã•ã„ã€‚
1ã¤ã§ã‚‚å•é¡ŒãŒã‚ã‚Œã° passed ã‚’ false ã«ã—ã¦ãã ã•ã„ã€‚
JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def call_gemini_api(prompt: str, max_retries: int = 3) -> str:
    """Gemini APIã‚’å‘¼ã³å‡ºã™ã€‚"""
    if not API_KEY:
        raise ValueError("GOOGLE_API_KEY is not set.")

    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent"
    params = {"key": API_KEY}
    headers = {"Content-Type": "application/json"}

    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"response_mime_type": "application/json"}
    }

    for attempt in range(max_retries + 1):
        response = requests.post(url, headers=headers, json=data, params=params)

        if response.status_code == 200:
            break
        elif response.status_code == 429 and attempt < max_retries:
            wait_time = 30 * (2 ** attempt)
            print(f"â³ ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆåˆ°é”ã€‚{wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... ({attempt + 1}/{max_retries})")
            import time
            time.sleep(wait_time)
        else:
            raise Exception(f"API Error: {response.status_code} - {response.text}")

    result = response.json()
    try:
        if "candidates" in result and result["candidates"]:
            return result["candidates"][0]["content"]["parts"][0]["text"]
        else:
            raise Exception(f"Empty candidates in response: {result}")
    except (KeyError, IndexError):
        raise Exception(f"Unexpected API response format: {result}")


def load_knowledge_graph() -> Optional[Dict[str, Any]]:
    """ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    if not os.path.exists(MASTER_GRAPH_PATH):
        print("âš ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è‰æ¡ˆã®ã¿ã§è¨˜äº‹ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
        return None
    
    try:
        with open(MASTER_GRAPH_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        return None


def extract_blog_context(graph: Dict[str, Any]) -> str:
    """ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒ–ãƒ­ã‚°è¨˜äº‹ã«å½¹ç«‹ã¤ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚

    æœ€è¿‘ã®çŸ¥è¦‹ãƒ»æ„Ÿæƒ…ãƒ»ã‚¿ã‚¹ã‚¯ã®å‚¾å‘ã‹ã‚‰ã€è¨˜äº‹ã®åˆ‡ã‚Šå£ã«ä½¿ãˆã‚‹æƒ…å ±ã‚’æä¾›ã™ã‚‹ã€‚
    """
    nodes = graph.get("nodes", [])
    
    # çŸ¥è¦‹ã‹ã‚‰ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
    insights = [n for n in nodes if n.get("type") == "çŸ¥è¦‹"]
    insights_sorted = sorted(insights, key=lambda x: x.get("last_seen", ""), reverse=True)[:8]
    
    # æ„Ÿæƒ…ã®å‚¾å‘ã‚’æŠ½å‡º
    emotions = [n for n in nodes if n.get("type") == "æ„Ÿæƒ…"]
    emotions_sorted = sorted(emotions, key=lambda x: x.get("last_seen", ""), reverse=True)[:5]
    
    # ã‚¿ã‚¹ã‚¯ã®å‚¾å‘ã‚’æŠ½å‡º
    tasks = [n for n in nodes if n.get("type") == "ã‚¿ã‚¹ã‚¯"]
    tasks_sorted = sorted(tasks, key=lambda x: x.get("last_seen", ""), reverse=True)[:5]
    
    context = "### ç­†è€…ã®æœ€è¿‘ã®é–¢å¿ƒäº‹ï¼ˆè¨˜äº‹ã«æ·±ã¿ã‚’æŒãŸã›ã‚‹ãŸã‚ã®èƒŒæ™¯æƒ…å ±ï¼‰\n"
    
    if insights_sorted:
        context += "\n**æœ€è¿‘ã®æ°—ã¥ã:**\n"
        for i in insights_sorted:
            label = i.get('label', '')
            detail = i.get('detail', '')
            if detail:
                context += f"- {label}: {detail[:80]}\n"
            else:
                context += f"- {label}\n"
    
    if emotions_sorted:
        context += "\n**æœ€è¿‘ã®æ„Ÿæƒ…å‚¾å‘:**\n"
        for e in emotions_sorted:
            label = e.get('label', '')
            sentiment = e.get('sentiment', 0)
            tone = "ãƒã‚¸ãƒ†ã‚£ãƒ–" if sentiment > 0 else "ãƒã‚¬ãƒ†ã‚£ãƒ–" if sentiment < 0 else "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«"
            context += f"- {label}ï¼ˆ{tone}ï¼‰\n"
    
    if tasks_sorted:
        context += "\n**æœ€è¿‘å–ã‚Šçµ„ã‚“ã§ã„ã‚‹ã“ã¨:**\n"
        for t in tasks_sorted:
            label = t.get('label', '')
            context += f"- {label}\n"
    
    return context


def parse_blog_memo(memo_text: str) -> Dict[str, str]:
    """ãƒ–ãƒ­ã‚°ç”¨ãƒ¡ãƒ¢ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ã€‚

    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå½¢å¼ã«ã‚‚è‡ªç”±å½¢å¼ã«ã‚‚å¯¾å¿œã™ã‚‹ã€‚
    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:
    - ãƒ†ãƒ¼ãƒ
    - ã‚«ãƒ†ã‚´ãƒª
    - ä¼ãˆãŸã„ã“ã¨
    - ãƒ¡ãƒ¢
    """
    fields = {
        "ãƒ†ãƒ¼ãƒ": "",
        "ã‚«ãƒ†ã‚´ãƒª": "",
        "ä¼ãˆãŸã„ã“ã¨": "",
        "ãƒ¡ãƒ¢": ""
    }
    
    known_keys = list(fields.keys())
    has_template = any(f"{key}:" in memo_text or f"{key}ï¼š" in memo_text for key in known_keys[:2])
    
    if not has_template:
        fields["ãƒ¡ãƒ¢"] = memo_text.strip()
        return fields
    
    current_key = None
    current_lines = []
    
    for line in memo_text.split("\n"):
        stripped = line.strip()
        if not stripped:
            if current_key:
                current_lines.append("")
            continue
        
        matched_key = None
        for key in known_keys:
            if stripped.startswith(f"{key}:") or stripped.startswith(f"{key}ï¼š"):
                matched_key = key
                break
        
        if matched_key:
            if current_key:
                fields[current_key] = "\n".join(current_lines).strip()
            current_key = matched_key
            sep = "ï¼š" if f"{matched_key}ï¼š" in stripped else ":"
            value = stripped.split(sep, 1)[1].strip()
            current_lines = [value] if value else []
        else:
            current_lines.append(stripped)
    
    if current_key:
        fields[current_key] = "\n".join(current_lines).strip()
    
    return fields


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def build_generation_prompt(fields: Dict[str, str], context: str) -> str:
    """ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿ç«‹ã¦ã‚‹ã€‚"""
    memo_section = "### ãƒ–ãƒ­ã‚°ã®ãƒã‚¿ï¼ˆãƒãƒ¡ãƒ©ã§æ›¸ã„ãŸãƒ¡ãƒ¢ï¼‰\n\n"
    
    if fields["ãƒ†ãƒ¼ãƒ"]:
        memo_section += f"**ãƒ†ãƒ¼ãƒ:** {fields['ãƒ†ãƒ¼ãƒ']}\n"
    if fields["ã‚«ãƒ†ã‚´ãƒª"]:
        memo_section += f"**ã‚«ãƒ†ã‚´ãƒª:** {fields['ã‚«ãƒ†ã‚´ãƒª']}\n"
    if fields["ä¼ãˆãŸã„ã“ã¨"]:
        memo_section += f"\n**ä¼ãˆãŸã„ã“ã¨:**\n{fields['ä¼ãˆãŸã„ã“ã¨']}\n"
    if fields["ãƒ¡ãƒ¢"]:
        memo_section += f"\n**ãƒ¡ãƒ¢ã®å†…å®¹:**\n{fields['ãƒ¡ãƒ¢']}\n"
    
    return f"""
{BLOG_SYSTEM_PROMPT}

{context}

{memo_section}

### æŒ‡ç¤º
ä¸Šè¨˜ã®ãƒ¡ãƒ¢ã‚’ã‚‚ã¨ã«ã€èª­è€…ãŒæ¥½ã—ãèª­ã‚ã‚‹ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’1æœ¬åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
ãƒ¡ãƒ¢ã®å†…å®¹ã‚’ãã®ã¾ã¾ä½¿ã†ã®ã§ã¯ãªãã€ãƒ–ãƒ­ã‚°è¨˜äº‹ã¨ã—ã¦å†æ§‹æˆã—ã¦ãã ã•ã„ã€‚
èƒŒæ™¯æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã€è¨˜äº‹ã«æ·±ã¿ã‚’æŒãŸã›ã‚‹ãŸã‚ã«æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚
"""


def review_article(article_body: str) -> Tuple[bool, list]:
    """ç”Ÿæˆã•ã‚ŒãŸãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã€‚"""
    prompt = f"""
{REVIEW_PROMPT}

### ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®ãƒ–ãƒ­ã‚°è¨˜äº‹
{article_body}
"""
    
    print("ğŸ” å“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­...")
    json_text = call_gemini_api(prompt)
    result = json.loads(json_text)
    passed = result.get("passed", True)
    issues = result.get("issues", [])
    
    if issues:
        for issue in issues:
            print(f"   âš ï¸ [{issue.get('type', 'ä¸æ˜')}] {issue.get('detail', '')}")
    else:
        print("   âœ… å“è³ªãƒã‚§ãƒƒã‚¯åˆæ ¼")
    
    return passed, issues


def generate_blog_article(memo_text: str, context: str, max_revisions: int = 1) -> Dict[str, Any]:
    """ãƒ¡ãƒ¢ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ç”Ÿæˆã™ã‚‹ã€‚

    ç”Ÿæˆå¾Œã«è‡ªå·±ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡Œã„ã€å“è³ªã«å•é¡ŒãŒã‚ã‚Œã°
    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åæ˜ ã—ã¦å†ç”Ÿæˆã™ã‚‹ã€‚
    """
    
    fields = parse_blog_memo(memo_text)
    
    prompt = build_generation_prompt(fields, context)
    print("ğŸ“ ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ç”Ÿæˆä¸­...")
    json_text = call_gemini_api(prompt)
    article_data = json.loads(json_text)
    
    for revision in range(max_revisions):
        article_body = article_data.get("body", "")
        if not article_body:
            break
        
        passed, issues = review_article(article_body)
        
        if passed:
            break
        
        feedback_lines = []
        for issue in issues:
            feedback_lines.append(
                f"- [{issue.get('type', '')}] {issue.get('detail', '')}\n"
                f"  æ”¹å–„æ¡ˆ: {issue.get('suggestion', '')}"
            )
        feedback_section = "\n".join(feedback_lines)
        
        revision_prompt = f"""
{BLOG_SYSTEM_PROMPT}

{context}

{build_generation_prompt(fields, context)}

### å‰å›ã®ç”Ÿæˆçµæœã«å¯¾ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
ä»¥ä¸‹ã®å•é¡ŒãŒæŒ‡æ‘˜ã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã‚’å…¨ã¦ä¿®æ­£ã—ãŸä¸Šã§ã€è¨˜äº‹ã‚’æ›¸ãç›´ã—ã¦ãã ã•ã„ã€‚

{feedback_section}

### å‰å›ã®æœ¬æ–‡ï¼ˆå‚è€ƒï¼‰
{article_body[:1000]}...

### æŒ‡ç¤º
ä¸Šè¨˜ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åæ˜ ã—ã€å•é¡Œã‚’ä¿®æ­£ã—ãŸæ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
è¨˜äº‹ã®éª¨æ ¼ã¯ç¶­æŒã—ã¤ã¤ã€æŒ‡æ‘˜ã•ã‚ŒãŸå“è³ªå•é¡Œã‚’è§£æ¶ˆã—ã¦ãã ã•ã„ã€‚
"""
        
        print(f"ğŸ“ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åæ˜ ç‰ˆã‚’ç”Ÿæˆä¸­... (ãƒªãƒ“ã‚¸ãƒ§ãƒ³ {revision + 1}/{max_revisions})")
        json_text = call_gemini_api(revision_prompt)
        article_data = json.loads(json_text)
    
    return article_data


def save_article(article_data: Dict[str, Any], source_file: str) -> tuple:
    """ç”Ÿæˆã•ã‚ŒãŸãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚"""
    if not os.path.exists(BLOG_READY_DIR):
        os.makedirs(BLOG_READY_DIR)
    
    date_str = datetime.now().strftime('%Y%m%d')
    title = article_data.get("title", "ç„¡é¡Œ")
    
    safe_title = re.sub(r'[\\/*?:"<>|]', '', title)[:50]
    
    md_filename = f"{date_str}_{safe_title}.md"
    md_path = os.path.join(BLOG_READY_DIR, md_filename)
    
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(article_data.get("body", ""))
    
    meta_filename = f"{date_str}_{safe_title}.json"
    meta_path = os.path.join(BLOG_READY_DIR, meta_filename)
    
    meta = {
        "title": title,
        "description": article_data.get("description", ""),
        "categories": article_data.get("categories", []),
        "estimated_read_time": article_data.get("estimated_read_time", ""),
        "source_file": source_file,
        "generated_at": datetime.now().isoformat(),
        "type": "blog_article"
    }
    
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {md_path}")
    print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {meta_path}")
    
    return md_path, meta_path


def publish_to_hatena(md_path: str, meta_path: str):
    """ã¯ã¦ãªãƒ–ãƒ­ã‚°ã«ä¸‹æ›¸ãæŠ•ç¨¿ã™ã‚‹ã€‚"""
    cmd = ["python3", HATENA_PUBLISHER_SCRIPT, md_path, "--meta", meta_path, "--force"]
    print("ğŸš€ ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã‚’é–‹å§‹...")
    result = subprocess.run(cmd)
    if result.returncode != 0:
        print(f"âš ï¸ ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ (returncode={result.returncode})")
    else:
        print("âœ… ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ãŒå®Œäº†ã—ã¾ã—ãŸ")


def main():
    parser = argparse.ArgumentParser(description="ãƒãƒ¡ãƒ©ãƒ¡ãƒ¢ â†’ ãƒ–ãƒ­ã‚°è¨˜äº‹ â†’ ã¯ã¦ãªãƒ–ãƒ­ã‚°æŠ•ç¨¿")
    parser.add_argument("input_file", help="ãƒ¡ãƒ¢ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--skip-publish", action="store_true", help="ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--master-graph", default=MASTER_GRAPH_PATH, help="ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®ãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    # 1. ãƒ¡ãƒ¢ã®èª­ã¿è¾¼ã¿
    try:
        import unicodedata
        args.input_file = unicodedata.normalize('NFC', args.input_file)
        with open(args.input_file, "r", encoding="utf-8") as f:
            memo_text = f.read()
    except FileNotFoundError:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.input_file}")
        return
    
    if not memo_text.strip():
        print("âŒ ãƒ¡ãƒ¢ãŒç©ºã§ã™ã€‚")
        return
    
    print(f"ğŸ“„ ãƒ¡ãƒ¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {args.input_file}")
    print(f"   æ–‡å­—æ•°: {len(memo_text)}")
    
    # 2. ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    context = ""
    graph = load_knowledge_graph()
    if graph:
        context = extract_blog_context(graph)
        print("ğŸ“Š ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    
    # 3. ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ç”Ÿæˆ
    try:
        article_data = generate_blog_article(memo_text, context)
        print(f"âœ¨ è¨˜äº‹ç”Ÿæˆå®Œäº†: ã€Œ{article_data.get('title', 'ç„¡é¡Œ')}ã€")
    except Exception as e:
        print(f"âŒ è¨˜äº‹ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # 4. ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    md_path, meta_path = save_article(article_data, args.input_file)
    
    # 5. ã¯ã¦ãªãƒ–ãƒ­ã‚°ã«æŠ•ç¨¿
    if not args.skip_publish:
        publish_to_hatena(md_path, meta_path)
    else:
        print("â­ï¸ ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")


if __name__ == "__main__":
    main()
