import json
import os
import math
import argparse
from datetime import datetime, timezone, timedelta

# â”€â”€ æ„Ÿæƒ…ã‚¿ã‚¤ãƒ—åˆ¥ã®æ¸›è¡°ä¿‚æ•° (1æ—¥ã‚ãŸã‚Š) â”€â”€
# æ„Ÿæƒ…ãŒè¨€åŠã•ã‚Œãªã„æ—¥ãŒç¶šãã»ã© active_sentiment ãŒä¸­æ€§å€¤ã«è¿‘ã¥ã
EMOTION_DECAY_RATES = {
    "å–œã³": 0.30,      # æ˜‚ã‚Šã¯çŸ­å‘½ã€‚1æ—¥ã§ã‹ãªã‚Šå†·ã‚ã‚‹
    "é”æˆæ„Ÿ": 0.15,    # æ•°æ—¥é–“ã¯å……å®Ÿæ„ŸãŒæ®‹ã‚‹
    "ä¸å®‰": 0.05,      # è§£æ±ºã•ã‚Œãªã„é™ã‚Šã»ã¼æ¶ˆãˆãªã„
    "æ€’ã‚Š": 0.20,      # æ—©ã‚ã«å†·ã‚ã‚‹
    "ãã®ä»–": 0.10,    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
}

# â”€â”€ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰è§£æ±º â”€â”€
_EMBED_SIMILARITY_THRESHOLD = 0.85

def _cosine_similarity(a, b):
    """2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ã€‚"""
    dot = sum(x * y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x * x for x in a))
    norm_b = math.sqrt(sum(x * x for x in b))
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot / (norm_a * norm_b)


def _get_embeddings(texts):
    """Gemini Embedding APIã§ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®embeddingã‚’å–å¾—ã™ã‚‹ã€‚"""
    try:
        import google.generativeai as genai
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            print("   âš ï¸ GOOGLE_API_KEY ãŒæœªè¨­å®šã®ãŸã‚ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return []
        genai.configure(api_key=api_key)
        result = genai.embed_content(
            model="models/gemini-embedding-001",
            content=texts,
        )
        return result["embedding"]
    except Exception as e:
        print(f"   âš ï¸ Embedding API ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def _resolve_target_by_vector(target_id, master_nodes_dict, _cache={}):
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆIDã«æœ€ã‚‚æ„å‘³çš„ã«è¿‘ã„ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‚’æ¢ã™ã€‚

    çµæœã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦åŒä¸€ãƒãƒ¼ã‚¸å†…ã§ã®é‡è¤‡APIå‘¼ã³å‡ºã—ã‚’é˜²ãã€‚
    """
    if target_id in _cache:
        return _cache[target_id]

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒ©ãƒ™ãƒ«éƒ¨åˆ†ã‚’æŠ½å‡º
    target_label = target_id.split(":", 1)[-1] if ":" in target_id else target_id
    target_type = target_id.split(":", 1)[0] if ":" in target_id else ""

    # ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã®å€™è£œãƒãƒ¼ãƒ‰ã‚’åé›†
    candidates = []
    for nid, node in master_nodes_dict.items():
        node_type = node.get("type", "")
        node_label = node.get("label", "")
        # ã‚¿ã‚¤ãƒ—ãŒå¤§ããç•°ãªã‚‹å ´åˆã¯é™¤å¤–
        if target_type and target_type not in ("ç›®æ¨™", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ") and node_type not in ("ç›®æ¨™", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ", target_type):
            continue
        if target_type in ("ç›®æ¨™", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ") and node_type not in ("ç›®æ¨™", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ"):
            continue
        if node_label:
            candidates.append((nid, node_label))

    if not candidates:
        _cache[target_id] = None
        return None

    # embeddingã‚’å–å¾—
    texts = [target_label] + [c[1] for c in candidates]
    embeddings = _get_embeddings(texts)
    if len(embeddings) < 2:
        _cache[target_id] = None
        return None

    target_emb = embeddings[0]
    best_sim = -1.0
    best_id = None
    for i, (nid, label) in enumerate(candidates):
        sim = _cosine_similarity(target_emb, embeddings[i + 1])
        if sim > best_sim:
            best_sim = sim
            best_id = nid

    if best_sim >= _EMBED_SIMILARITY_THRESHOLD:
        print(f"   ğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: '{target_label}' â†’ '{master_nodes_dict[best_id].get('label', best_id)}' (é¡ä¼¼åº¦: {best_sim:.3f})")
        _cache[target_id] = best_id
        return best_id
    else:
        print(f"   âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: '{target_label}' ã«è©²å½“ãªã— (æœ€å¤§é¡ä¼¼åº¦: {best_sim:.3f})")
        _cache[target_id] = None
        return None

def _resolve_label_by_vector(label, node_type, master_nodes_dict, _cache={}):
    """Pass 1ç”¨: ãƒ©ãƒ™ãƒ«ã¨ã‚¿ã‚¤ãƒ—ã‹ã‚‰ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•å†…ã®é¡ä¼¼ãƒãƒ¼ãƒ‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§æ¢ã™ã€‚

    æ—¥è¨˜å‹ãƒãƒ¼ãƒ‰ã‚„ã‚¿ã‚¤ãƒ—ãŒå¤§ããç•°ãªã‚‹ãƒãƒ¼ãƒ‰ã¯é™¤å¤–ã™ã‚‹ã€‚
    """
    cache_key = f"{node_type}:{label}"
    if cache_key in _cache:
        return _cache[cache_key]

    # æ—¥è¨˜å‹ã¯ã‚¹ã‚­ãƒƒãƒ—
    if node_type in ('æ—¥è¨˜', 'diary'):
        _cache[cache_key] = None
        return None

    # ç›®æ¨™ã¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ç›¸äº’ã«æ¤œç´¢å¯èƒ½
    GOAL_TYPES = {'ç›®æ¨™', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'goal', 'project'}
    type_group_match = node_type in GOAL_TYPES

    candidates = []
    for nid, node in master_nodes_dict.items():
        n_type = node.get('type', '')
        n_label = node.get('label', '')
        if not n_label or n_type in ('æ—¥è¨˜', 'diary'):
            continue
        # ã‚¿ã‚¤ãƒ—ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if type_group_match:
            if n_type not in GOAL_TYPES:
                continue
        elif node_type and n_type != node_type:
            continue
        candidates.append((nid, n_label))

    if not candidates:
        _cache[cache_key] = None
        return None

    texts = [label] + [c[1] for c in candidates]
    embeddings = _get_embeddings(texts)
    if len(embeddings) < 2:
        _cache[cache_key] = None
        return None

    target_emb = embeddings[0]
    best_sim = -1.0
    best_id = None
    for i, (nid, clabel) in enumerate(candidates):
        sim = _cosine_similarity(target_emb, embeddings[i + 1])
        if sim > best_sim:
            best_sim = sim
            best_id = nid

    if best_sim >= _EMBED_SIMILARITY_THRESHOLD:
        print(f"   ğŸ” Pass1ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: '{label}' â†’ '{master_nodes_dict[best_id].get('label', best_id)}' (é¡ä¼¼åº¦: {best_sim:.3f})")
        _cache[cache_key] = best_id
        return best_id
    else:
        _cache[cache_key] = None
        return None

def load_graph(filepath):
    """Loads a graph JSON file. Returns an empty structure if file doesn't exist."""
    if not os.path.exists(filepath):
        print(f"â„¹ï¸  Master graph not found at {filepath}. Creating new one.")
        return {"nodes": [], "edges": [], "metadata": {}}
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError:
        print(f"âš ï¸  Error decoding JSON from {filepath}. Returning empty graph.")
        return {"nodes": [], "edges": [], "metadata": {}}

def _days_since(last_seen_str: str) -> int:
    """last_seenæ–‡å­—åˆ—ã‹ã‚‰ä»Šæ—¥ã¾ã§ã®çµŒéæ—¥æ•°ã‚’è¿”ã™ã€‚ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯0ã€‚"""
    try:
        dt = datetime.fromisoformat(last_seen_str)
        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ãŒä»˜ã„ã¦ã„ãªã„å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«æ‰±ã„
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        now = datetime.now(tz=timezone.utc)
        return max(0, (now - dt).days)
    except Exception:
        return 0


def _make_edge_key_set(graph: dict) -> set:
    """ã‚°ãƒ©ãƒ•å†…ã®ã‚¨ãƒƒã‚¸ã‚’ source|target|type å½¢å¼ã®ã‚­ãƒ¼ã‚»ãƒƒãƒˆã«å¤‰æ›ã™ã‚‹ã€‚"""
    keys = set()
    for e in graph.get('edges', []):
        key = f"{e.get('source', '')}|{e.get('target', '')}|{e.get('type', 'UNKNOWN')}"
        keys.add(key)
    return keys


def _calc_trend(history: list) -> str:
    """emotion_history ã®ç›´è¿‘3ä»¶ã® active_sentiment ã‹ã‚‰å‚¾å‘ã‚’è¨ˆç®—ã™ã‚‹ã€‚"""
    recent = [h.get("active_sentiment") for h in history[-3:] if h.get("active_sentiment") is not None]
    if len(recent) < 2:
        return "å®‰å®š"
    diff = recent[-1] - recent[0]
    if diff > 0.05:
        return "ä¸Šæ˜‡"
    elif diff < -0.05:
        return "ä¸‹é™"
    return "å®‰å®š"


def apply_emotion_decay(master: dict, daily_node_ids: set, today_str: str) -> None:
    """æ„Ÿæƒ…ãƒãƒ¼ãƒ‰ã® active_sentiment ã‚’æ¸›è¡°ã•ã›ã€emotion_history ã«ä»Šæ—¥åˆ†ã‚’è¿½è¨˜ã™ã‚‹ã€‚

    - ä»Šæ—¥è¨€åŠã•ã‚ŒãŸæ„Ÿæƒ…ãƒãƒ¼ãƒ‰: active_sentiment ã‚’ sentiment æ°´æº–ã«æˆ»ã™
    - ä»Šæ—¥è¨€åŠã•ã‚Œãªã‹ã£ãŸæ„Ÿæƒ…ãƒãƒ¼ãƒ‰: emotion_category ã«å¿œã˜ãŸä¿‚æ•°ã§æ¸›è¡°
    """
    for node in master.get("nodes", []):
        if node.get("type") != "æ„Ÿæƒ…":
            continue

        sentiment = node.get("sentiment", 0)
        active = node.get("active_sentiment", sentiment)
        category = node.get("emotion_category", "ãã®ä»–")
        trigger = node.get("trigger")

        if node["id"] in daily_node_ids:
            # ä»Šæ—¥è¨€åŠ â†’ active_sentiment ã‚’ sentiment æ°´æº–ã«æˆ»ã™
            new_active = sentiment
        else:
            # è¨€åŠãªã— â†’ ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ä¿‚æ•°ã§æ¸›è¡°ã€‚0ã«å‘ã‹ã£ã¦è¿‘ã¥ã
            decay = EMOTION_DECAY_RATES.get(category, EMOTION_DECAY_RATES["ãã®ä»–"])
            # active_sentiment ã¯ 0 ã«åæŸ (å®Œå…¨ã«å¿˜ã‚Œã‚‹)
            new_active = round(active * (1 - decay), 4)

        node["active_sentiment"] = new_active

        # emotion_history ã«ä»Šæ—¥åˆ†ã‚’è¿½è¨˜ (é‡è¤‡é˜²æ­¢)
        history = node.get("emotion_history", [])
        if not any(h.get("date") == today_str for h in history):
            history.append({
                "date": today_str,
                "sentiment": sentiment,
                "active_sentiment": new_active,
                "trigger": trigger if node["id"] in daily_node_ids else None
            })
        node["emotion_history"] = history

        # peak_sentiment ã¨ trend ã‚’æ›´æ–°
        all_sentiments = [h["sentiment"] for h in history if h.get("sentiment") is not None]
        node["peak_sentiment"] = max(all_sentiments) if all_sentiments else sentiment
        node["trend"] = _calc_trend(history)

    active_emotions = [n for n in master.get("nodes", []) if n.get("type") == "æ„Ÿæƒ…"]
    print(f"ğŸ’« Emotion decay applied: {len(active_emotions)} emotion nodes updated.")


def apply_weight_decay(master: dict, daily_node_ids: set, daily_edge_keys: set) -> None:
    """ä»Šå›ã®ãƒãƒ¼ã‚¸ã§ç™»å ´ã—ãªã‹ã£ãŸãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã®weightã‚’ã€
    last_seenã‹ã‚‰ã®çµŒéæ—¥æ•°ã«å¿œã˜ã¦å°åˆ»ã¿ã«æ¸›è¡°ã•ã›ã‚‹ã€‚

    - æ¸›è¡°é‡: 0.05 / æ—¥ (ãƒãƒ¼ã‚¸æœªç™»å ´1å›ã«ã¤ã)
    - ä¸‹é™  : 0.1
    - æ—¥è¨˜å‹ãƒãƒ¼ãƒ‰ã¯å¯¾è±¡å¤–
    """
    DECAY_PER_DAY = 0.05
    MIN_WEIGHT = 0.1
    DIARY_TYPES = {'æ—¥è¨˜', 'diary'}

    # --- ãƒãƒ¼ãƒ‰ ---
    for node in master.get('nodes', []):
        if node.get('type') in DIARY_TYPES:
            continue
        if node['id'] in daily_node_ids:
            # ä»Šå›ç™»å ´ã—ãŸãƒãƒ¼ãƒ‰ã¯æ¸›è¡°ã—ãªã„
            continue

        missed_days = _days_since(node.get('last_seen', '')) if node.get('last_seen') else 1
        # å°‘ãªãã¨ã‚‚ä»Šå›ã®ãƒãƒ¼ã‚¸åˆ†1å›ã¯æ¸›è¡°ã•ã›ã‚‹
        missed_days = max(1, missed_days)
        decay = DECAY_PER_DAY * missed_days
        node['weight'] = max(MIN_WEIGHT, round(node.get('weight', 1) - decay, 4))

    # --- ã‚¨ãƒƒã‚¸ ---
    for edge in master.get('edges', []):
        key = f"{edge.get('source', '')}|{edge.get('target', '')}|{edge.get('type', 'UNKNOWN')}"
        if key in daily_edge_keys:
            continue

        missed_days = _days_since(edge.get('last_seen', '')) if edge.get('last_seen') else 1
        missed_days = max(1, missed_days)
        decay = DECAY_PER_DAY * missed_days
        edge['weight'] = max(MIN_WEIGHT, round(edge.get('weight', 1) - decay, 4))

    # ãƒ­ã‚°
    decayed_nodes = [n for n in master.get('nodes', []) if n['id'] not in daily_node_ids and n.get('type') not in DIARY_TYPES]
    print(f"â³ Weight decay applied: {len(decayed_nodes)} nodes / {len(master.get('edges', [])) - len(daily_edge_keys)} edges")


def merge_graphs(master, daily):
    """Merges a daily graph into the master graph."""
    
    # --- 1. Merge Nodes ---
    master_nodes = {n['id']: n for n in master.get('nodes', [])}
    
    # Map label+type to ID for duplicate detection
    # only for specific types where names should be unique identifiers
    label_map = {} 
    mergeable_types = {'äººç‰©', 'å ´æ‰€', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'æ¦‚å¿µ', 'ç›®æ¨™', 'æ„Ÿæƒ…', 'çŸ¥è¦‹', 'ã‚¿ã‚¹ã‚¯', 'å‡ºæ¥äº‹', 'åˆ¶ç´„', 'person', 'place', 'project', 'concept', 'goal', 'emotion', 'insight', 'task', 'event'}
    
    # First, build a label_map from the MASTER
    for n in master.get('nodes', []):
        if n.get('label'):
            l_key = n.get('label')
            if l_key not in label_map or n.get('type') in mergeable_types:
                label_map[l_key] = n['id']
    new_node_count = 0
    updated_node_count = 0
    id_remap = {} 

    # Pass 1: Global Label Normalization
    # Goal: Ensure every label maps to exactly ONE canonical ID (preferring Master IDs if available)
    for node in daily.get('nodes', []):
        raw_id = node['id']
        nlabel = node.get('label')
        
        if nlabel:
            if nlabel in label_map:
                # This label already exists (either in Master or established earlier in this Daily batch)
                target_id = label_map[nlabel]
                if raw_id != target_id:
                    print(f"ğŸ”„ Remapping node: {raw_id} -> {target_id} (Label: '{nlabel}')")
                    id_remap[raw_id] = target_id
                    node['id'] = target_id
            else:
                # ãƒ©ãƒ™ãƒ«å®Œå…¨ä¸€è‡´ãªã— â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ãƒã‚¹ã‚¿ãƒ¼å†…ã®é¡ä¼¼ãƒãƒ¼ãƒ‰ã‚’æ¢ã™
                ntype = node.get('type', '')
                resolved_id = _resolve_label_by_vector(nlabel, ntype, master_nodes)
                if resolved_id:
                    print(f"ğŸ”„ Remapping node (vector): {raw_id} -> {resolved_id} (Label: '{nlabel}')")
                    id_remap[raw_id] = resolved_id
                    node['id'] = resolved_id
                    label_map[nlabel] = resolved_id
                else:
                    # æœ¬å½“ã«æ–°ã—ã„ãƒãƒ¼ãƒ‰
                    label_map[nlabel] = raw_id

    # Pass 2: Property Merging with Normalized IDs
    # Now that all nodes in 'daily' have their IDs normalized to either a Master ID or a Canonical Daily ID,
    # we can safely merge them.
    for node in daily.get('nodes', []):
        nid = node['id']
        ntype = node.get('type')
        current_time = datetime.now().isoformat()

        # REMOVAL LOGIC: If this node was remapped, we need to check if the OLD raw ID
        # is still lingering in master_nodes (this is how duplicates stay alive)
        # However, because we iterate over 'daily', we handle the 'raw_id -> target_id' transition.
        # Let's ensure ANY node with this label that isn't the current 'nid' gets cleared.
        label = node.get('label')
        if label:
            canonical_id = label_map.get(label)
            # Find any other nodes in master that have this label but different ID and purge them
            to_delete = [old_id for old_id, old_node in master_nodes.items() 
                         if old_node.get('label') == label and old_id != canonical_id]
            for old_id in to_delete:
                print(f"ğŸ—‘ï¸  Removing duplicate node from Master: {old_id} (Label: '{label}')")
                del master_nodes[old_id]

        if nid in master_nodes:
            # Update existing node in master
            existing = master_nodes[nid]
            
            # Simple overwrite with latest data from daily
            existing['label'] = node.get('label', existing.get('label'))
            existing['detail'] = node.get('detail', existing.get('detail'))
            
            # Type update (prefer mergeable_types)
            if ntype in mergeable_types:
                existing['type'] = ntype
            
            if 'status' in node:
                print(f"   ğŸ”„ Updating status for {nid}: {existing.get('status')} -> {node['status']}")
                existing['status'] = node['status']
            
            if 'date' in node:
                existing['date'] = node['date']
            
            if 'analysis_content' in node:
                existing['analysis_content'] = node['analysis_content']
            
            if 'sentiment' in node:
                existing['sentiment'] = node['sentiment']

            if existing.get('type') in ('diary', 'æ—¥è¨˜'):
                existing['weight'] = 1
            else:
                existing['weight'] = existing.get('weight', 1) + node.get('weight', 1)
            
            existing_tags = set(existing.get('tags', []))
            new_tags = set(node.get('tags', []))
            existing['tags'] = sorted(list(existing_tags.union(new_tags)))
            
            existing['last_seen'] = current_time
            updated_node_count += 1
        else:
            # This is a truly new node (new unique label)
            node['first_seen'] = current_time
            node['last_seen'] = current_time
            node['weight'] = node.get('weight', 1)
            master_nodes[nid] = node
            new_node_count += 1

    # Finalize master nodes list
    master['nodes'] = list(master_nodes.values())

    # --- 2. Merge Edges ---
    master_edges = {}
    for e in master.get('edges', []):
        try:
            key = f"{e.get('source', '')}|{e.get('target', '')}|{e.get('type', 'UNKNOWN')}"
            master_edges[key] = e
        except Exception:
            continue
        
    new_edge_count = 0
    updated_edge_count = 0
        
    for edge in daily.get('edges', []):
        # Apply remapping for edges too
        source = id_remap.get(edge['source'], edge['source'])
        target = id_remap.get(edge['target'], edge['target'])
        
        try:
            if source == target:
                continue
            key = f"{source}|{target}|{edge.get('type', 'UNKNOWN')}"
        except Exception:
            continue

        current_time = datetime.now().isoformat()
        
        if key in master_edges:
            # Update existing edge
            existing = master_edges[key]
            existing['weight'] = existing.get('weight', 1) + 1
            existing['last_seen'] = current_time
            existing['label'] = edge.get('label', existing.get('label')) # Update label to latest context
            updated_edge_count += 1
        else:
            # Add new edge
            # Ensure we use remapped IDs
            new_edge = edge.copy()
            new_edge['source'] = source
            new_edge['target'] = target
            new_edge['first_seen'] = current_time
            new_edge['last_seen'] = current_time
            new_edge['weight'] = 1
            master_edges[key] = new_edge
            new_edge_count += 1
            
    master['edges'] = list(master_edges.values())
    
    # --- 2.5 è¨€åŠã‚¨ãƒƒã‚¸ã§å‚ç…§ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã® last_seen ã‚’æ›´æ–° ---
    # æ—¥è¨˜ãƒãƒ¼ãƒ‰ã‹ã‚‰ã€Œè¨€åŠã™ã‚‹ã€ã‚¨ãƒƒã‚¸ã§ç¹‹ãŒã£ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã¯ã€
    # ãƒ‡ã‚¤ãƒªãƒ¼ã‚°ãƒ©ãƒ•ã«ç›´æ¥ãƒãƒ¼ãƒ‰ã¨ã—ã¦æŠ½å‡ºã•ã‚Œãªãã¦ã‚‚ã€è¨€åŠãŒã‚ã£ãŸäº‹å®Ÿã‚’
    # last_seen ã«åæ˜ ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šç›®æ¨™ã®é€²æ—è¿½è·¡ãŒæ­£ç¢ºã«ãªã‚‹ã€‚
    # IDä¸ä¸€è‡´æ™‚ã¯Gemini Embedding APIã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’è¡Œã„ã€æ„å‘³çš„ã«æœ€ã‚‚
    # è¿‘ã„ãƒãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹ã€‚
    mention_updated = 0
    vector_resolved = 0
    current_time_mention = datetime.now().isoformat()
    master_nodes_dict = {n['id']: n for n in master['nodes']}
    
    for edge in daily.get('edges', []):
        if edge.get('type') == 'è¨€åŠã™ã‚‹':
            target_id = id_remap.get(edge['target'], edge['target'])
            if target_id in master_nodes_dict:
                # IDå®Œå…¨ä¸€è‡´
                master_nodes_dict[target_id]['last_seen'] = current_time_mention
                mention_updated += 1
            else:
                # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                resolved_id = _resolve_target_by_vector(target_id, master_nodes_dict)
                if resolved_id:
                    master_nodes_dict[resolved_id]['last_seen'] = current_time_mention
                    mention_updated += 1
                    vector_resolved += 1
                    # æ¬¡å›ä»¥é™IDä¸€è‡´ã™ã‚‹ã‚ˆã†ã«ãƒªãƒãƒƒãƒ—ã‚’è¨˜éŒ²
                    id_remap[edge['target']] = resolved_id
    
    master['nodes'] = list(master_nodes_dict.values())
    
    if mention_updated:
        msg = f"   ğŸ”— è¨€åŠã‚¨ãƒƒã‚¸çµŒç”±ã§ {mention_updated} ãƒãƒ¼ãƒ‰ã® last_seen ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚"
        if vector_resolved:
            msg += f" (ã†ã¡ {vector_resolved} ä»¶ã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§è§£æ±º)"
        print(msg)

    # --- 3. Update Metadata ---
    if 'metadata' not in master:
        master['metadata'] = {}
        
    master['metadata']['last_updated'] = datetime.now().isoformat()
    master['metadata']['node_count'] = len(master['nodes'])
    master['metadata']['edge_count'] = len(master['edges'])
    
    print(f"âœ… Merge Complete!")
    print(f"   Nodes: {new_node_count} new, {updated_node_count} updated.")
    print(f"   Edges: {new_edge_count} new, {updated_edge_count} updated.")

    # --- 4. Weightæ™‚é–“æ¸›è¡° ---
    daily_node_ids = set(n['id'] for n in daily.get('nodes', []))
    apply_weight_decay(master, daily_node_ids, _make_edge_key_set(daily))

    # --- 5. æ„Ÿæƒ…ã® active_sentiment æ¸›è¡° ---
    today_str = datetime.now().strftime("%Y-%m-%d")
    apply_emotion_decay(master, daily_node_ids, today_str)

    return master

def main():
    parser = argparse.ArgumentParser(description="Merge a daily knowledge graph into the master graph.")
    parser.add_argument("--master", help="Path to master graph JSON", default="master_graph.json")
    parser.add_argument("--daily", help="Path to daily graph JSON", required=True)
    parser.add_argument("--output", help="Path to output master graph JSON (defaults to overwriting master)", default=None)
    
    args = parser.parse_args()
    
    output_path = args.output if args.output else args.master
    
    print(f"ğŸ“‚ Loading Master: {args.master}")
    master_graph = load_graph(args.master)
    
    print(f"ğŸ“‚ Loading Daily:  {args.daily}")
    daily_graph = load_graph(args.daily)
    
    updated_master = merge_graphs(master_graph, daily_graph)
    
    # Ensure directory exists for output
    output_dir = os.path.dirname(os.path.abspath(output_path))
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(updated_master, f, indent=2, ensure_ascii=False)
        
    print(f"ğŸ’¾ Saved updated master graph to: {output_path}")

if __name__ == "__main__":
    main()
