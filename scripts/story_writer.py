"""
story_writer.py â€” ãƒãƒ¡ãƒ©è‰æ¡ˆ â†’ ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ç”Ÿæˆ

ãƒãƒ¡ãƒ©ã§æ›¸ã„ãŸè‰æ¡ˆã‚’ã‚‚ã¨ã«ã€æ¶ç©ºã®ç™»å ´äººç‰©ã«ã‚ˆã‚‹
1è©±å®Œçµã®ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ã‚’Gemini APIã§ç”Ÿæˆã™ã‚‹ã€‚
ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã¯ãƒ†ãƒ¼ãƒã®ç€æƒ³æºã¨ã—ã¦ã®ã¿ä½¿ç”¨ã—ã€å€‹äººæƒ…å ±ã¯å‡ºåŠ›ã—ãªã„ã€‚
ç”Ÿæˆå¾Œã€hatena_publisher.py ã‚’å‘¼ã³å‡ºã—ã¦ã¯ã¦ãªãƒ–ãƒ­ã‚°ã«ä¸‹æ›¸ãæŠ•ç¨¿ã™ã‚‹ã€‚

ä»¶åã«ã€ŒSTORYã€ã‚’å«ã‚€ãƒ¡ãƒ¼ãƒ«ã‹ã‚‰ç™ºç«ã™ã‚‹ã€‚
"""

import os
import json
import re
import argparse
import subprocess
from datetime import datetime
import requests
from typing import Dict, Any, Optional, Tuple

# è¨­å®š
API_KEY = os.getenv("GOOGLE_API_KEY")
MASTER_GRAPH_PATH = "knowledge_graph.jsonld"
BLOG_READY_DIR = "blog_ready"
HATENA_PUBLISHER_SCRIPT = "scripts/hatena_publisher.py"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FICTION_SYSTEM_PROMPT = """
# å½¹å‰²
ã‚ãªãŸã¯æ˜Ÿæ–°ä¸€ã®ã‚ˆã†ãªã‚·ãƒ§ãƒ¼ãƒˆã‚·ãƒ§ãƒ¼ãƒˆã®åæ‰‹ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæä¾›ã™ã‚‹ã€Œãƒ†ãƒ¼ãƒã€ã€Œæ„Ÿæƒ…ã€ã€Œæ•™è¨“ã€ã‚’ã‚‚ã¨ã«ã€
**å®Œå…¨ãªãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³ã®1è©±å®ŒçµçŸ­ç·¨å°èª¬**ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚

# çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«
1. ç™»å ´äººç‰©ã¯å…¨å“¡æ¶ç©ºã®äººç‰©ã«ã™ã‚‹ã€‚å®Ÿåœ¨ã®äººåãƒ»ä¼æ¥­åãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã€‚
2. å…·ä½“çš„ãªæ—¥ä»˜ãƒ»ä½æ‰€ãƒ»é‡‘é¡ãƒ»çµ„ç¹”åãªã©ã€å€‹äººã‚’ç‰¹å®šã§ãã‚‹æƒ…å ±ã¯æ›¸ã‹ãªã„ã€‚
3. ã€Œãƒ†ãƒ¼ãƒã®ãƒ’ãƒ³ãƒˆã€ã¯ã‚ãã¾ã§ç€æƒ³ã®ç´ æã§ã‚ã‚Šã€äº‹å®Ÿã¨ã—ã¦ãã®ã¾ã¾æ›¸ã‹ãªã„ã€‚
4. ã“ã‚Œã¯ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šã€å®Ÿè©±ã§ã¯ãªã„ã€‚èª­è€…ã‚‚ãã†å—ã‘å–ã‚Œã‚‹æ–‡ä½“ã«ã™ã‚‹ã€‚

# ç‰©èªã®æ§‹æˆ â€” æ„å¤–æ€§ã¯å¿…é ˆ
ç‰©èªã‚’ã€Œãƒ†ãƒ¼ãƒ â†’ ãƒ†ãƒ¼ãƒã®æ·±åŒ– â†’ ãƒ†ãƒ¼ãƒã®ç€åœ°ã€ã§ä¸€ç›´ç·šã«æ›¸ã„ã¦ã¯ãªã‚‰ãªã„ã€‚
èª­è€…ãŒé€”ä¸­ã§çµæœ«ã‚’äºˆæ¸¬ã§ãã‚‹å±•é–‹ã¯å¤±æ•—ã§ã‚ã‚‹ã€‚
ä»¥ä¸‹ã®æ§‹æˆã‚’å¿…ãšå®ˆã‚‹ã“ã¨:

1. **å°å…¥ï¼ˆãƒ•ãƒƒã‚¯ï¼‰**: èª­è€…ã‚’å¼•ãè¾¼ã‚€å…·ä½“çš„ãªã‚·ãƒ¼ãƒ³ã€‚äº”æ„Ÿã§æãã€‚
   ã“ã“ã§èª­è€…ã«ã€Œã“ã®è©±ã¯ã“ã†ã„ã†æ–¹å‘ã ã‚ã†ã€ã¨æ€ã‚ã›ã‚‹ã€‚
2. **æ·±åŒ–ï¼ˆãƒŸã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼‰**: ç‰©èªã®æœ¬ç­‹ã«è¦‹ãˆã‚‹å±•é–‹ã‚’ä¸å¯§ã«æãã€‚
   èª­è€…ã®äºˆæƒ³ã‚’ç¢ºä¿¡ã«è¿‘ã¥ã‘ã•ã›ã‚‹ã€‚ã“ã“ãŒç½ ã§ã‚ã‚‹ã€‚
3. **è»¢è¦†ï¼ˆã²ã£ãã‚Šè¿”ã—ï¼‰**: ç‰©èªã®å¾ŒåŠã§ã€èª­è€…ã®äºˆæƒ³ã‚’è£åˆ‡ã‚‹å±•é–‹ã‚’å…¥ã‚Œã‚‹ã€‚
   ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æŠ€æ³•ã‚’ä½¿ã†ã“ã¨:
   - **è¦–ç‚¹ã®ã‚ºãƒ©ã—**: å®Ÿã¯ä¸»äººå…¬ãŒæ€ã£ã¦ã„ãŸçŠ¶æ³ã¨ç¾å®ŸãŒé•ã£ãŸ
   - **ä¸»å®¢é€†è»¢**: åŠ©ã‘ã‚‹å´ã¨åŠ©ã‘ã‚‰ã‚Œã‚‹å´ã€è¦³å¯Ÿè€…ã¨è¢«è¦³å¯Ÿè€…ãŒé€†ã ã£ãŸ
   - **æ„å‘³ã®åè»¢**: ãšã£ã¨å¦å®šçš„ã«æã„ã¦ã„ãŸã‚‚ã®ãŒå®Ÿã¯è‚¯å®šçš„ã ã£ãŸã€ã¾ãŸã¯ãã®é€†
   - **ç‰©èªã®å…¥ã‚Œå­**: ç‰©èªå†…ã®å‡ºæ¥äº‹ãŒã€ã‚‚ã†ä¸€æ®µä¸Šã®æ§‹é€ ã§åˆ¥ã®æ„å‘³ã‚’æŒã¤
   - **æ™‚é–“ã®ãƒˆãƒªãƒƒã‚¯**: é †åºã‚„å› æœãŒèª­è€…ã®èªè­˜ã¨ç•°ãªã£ã¦ã„ãŸ
   - **ä¸æ¡ç†ãªå¸°çµ**: è«–ç†çš„ãªã®ã«äºˆæƒ³å¤–ã®çµè«–ã«è‡³ã‚‹
4. **è½ã¨ã—ï¼ˆä½™éŸ»ï¼‰**: çŸ­ãã€ä¹¾ã„ãŸä½™éŸ»ã§ç· ã‚ã‚‹ã€‚æœ€å¾Œã®ä¸€æ–‡ã§ä¸–ç•Œã®è¦‹ãˆæ–¹ãŒå¤‰ã‚ã‚‹ã€‚

# çµ¶å¯¾ã«é¿ã‘ã‚‹ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³
- ãƒ†ãƒ¼ãƒã®æ¯”å–©ãŒå†’é ­ã‹ã‚‰çµæœ«ã¾ã§ä¸€è²«ã—ã¦åŒã˜æ–¹å‘ã«é€²ã‚€ã“ã¨ï¼ˆä¾‹: æ±šã‚Œâ†’å¿ƒã®å‚·â†’æ™‚é–“ãŒç™’ã™ï¼‰
- ä¸»äººå…¬ãŒé€”ä¸­ã§ã€Œæ°—ã¥ãã€ã‚’å¾—ã¦ã€ãã®ã¾ã¾ç©ã‚„ã‹ã«çµ‚ã‚ã‚‹ã“ã¨
- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œä¸­äººç‰©ã®å†…çœã¨ã—ã¦ãã®ã¾ã¾èªã‚‰ã›ã‚‹ã“ã¨
- ã€Œæ•™è¨“ã€ã‚’çµæœ«ã®ç›´å‰ã§åœ°ã®æ–‡ã¨ã—ã¦èª¬æ˜ã™ã‚‹ã“ã¨

# ç‰©èªã®å“è³ªãƒ«ãƒ¼ãƒ«
ä»¥ä¸‹ã¯ã€èª­è€…ãŒã€Œå¾®å¦™ã ãªã€ã¨æ„Ÿã˜ã‚‹åŸå› ã«ãªã‚‹å…¸å‹çš„ãªå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚ã‚‹ã€‚å¿…ãšå›é¿ã™ã‚‹ã“ã¨ã€‚

## å±•é–‹ã®è‡ªç„¶ã•
- ç¾å®Ÿçš„ãªè¡Œå‹•ã¨éç¾å®Ÿçš„ãªè¡Œå‹•ã®é–“ã«ã€ååˆ†ãªã€Œæ©‹ã€ã‚’æ¶ã‘ã‚‹ã“ã¨ã€‚
  ãŸã¨ãˆã°ã€Œç”·ãŒå‰¥é›¢å‰¤ã‚’è‡ªåˆ†ã®ä½“ã«ä½¿ã„å§‹ã‚ãŸã€ã®ã‚ˆã†ãªé£›èºã¯ã€
  ãã®è¡Œç‚ºã«è‡³ã‚‹å¿ƒç†çš„ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¤‡æ•°ã®ã‚¹ãƒ†ãƒƒãƒ—ã§æå†™ã—ãªã‘ã‚Œã°ä¸è‡ªç„¶ã«ãªã‚‹ã€‚
- ç•°å¸¸ãªè¡Œå‹•ã‚’æãå ´åˆã€ã¾ãšæ—¥å¸¸ã®å°ã•ãªé€¸è„±ã‹ã‚‰å§‹ã‚ã€æ®µéšçš„ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ãƒˆã•ã›ã‚‹ã€‚

## ä¼ç·šã®æ¤ãˆæ–¹
- çµæœ«ã§é‡è¦ã«ãªã‚‹è¦ç´ ã¯ã€ç‰©èªã®å‰åŠã§ã€Œåˆ¥ã®æ„å‘³ã€ã¨ã—ã¦è‡ªç„¶ã«ç™»å ´ã•ã›ã¦ãŠãã“ã¨ã€‚
- ã€Œèª­ã¿è¿”ã™ã¨ã€ã‚ã®æå†™ã¯ä¼ç·šã ã£ãŸã®ã‹ã€ã¨æ°—ã¥ã‹ã›ã‚‹ã®ãŒç†æƒ³ã€‚
- ä¼ç·šã‚’æ¤ãˆãšã«å¾ŒåŠã§å”çªã«æ–°è¦ç´ ã‚’å‡ºã™ã®ã¯ç¦æ­¢ã€‚

## ã‚ªãƒã®è¦‹ã›æ–¹
- è»¢è¦†ã®ç¬é–“ã¯ã€ç™»å ´äººç‰©ã®ã‚»ãƒªãƒ•ã§èª¬æ˜ã—ã¦ã¯ãªã‚‰ãªã„ã€‚
  ç‰¹ã«ã€Œå®Ÿã¯ã“ã†ã ã£ãŸã®ã ã€ã¨ç¨®æ˜ã‹ã—ã™ã‚‹ã‚»ãƒªãƒ•ã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„ã€‚
- ç¨®æ˜ã‹ã—ã¯ã€èª­è€…ãŒç›®æ’ƒã™ã‚‹ã€Œå‡ºæ¥äº‹ã€ã‚„ã€Œæå†™ã€ã§ä¼ãˆã‚‹ã€‚
  ä½œæ¥­å“¡ãŒã€Œã“ã‚Œã¯çŸ¥çš„ç”Ÿå‘½ä½“å‹ç²˜ç€æ±šã‚Œã ã€ã¨ç”¨èªã§èª¬æ˜ã™ã‚‹ã®ã¯æœ€æ‚ªã®ä¾‹ã€‚
  ä»£ã‚ã‚Šã«ã€ä½œæ¥­å“¡ãŒã‚¹ã‚¯ãƒ¬ãƒ¼ãƒ‘ãƒ¼ã§ã€Œç”·ã€ã‚’å‰¥ãŒã™å‹•ä½œã‚’æå†™ã—ã€èª­è€…ã«æ‚Ÿã‚‰ã›ã‚‹ã€‚
- ã‚ªãƒã®ã‚ã¨ã«ç™»å ´äººç‰©ãŒæ„å‘³ã‚’è§£èª¬ã™ã‚‹ã‚»ãƒªãƒ•ã‚’å…¥ã‚Œãªã„ã€‚ä½™éŸ»ã¯æ²ˆé»™ã§ä½œã‚‹ã€‚
- ç‰©èªã®é€”ä¸­ã§ã‚‚ã€ç™»å ´äººç‰©ã«ãƒ†ãƒ¼ãƒã®æ¯”å–©çš„æ„å‘³ã‚’ä»£å¼ã•ã›ã¦ã¯ãªã‚‰ãªã„ã€‚
  ãŸã¨ãˆã°ã€Œã“ã‚Œã¯æº¶å‰¤ã§ã¯ãªã„ã€‚æ™‚é–“ã‚’å‡ç¸®ã—ãŸã‚‚ã®ã ã€ã®ã‚ˆã†ãªã€
  é“å…·ã‚„å‡ºæ¥äº‹ã®è±¡å¾´çš„æ„å‘³ã‚’ç™»å ´äººç‰©ãŒç›´æ¥èªã‚‹ã‚»ãƒªãƒ•ã¯ç¦æ­¢ã€‚
  ç™»å ´äººç‰©ã¯å…·ä½“çš„ãªæŒ‡ç¤ºã‚„äº‹å®Ÿã ã‘ã‚’èªã‚Šã€æ„å‘³ã¥ã‘ã¯èª­è€…ã«å§”ã­ã‚‹ã€‚

## æ§‹é€ ã®æ˜å¿«ã•
- ã©ã‚Œã ã‘è¤‡é›‘ãªä»•æ›ã‘ã§ã‚‚ã€èª­è€…ãŒã€Œä½•ãŒèµ·ããŸã‹ã€ã‚’ç†è§£ã§ããªã‘ã‚Œã°å¤±æ•—ã€‚
- ä¸»å®¢é€†è»¢ã‚’ä½¿ã†å ´åˆã€é€†è»¢ã®ã‚ã¨ã®ä¸–ç•ŒãŒçŸ›ç›¾ãªãæˆç«‹ã™ã‚‹ã‚ˆã†æ³¨æ„ã™ã‚‹ã€‚
- å†’é ­ã®çŠ¶æ³æå†™ãŒã€è»¢è¦†å¾Œã«ã€Œãã†ã„ã†ã“ã¨ã ã£ãŸã®ã‹ã€ã¨å†è§£é‡ˆã§ãã‚‹ä¸€è²«æ€§ã‚’ä¿ã¤ã“ã¨ã€‚

# æ–‡ä½“ã®ãƒ«ãƒ¼ãƒ«
- æ˜Ÿæ–°ä¸€ã®ã‚·ãƒ§ãƒ¼ãƒˆã‚·ãƒ§ãƒ¼ãƒˆã®ã‚ˆã†ã«æ›¸ãã€‚æ·¡ç™½ãªèªã‚Šå£ã«æ¯’ã‚„çš®è‚‰ã‚’å¿ã°ã›ã‚‹ã€‚
- ç™»å ´äººç‰©ã«ã¯å›ºæœ‰åè©ã®åå‰ã‚’ã¤ã‘ãªã„ã€‚ã€Œç§ã€ã€Œåƒ•ã€ã€Œå½¼ã€ã€Œå½¼å¥³ã€ã€Œç”·ã€ã€Œå¥³ã€ã€Œãã®äººã€ã®ã‚ˆã†ã«å‘¼ã¶ã€‚
- èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ãªãƒˆãƒ¼ãƒ³ã€‚å …ã™ããšã€å´©ã—ã™ããšã€‚
- çŸ­ã„æ–‡ã¨é•·ã„æ–‡ã‚’æ··ãœã¦ãƒªã‚ºãƒ ã‚’ä½œã‚‹ã€‚
- ã€Œä¼ãˆãŸã„ã“ã¨ã€ã¯çµ¶å¯¾ã«ç›´æ¥æ›¸ã‹ãªã„ã€‚ç‰©èªã®æ§‹é€ ã§èª­è€…ã«æ°—ã¥ã‹ã›ã‚‹ã€‚
- ä¸»äººå…¬ã«æ‚Ÿã‚‰ã›ã‚‹ã®ã§ã¯ãªãã€èª­è€…ã«æ‚Ÿã‚‰ã›ã‚‹ã€‚
- ç™»å ´äººç‰©ã®ã‚»ãƒªãƒ•ã¯æœ€å°é™ã«ã™ã‚‹ã€‚èªã‚Šã™ãã‚‹ç™»å ´äººç‰©ã¯ç‰©èªã‚’æ®ºã™ã€‚
- ç™»å ´äººç‰©ã¯ãƒ†ãƒ¼ãƒã®ä»£å¼è€…ã«ã—ãªã„ã€‚å“²å­¦çš„ãƒ»è©©çš„ãªæ ¼è¨€ã‚’è¨€ã‚ã›ãªã„ã€‚
  ç™»å ´äººç‰©ã®è¨€è‘‰ã¯ã€ãã®äººç‰©ãŒå®Ÿéš›ã«ãã®å ´ã§è¨€ã†ã§ã‚ã‚ã†è‡ªç„¶ãªã‚»ãƒªãƒ•ã ã‘ã«ã™ã‚‹ã€‚
- å°‚é–€çš„ãªæ¦‚å¿µã¯ã€ç‰©èªã®ä¸­ã§è‡ªç„¶ã«ä¼ã‚ã‚‹ã‚ˆã†ã«æå†™ã™ã‚‹ã€‚
- ãƒã‚¦ãƒ„ãƒ¼è¨˜äº‹ã«ã—ãªã„ã€‚ã‚ãã¾ã§ã€Œèª­ã¿ç‰©ã€ã¨ã—ã¦æ¥½ã—ã‚ã‚‹ã‚‚ã®ã«ã€‚
- è£½å“åã‚„ã‚µãƒ¼ãƒ“ã‚¹åã‚‚å›ºæœ‰åè©ã‚’ä½¿ã‚ãšã€ä¸€èˆ¬çš„ãªè¡¨ç¾ã«ç½®ãæ›ãˆã‚‹ã€‚

# å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:

{
  "title": "å°èª¬ã®ã‚¿ã‚¤ãƒˆãƒ«",
  "body": "æœ¬æ–‡ï¼ˆã¯ã¦ãªãƒ–ãƒ­ã‚°ç”¨Markdownå½¢å¼ï¼‰",
  "description": "ãƒ¡ã‚¿ãƒ‡ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆ120æ–‡å­—ä»¥å†…ã®ç‰©èªæ¦‚è¦ï¼‰",
  "categories": ["çŸ­ç·¨å°èª¬", "ã‚«ãƒ†ã‚´ãƒª2"],
  "estimated_read_time": "â—‹åˆ†"
}

# é‡è¦ãªæ³¨æ„äº‹é …
- è‰æ¡ˆã®æ ¸å¿ƒãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚„ãƒ†ãƒ¼ãƒã¯å¿…ãšç‰©èªã«åæ˜ ã•ã›ã‚‹ã€‚ãŸã ã—åæ˜ ã®ä»•æ–¹ã«æ„å¤–æ€§ã‚’æŒãŸã›ã‚‹ã€‚
- è¨˜äº‹ã®é•·ã•ã¯2000ã€œ4000æ–‡å­—ç¨‹åº¦ã€‚èª­ã¿åˆ‡ã‚Œã‚‹é•·ã•ã€‚
- è¦‹å‡ºã—ã¯æœ¬æ–‡ä¸­ã«2ã€œ3å€‹ã€‚å¤šã™ããªã„ã€‚
- ã¯ã¦ãªãƒ–ãƒ­ã‚°ã®Markdownå½¢å¼ã«å¾“ã†ã€‚

è¨€èª: æ—¥æœ¬èªã€‚
JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""

REVIEW_PROMPT = """
ã‚ãªãŸã¯å°èª¬ã®ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®çŸ­ç·¨å°èª¬ã‚’èª­ã¿ã€å“è³ªã‚’å³ã—ããƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

## ãƒã‚§ãƒƒã‚¯é …ç›®
ä»¥ä¸‹ã®å„é …ç›®ã«ã¤ã„ã¦ã€å•é¡ŒãŒã‚ã‚Œã°å…·ä½“çš„ã«æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚

1. **å±•é–‹ã®é£›èº**: ç¾å®Ÿçš„ãªè¡Œå‹•ã‹ã‚‰éç¾å®Ÿçš„ãªè¡Œå‹•ã¸ã®ç§»è¡ŒãŒå”çªã§ã¯ãªã„ã‹ã€‚
   å¿ƒç†çš„ãªæ®µéšãŒçœç•¥ã•ã‚Œã¦ã„ãªã„ã‹ã€‚
2. **ä¼ç·šã®æ©Ÿèƒ½**: çµæœ«ã§é‡è¦ã«ãªã‚‹è¦ç´ ãŒã€å‰åŠã«è‡ªç„¶ãªå½¢ã§ç™»å ´ã—ã¦ã„ã‚‹ã‹ã€‚
   å”çªãªæ–°è¦ç´ ãŒå¾ŒåŠã§å‡ºç¾ã—ã¦ã„ãªã„ã‹ã€‚
3. **èª¬æ˜çš„ã™ãã‚‹ã‚»ãƒªãƒ•**: ç™»å ´äººç‰©ãŒç‰©èªã®ä»•æ›ã‘ã‚„æ„å‘³ã‚’ç›´æ¥èª¬æ˜ã™ã‚‹ã‚»ãƒªãƒ•ã‚’è¨€ã£ã¦ã„ãªã„ã‹ã€‚
   ç‰¹ã«å°‚é–€ç”¨èªã‚„åˆ†é¡åã‚’ä½¿ã£ã¦ã€Œç¨®æ˜ã‹ã—ã€ã—ã¦ã„ãªã„ã‹ã€‚
4. **æ§‹é€ ã®æ˜å¿«ã•**: èª­è€…ãŒã€Œä½•ãŒèµ·ããŸã‹ã€ã‚’ç†è§£ã§ãã‚‹æ§‹é€ ã«ãªã£ã¦ã„ã‚‹ã‹ã€‚
   å†’é ­ã®æå†™ãŒã€è»¢è¦†å¾Œã«å†è§£é‡ˆã—ã¦çŸ›ç›¾ãŒãªã„ã‹ã€‚
5. **äºˆå®šèª¿å’Œ**: ãƒ†ãƒ¼ãƒã®æ¯”å–©ãŒæœ€åˆã‹ã‚‰æœ€å¾Œã¾ã§ä¸€ç›´ç·šã«é€²ã‚“ã§ã„ãªã„ã‹ã€‚

## å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:

{
  "passed": true/false,
  "issues": [
    {
      "type": "å±•é–‹ã®é£›èº",
      "detail": "å…·ä½“çš„ãªå•é¡Œç®‡æ‰€ã¨ç†ç”±",
      "suggestion": "æ”¹å–„æ¡ˆ"
    }
  ]
}

å•é¡ŒãŒãªã‘ã‚Œã° passed ã‚’ true ã«ã—ã€issues ã¯ç©ºé…åˆ—ã«ã—ã¦ãã ã•ã„ã€‚
1ã¤ã§ã‚‚å•é¡ŒãŒã‚ã‚Œã° passed ã‚’ false ã«ã—ã¦ãã ã•ã„ã€‚
JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def call_gemini_api(prompt: str, max_retries: int = 3) -> str:
    """Gemini APIã‚’å‘¼ã³å‡ºã™ã€‚"""
    if not API_KEY:
        raise ValueError("GOOGLE_API_KEY is not set.")

    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent"
    params = {"key": API_KEY}
    headers = {"Content-Type": "application/json"}

    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"response_mime_type": "application/json"}
    }

    for attempt in range(max_retries + 1):
        response = requests.post(url, headers=headers, json=data, params=params)

        if response.status_code == 200:
            break
        elif response.status_code == 429 and attempt < max_retries:
            wait_time = 30 * (2 ** attempt)
            print(f"â³ ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆåˆ°é”ã€‚{wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... ({attempt + 1}/{max_retries})")
            import time
            time.sleep(wait_time)
        else:
            raise Exception(f"API Error: {response.status_code} - {response.text}")

    result = response.json()
    try:
        if "candidates" in result and result["candidates"]:
            return result["candidates"][0]["content"]["parts"][0]["text"]
        else:
            raise Exception(f"Empty candidates in response: {result}")
    except (KeyError, IndexError):
        raise Exception(f"Unexpected API response format: {result}")


def load_knowledge_graph() -> Optional[Dict[str, Any]]:
    """ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    if not os.path.exists(MASTER_GRAPH_PATH):
        print("âš ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è‰æ¡ˆã®ã¿ã§è¨˜äº‹ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
        return None
    
    try:
        with open(MASTER_GRAPH_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        return None


def extract_relevant_context(graph: Dict[str, Any]) -> str:
    """ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰åŒ¿ååŒ–ã•ã‚ŒãŸãƒ†ãƒ¼ãƒãƒ’ãƒ³ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚
    
    å€‹äººã‚’ç‰¹å®šã§ãã‚‹æƒ…å ±ã¯é™¤å¤–ã—ã€ãƒ†ãƒ¼ãƒãƒ»æ„Ÿæƒ…ãƒ»æ•™è¨“ãƒ¬ãƒ™ãƒ«ã«æŠ½è±¡åŒ–ã™ã‚‹ã€‚
    """
    nodes = graph.get("nodes", [])
    
    # çŸ¥è¦‹ã‹ã‚‰ãƒ†ãƒ¼ãƒã‚’æŠ½å‡ºï¼ˆãƒ©ãƒ™ãƒ«ã®ã¿ã€è©³ç´°ã¯é™¤å¤–ï¼‰
    insights = [n for n in nodes if n.get("type") == "çŸ¥è¦‹"]
    insights_sorted = sorted(insights, key=lambda x: x.get("last_seen", ""), reverse=True)[:5]
    
    # æ„Ÿæƒ…ã®å‚¾å‘ã‚’æŠ½å‡ºï¼ˆå…·ä½“çš„ãªå†…å®¹ã¯é™¤å¤–ï¼‰
    emotions = [n for n in nodes if n.get("type") == "æ„Ÿæƒ…"]
    emotions_sorted = sorted(emotions, key=lambda x: x.get("last_seen", ""), reverse=True)[:3]
    
    context = "### ãƒ†ãƒ¼ãƒã®ãƒ’ãƒ³ãƒˆï¼ˆç€æƒ³ã®ç´ æã€‚äº‹å®Ÿã¨ã—ã¦ãã®ã¾ã¾ä½¿ã‚ãªã„ã“ã¨ï¼‰\n"
    
    if insights_sorted:
        context += "\n**æœ€è¿‘ã®æ°—ã¥ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:**\n"
        for i in insights_sorted:
            # ãƒ©ãƒ™ãƒ«ã®ã¿æ¸¡ã™ã€‚è©³ç´°ã‚„å›ºæœ‰åè©ã¯å«ã‚ãªã„
            label = i.get('label', '')
            context += f"- {label}\n"
    
    if emotions_sorted:
        context += "\n**æœ€è¿‘ã®æ„Ÿæƒ…ãƒˆãƒ¼ãƒ³:**\n"
        for e in emotions_sorted:
            sentiment = e.get('sentiment', 0)
            tone = "ãƒã‚¸ãƒ†ã‚£ãƒ–" if sentiment > 0 else "ãƒã‚¬ãƒ†ã‚£ãƒ–" if sentiment < 0 else "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«"
            context += f"- {tone}ãªæ„Ÿæƒ…\n"
    
    return context


def parse_draft_template(draft_text: str) -> Dict[str, str]:
    """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå½¢å¼ã®è‰æ¡ˆã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ã€‚
    
    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:
    - ãƒ†ãƒ¼ãƒ
    - ã‚¸ãƒ£ãƒ³ãƒ«
    - ãƒˆãƒ¼ãƒ³
    - ä¼ãˆãŸã„ã“ã¨
    - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
    - èª­å¾Œæ„Ÿ
    
    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ²¿ã£ã¦ã„ãªã„å ´åˆã¯ã€å…¨æ–‡ã‚’ã€Œã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€ã¨ã—ã¦æ‰±ã†ã€‚
    """
    fields = {
        "ãƒ†ãƒ¼ãƒ": "",
        "ã‚¸ãƒ£ãƒ³ãƒ«": "",
        "ãƒˆãƒ¼ãƒ³": "",
        "ä¼ãˆãŸã„ã“ã¨": "",
        "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰": "",
        "èª­å¾Œæ„Ÿ": ""
    }
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå½¢å¼ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    known_keys = list(fields.keys())
    has_template = any(f"{key}:" in draft_text or f"{key}ï¼š" in draft_text for key in known_keys[:3])
    
    if not has_template:
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãªã—: å…¨æ–‡ã‚’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã†
        fields["ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰"] = draft_text.strip()
        return fields
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹
    current_key = None
    current_lines = []
    
    for line in draft_text.split("\n"):
        stripped = line.strip()
        if not stripped:
            if current_key:
                current_lines.append("")
            continue
        
        # æ–°ã—ã„ã‚­ãƒ¼ã®æ¤œå‡º
        matched_key = None
        for key in known_keys:
            if stripped.startswith(f"{key}:") or stripped.startswith(f"{key}ï¼š"):
                matched_key = key
                break
        
        if matched_key:
            # å‰ã®ã‚­ãƒ¼ã®å†…å®¹ã‚’ä¿å­˜
            if current_key:
                fields[current_key] = "\n".join(current_lines).strip()
            current_key = matched_key
            # ã‚³ãƒ­ãƒ³ä»¥é™ã®å€¤ã‚’å–å¾—
            sep = "ï¼š" if f"{matched_key}ï¼š" in stripped else ":"
            value = stripped.split(sep, 1)[1].strip()
            current_lines = [value] if value else []
        else:
            current_lines.append(stripped)
    
    # æœ€å¾Œã®ã‚­ãƒ¼ã®å†…å®¹ã‚’ä¿å­˜
    if current_key:
        fields[current_key] = "\n".join(current_lines).strip()
    
    return fields


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def build_generation_prompt(fields: Dict[str, str], context: str) -> str:
    """ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿ç«‹ã¦ã‚‹ã€‚"""
    draft_section = "### ç‰©èªã®ç´ æ\n\n"
    
    if fields["ãƒ†ãƒ¼ãƒ"]:
        draft_section += f"**ãƒ†ãƒ¼ãƒ:** {fields['ãƒ†ãƒ¼ãƒ']}\n"
    if fields["ã‚¸ãƒ£ãƒ³ãƒ«"]:
        draft_section += f"**ã‚¸ãƒ£ãƒ³ãƒ«:** {fields['ã‚¸ãƒ£ãƒ³ãƒ«']}\n"
    if fields["ãƒˆãƒ¼ãƒ³"]:
        draft_section += f"**ãƒˆãƒ¼ãƒ³:** {fields['ãƒˆãƒ¼ãƒ³']}\n"
    if fields["ä¼ãˆãŸã„ã“ã¨"]:
        draft_section += f"\n**ç‰©èªã‚’é€šã˜ã¦ä¼ãˆãŸã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:**\n{fields['ä¼ãˆãŸã„ã“ã¨']}\n"
    if fields["ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰"]:
        draft_section += f"\n**ç‰©èªã®ç¨®ã«ãªã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã¯ã‚ãã¾ã§ç€æƒ³ã€‚ãã®ã¾ã¾ä½¿ã‚ãšã€æ¶ç©ºã®ç‰©èªã«å¤‰æ›ã™ã‚‹ã“ã¨ï¼‰:**\n{fields['ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰']}\n"
    if fields["èª­å¾Œæ„Ÿ"]:
        draft_section += f"\n**èª­å¾Œã«æ®‹ã—ãŸã„æ„Ÿæƒ…:** {fields['èª­å¾Œæ„Ÿ']}\n"
    
    return f"""
{FICTION_SYSTEM_PROMPT}

{context}

{draft_section}

### æŒ‡ç¤º
ä¸Šè¨˜ã®ç´ æã‚’ã‚‚ã¨ã«ã€å®Œå…¨ãªãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨å°èª¬ã‚’1æœ¬åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
ç™»å ´äººç‰©ã¯å…¨å“¡æ¶ç©ºã®äººç‰©ã«ã—ã€å®Ÿåœ¨ã®äººç‰©ãƒ»ä¼æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚
è‰æ¡ˆã®ãƒ†ãƒ¼ãƒã‚„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ç‰©èªã«åæ˜ ã•ã›ã¤ã¤ã‚‚ã€å€‹äººã‚’ç‰¹å®šã§ããªã„å‰µä½œç‰©ã«ã—ã¦ãã ã•ã„ã€‚
"""


def review_fiction(story_body: str) -> Tuple[bool, list]:
    """ç”Ÿæˆã•ã‚ŒãŸå°èª¬ã‚’è‡ªå·±ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã€‚"""
    prompt = f"""
{REVIEW_PROMPT}

### ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®å°èª¬
{story_body}
"""
    
    print("ğŸ” å“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­...")
    json_text = call_gemini_api(prompt)
    result = json.loads(json_text)
    passed = result.get("passed", True)
    issues = result.get("issues", [])
    
    if issues:
        for issue in issues:
            print(f"   âš ï¸ [{issue.get('type', 'ä¸æ˜')}] {issue.get('detail', '')}")
    else:
        print("   âœ… å“è³ªãƒã‚§ãƒƒã‚¯åˆæ ¼")
    
    return passed, issues


def generate_fiction(draft_text: str, context: str, max_revisions: int = 1) -> Dict[str, Any]:
    """è‰æ¡ˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    
    ç”Ÿæˆå¾Œã«è‡ªå·±ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡Œã„ã€å“è³ªã«å•é¡ŒãŒã‚ã‚Œã°
    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åæ˜ ã—ã¦å†ç”Ÿæˆã™ã‚‹ã€‚
    """
    
    # è‰æ¡ˆã‚’ãƒ‘ãƒ¼ã‚¹
    fields = parse_draft_template(draft_text)
    
    # åˆå›ç”Ÿæˆ
    prompt = build_generation_prompt(fields, context)
    print("ğŸ“ ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ã‚’ç”Ÿæˆä¸­...")
    json_text = call_gemini_api(prompt)
    essay_data = json.loads(json_text)
    
    # è‡ªå·±ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨å†ç”Ÿæˆãƒ«ãƒ¼ãƒ—
    for revision in range(max_revisions):
        story_body = essay_data.get("body", "")
        if not story_body:
            break
        
        passed, issues = review_fiction(story_body)
        
        if passed:
            break
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åæ˜ ã—ãŸå†ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        feedback_lines = []
        for issue in issues:
            feedback_lines.append(
                f"- [{issue.get('type', '')}] {issue.get('detail', '')}\n"
                f"  æ”¹å–„æ¡ˆ: {issue.get('suggestion', '')}"
            )
        feedback_section = "\n".join(feedback_lines)
        
        revision_prompt = f"""
{FICTION_SYSTEM_PROMPT}

{context}

{build_generation_prompt(fields, context)}

### å‰å›ã®ç”Ÿæˆçµæœã«å¯¾ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
ä»¥ä¸‹ã®å•é¡ŒãŒæŒ‡æ‘˜ã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã‚’å…¨ã¦ä¿®æ­£ã—ãŸä¸Šã§ã€å°èª¬ã‚’æ›¸ãç›´ã—ã¦ãã ã•ã„ã€‚

{feedback_section}

### å‰å›ã®æœ¬æ–‡ï¼ˆå‚è€ƒï¼‰
{story_body[:1000]}...

### æŒ‡ç¤º
ä¸Šè¨˜ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åæ˜ ã—ã€å•é¡Œã‚’ä¿®æ­£ã—ãŸæ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®çŸ­ç·¨å°èª¬ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
ç‰©èªã®éª¨æ ¼ã¯ç¶­æŒã—ã¤ã¤ã€æŒ‡æ‘˜ã•ã‚ŒãŸå“è³ªå•é¡Œã‚’è§£æ¶ˆã—ã¦ãã ã•ã„ã€‚
"""
        
        print(f"ğŸ“ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åæ˜ ç‰ˆã‚’ç”Ÿæˆä¸­... (ãƒªãƒ“ã‚¸ãƒ§ãƒ³ {revision + 1}/{max_revisions})")
        json_text = call_gemini_api(revision_prompt)
        essay_data = json.loads(json_text)
    
    return essay_data


def save_essay(essay_data: Dict[str, Any], source_file: str) -> tuple:
    """ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒƒã‚»ã‚¤ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚"""
    if not os.path.exists(BLOG_READY_DIR):
        os.makedirs(BLOG_READY_DIR)
    
    date_str = datetime.now().strftime('%Y%m%d')
    title = essay_data.get("title", "ç„¡é¡Œ")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    import re
    safe_title = re.sub(r'[\\/*?:"<>|]', '', title)[:50]
    
    # Markdownè¨˜äº‹ã®ä¿å­˜
    md_filename = f"{date_str}_{safe_title}.md"
    md_path = os.path.join(BLOG_READY_DIR, md_filename)
    
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(essay_data.get("body", ""))
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    meta_filename = f"{date_str}_{safe_title}.json"
    meta_path = os.path.join(BLOG_READY_DIR, meta_filename)
    
    meta = {
        "title": title,
        "description": essay_data.get("description", ""),
        "categories": essay_data.get("categories", []),
        "estimated_read_time": essay_data.get("estimated_read_time", ""),
        "source_file": source_file,
        "generated_at": datetime.now().isoformat()
    }
    
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ã‚¨ãƒƒã‚»ã‚¤ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {md_path}")
    print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {meta_path}")
    
    return md_path, meta_path


def publish_to_hatena(md_path: str, meta_path: str):
    """ã¯ã¦ãªãƒ–ãƒ­ã‚°ã«ä¸‹æ›¸ãæŠ•ç¨¿ã™ã‚‹ã€‚"""
    cmd = ["python3", HATENA_PUBLISHER_SCRIPT, md_path, "--meta", meta_path]
    print("ğŸš€ ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã‚’é–‹å§‹...")
    result = subprocess.run(cmd)
    if result.returncode != 0:
        print(f"âš ï¸ ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ (returncode={result.returncode})")
    else:
        print("âœ… ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ãŒå®Œäº†ã—ã¾ã—ãŸ")


def main():
    parser = argparse.ArgumentParser(description="ãƒãƒ¡ãƒ©è‰æ¡ˆ â†’ 1è©±å®Œçµã‚¨ãƒƒã‚»ã‚¤ â†’ ã¯ã¦ãªãƒ–ãƒ­ã‚°æŠ•ç¨¿")
    parser.add_argument("input_file", help="è‰æ¡ˆãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--skip-publish", action="store_true", help="ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--master-graph", default=MASTER_GRAPH_PATH, help="ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®ãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    # 1. è‰æ¡ˆã®èª­ã¿è¾¼ã¿
    try:
        import unicodedata
        args.input_file = unicodedata.normalize('NFC', args.input_file)
        with open(args.input_file, "r", encoding="utf-8") as f:
            draft_text = f.read()
    except FileNotFoundError:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.input_file}")
        return
    
    if not draft_text.strip():
        print("âŒ è‰æ¡ˆãŒç©ºã§ã™ã€‚")
        return
    
    print(f"ğŸ“„ è‰æ¡ˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {args.input_file}")
    print(f"   æ–‡å­—æ•°: {len(draft_text)}")
    
    # 2. ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    context = ""
    graph = load_knowledge_graph()
    if graph:
        context = extract_relevant_context(graph)
        print(f"ğŸ“Š ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    
    # 3. ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ã‚’ç”Ÿæˆ
    try:
        essay_data = generate_fiction(draft_text, context)
        print(f"âœ¨ çŸ­ç·¨ç”Ÿæˆå®Œäº†: ã€Œ{essay_data.get('title', 'ç„¡é¡Œ')}ã€")
    except Exception as e:
        print(f"âŒ çŸ­ç·¨ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # 4. ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    md_path, meta_path = save_essay(essay_data, args.input_file)
    
    # 5. ã¯ã¦ãªãƒ–ãƒ­ã‚°ã«æŠ•ç¨¿
    if not args.skip_publish:
        publish_to_hatena(md_path, meta_path)
    else:
        print("â­ï¸ ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")


if __name__ == "__main__":
    main()
