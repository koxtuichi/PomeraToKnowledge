"""
blog_writer.py â€” ãƒãƒ¡ãƒ©è‰æ¡ˆ â†’ ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ç”Ÿæˆ

ãƒãƒ¡ãƒ©ã§æ›¸ã„ãŸè‰æ¡ˆã‚’ã‚‚ã¨ã«ã€æ¶ç©ºã®ç™»å ´äººç‰©ã«ã‚ˆã‚‹
1è©±å®Œçµã®ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ã‚’Gemini APIã§ç”Ÿæˆã™ã‚‹ã€‚
ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã¯ãƒ†ãƒ¼ãƒã®ç€æƒ³æºã¨ã—ã¦ã®ã¿ä½¿ç”¨ã—ã€å€‹äººæƒ…å ±ã¯å‡ºåŠ›ã—ãªã„ã€‚
ç”Ÿæˆå¾Œã€hatena_publisher.py ã‚’å‘¼ã³å‡ºã—ã¦ã¯ã¦ãªãƒ–ãƒ­ã‚°ã«ä¸‹æ›¸ãæŠ•ç¨¿ã™ã‚‹ã€‚
"""

import os
import json
import re
import argparse
import subprocess
from datetime import datetime
import requests
from typing import Dict, Any, Optional, Tuple

# è¨­å®š
API_KEY = os.getenv("GOOGLE_API_KEY")
MASTER_GRAPH_PATH = "knowledge_graph.jsonld"
BLOG_READY_DIR = "blog_ready"
HATENA_PUBLISHER_SCRIPT = "scripts/hatena_publisher.py"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FICTION_SYSTEM_PROMPT = """
# å½¹å‰²
ã‚ãªãŸã¯æ˜Ÿæ–°ä¸€ã®ã‚ˆã†ãªã‚·ãƒ§ãƒ¼ãƒˆã‚·ãƒ§ãƒ¼ãƒˆã®åæ‰‹ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæä¾›ã™ã‚‹ã€Œãƒ†ãƒ¼ãƒã€ã€Œæ„Ÿæƒ…ã€ã€Œæ•™è¨“ã€ã‚’ã‚‚ã¨ã«ã€
**å®Œå…¨ãªãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³ã®1è©±å®ŒçµçŸ­ç·¨å°èª¬**ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚

# çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«
1. ç™»å ´äººç‰©ã¯å…¨å“¡æ¶ç©ºã®äººç‰©ã«ã™ã‚‹ã€‚å®Ÿåœ¨ã®äººåãƒ»ä¼æ¥­åãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã€‚
2. å…·ä½“çš„ãªæ—¥ä»˜ãƒ»ä½æ‰€ãƒ»é‡‘é¡ãƒ»çµ„ç¹”åãªã©ã€å€‹äººã‚’ç‰¹å®šã§ãã‚‹æƒ…å ±ã¯æ›¸ã‹ãªã„ã€‚
3. ã€Œãƒ†ãƒ¼ãƒã®ãƒ’ãƒ³ãƒˆã€ã¯ã‚ãã¾ã§ç€æƒ³ã®ç´ æã§ã‚ã‚Šã€äº‹å®Ÿã¨ã—ã¦ãã®ã¾ã¾æ›¸ã‹ãªã„ã€‚
4. ã“ã‚Œã¯ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šã€å®Ÿè©±ã§ã¯ãªã„ã€‚èª­è€…ã‚‚ãã†å—ã‘å–ã‚Œã‚‹æ–‡ä½“ã«ã™ã‚‹ã€‚

# ç‰©èªã®æ§‹æˆ â€” æ„å¤–æ€§ã¯å¿…é ˆ
ç‰©èªã‚’ã€Œãƒ†ãƒ¼ãƒ â†’ ãƒ†ãƒ¼ãƒã®æ·±åŒ– â†’ ãƒ†ãƒ¼ãƒã®ç€åœ°ã€ã§ä¸€ç›´ç·šã«æ›¸ã„ã¦ã¯ãªã‚‰ãªã„ã€‚
èª­è€…ãŒé€”ä¸­ã§çµæœ«ã‚’äºˆæ¸¬ã§ãã‚‹å±•é–‹ã¯å¤±æ•—ã§ã‚ã‚‹ã€‚
ä»¥ä¸‹ã®æ§‹æˆã‚’å¿…ãšå®ˆã‚‹ã“ã¨:

1. **å°å…¥ï¼ˆãƒ•ãƒƒã‚¯ï¼‰**: èª­è€…ã‚’å¼•ãè¾¼ã‚€å…·ä½“çš„ãªã‚·ãƒ¼ãƒ³ã€‚äº”æ„Ÿã§æãã€‚
   ã“ã“ã§èª­è€…ã«ã€Œã“ã®è©±ã¯ã“ã†ã„ã†æ–¹å‘ã ã‚ã†ã€ã¨æ€ã‚ã›ã‚‹ã€‚
2. **æ·±åŒ–ï¼ˆãƒŸã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼‰**: ç‰©èªã®æœ¬ç­‹ã«è¦‹ãˆã‚‹å±•é–‹ã‚’ä¸å¯§ã«æãã€‚
   èª­è€…ã®äºˆæƒ³ã‚’ç¢ºä¿¡ã«è¿‘ã¥ã‘ã•ã›ã‚‹ã€‚ã“ã“ãŒç½ ã§ã‚ã‚‹ã€‚
3. **è»¢è¦†ï¼ˆã²ã£ãã‚Šè¿”ã—ï¼‰**: ç‰©èªã®å¾ŒåŠã§ã€èª­è€…ã®äºˆæƒ³ã‚’è£åˆ‡ã‚‹å±•é–‹ã‚’å…¥ã‚Œã‚‹ã€‚
   ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æŠ€æ³•ã‚’ä½¿ã†ã“ã¨:
   - **è¦–ç‚¹ã®ã‚ºãƒ©ã—**: å®Ÿã¯ä¸»äººå…¬ãŒæ€ã£ã¦ã„ãŸçŠ¶æ³ã¨ç¾å®ŸãŒé•ã£ãŸ
   - **ä¸»å®¢é€†è»¢**: åŠ©ã‘ã‚‹å´ã¨åŠ©ã‘ã‚‰ã‚Œã‚‹å´ã€è¦³å¯Ÿè€…ã¨è¢«è¦³å¯Ÿè€…ãŒé€†ã ã£ãŸ
   - **æ„å‘³ã®åè»¢**: ãšã£ã¨å¦å®šçš„ã«æã„ã¦ã„ãŸã‚‚ã®ãŒå®Ÿã¯è‚¯å®šçš„ã ã£ãŸã€ã¾ãŸã¯ãã®é€†
   - **ç‰©èªã®å…¥ã‚Œå­**: ç‰©èªå†…ã®å‡ºæ¥äº‹ãŒã€ã‚‚ã†ä¸€æ®µä¸Šã®æ§‹é€ ã§åˆ¥ã®æ„å‘³ã‚’æŒã¤
   - **æ™‚é–“ã®ãƒˆãƒªãƒƒã‚¯**: é †åºã‚„å› æœãŒèª­è€…ã®èªè­˜ã¨ç•°ãªã£ã¦ã„ãŸ
   - **ä¸æ¡ç†ãªå¸°çµ**: è«–ç†çš„ãªã®ã«äºˆæƒ³å¤–ã®çµè«–ã«è‡³ã‚‹
4. **è½ã¨ã—ï¼ˆä½™éŸ»ï¼‰**: çŸ­ãã€ä¹¾ã„ãŸä½™éŸ»ã§ç· ã‚ã‚‹ã€‚æœ€å¾Œã®ä¸€æ–‡ã§ä¸–ç•Œã®è¦‹ãˆæ–¹ãŒå¤‰ã‚ã‚‹ã€‚

# çµ¶å¯¾ã«é¿ã‘ã‚‹ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³
- ãƒ†ãƒ¼ãƒã®æ¯”å–©ãŒå†’é ­ã‹ã‚‰çµæœ«ã¾ã§ä¸€è²«ã—ã¦åŒã˜æ–¹å‘ã«é€²ã‚€ã“ã¨ï¼ˆä¾‹: æ±šã‚Œâ†’å¿ƒã®å‚·â†’æ™‚é–“ãŒç™’ã™ï¼‰
- ä¸»äººå…¬ãŒé€”ä¸­ã§ã€Œæ°—ã¥ãã€ã‚’å¾—ã¦ã€ãã®ã¾ã¾ç©ã‚„ã‹ã«çµ‚ã‚ã‚‹ã“ã¨
- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œä¸­äººç‰©ã®å†…çœã¨ã—ã¦ãã®ã¾ã¾èªã‚‰ã›ã‚‹ã“ã¨
- ã€Œæ•™è¨“ã€ã‚’çµæœ«ã®ç›´å‰ã§åœ°ã®æ–‡ã¨ã—ã¦èª¬æ˜ã™ã‚‹ã“ã¨

# æ–‡ä½“ã®ãƒ«ãƒ¼ãƒ«
- æ˜Ÿæ–°ä¸€ã®ã‚·ãƒ§ãƒ¼ãƒˆã‚·ãƒ§ãƒ¼ãƒˆã®ã‚ˆã†ã«æ›¸ãã€‚æ·¡ç™½ãªèªã‚Šå£ã«æ¯’ã‚„çš®è‚‰ã‚’å¿ã°ã›ã‚‹ã€‚
- ç™»å ´äººç‰©ã«ã¯å›ºæœ‰åè©ã®åå‰ã‚’ã¤ã‘ãªã„ã€‚ã€Œç§ã€ã€Œåƒ•ã€ã€Œå½¼ã€ã€Œå½¼å¥³ã€ã€Œç”·ã€ã€Œå¥³ã€ã€Œãã®äººã€ã®ã‚ˆã†ã«å‘¼ã¶ã€‚
- èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ãªãƒˆãƒ¼ãƒ³ã€‚å …ã™ããšã€å´©ã—ã™ããšã€‚
- çŸ­ã„æ–‡ã¨é•·ã„æ–‡ã‚’æ··ãœã¦ãƒªã‚ºãƒ ã‚’ä½œã‚‹ã€‚
- ã€Œä¼ãˆãŸã„ã“ã¨ã€ã¯çµ¶å¯¾ã«ç›´æ¥æ›¸ã‹ãªã„ã€‚ç‰©èªã®æ§‹é€ ã§èª­è€…ã«æ°—ã¥ã‹ã›ã‚‹ã€‚
- ä¸»äººå…¬ã«æ‚Ÿã‚‰ã›ã‚‹ã®ã§ã¯ãªãã€èª­è€…ã«æ‚Ÿã‚‰ã›ã‚‹ã€‚
- å°‚é–€çš„ãªæ¦‚å¿µã¯ã€ç‰©èªã®ä¸­ã§è‡ªç„¶ã«ä¼ã‚ã‚‹ã‚ˆã†ã«æå†™ã™ã‚‹ã€‚
- ãƒã‚¦ãƒ„ãƒ¼è¨˜äº‹ã«ã—ãªã„ã€‚ã‚ãã¾ã§ã€Œèª­ã¿ç‰©ã€ã¨ã—ã¦æ¥½ã—ã‚ã‚‹ã‚‚ã®ã«ã€‚
- è£½å“åã‚„ã‚µãƒ¼ãƒ“ã‚¹åã‚‚å›ºæœ‰åè©ã‚’ä½¿ã‚ãšã€ä¸€èˆ¬çš„ãªè¡¨ç¾ã«ç½®ãæ›ãˆã‚‹ã€‚

# å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:

{
  "title": "å°èª¬ã®ã‚¿ã‚¤ãƒˆãƒ«",
  "body": "æœ¬æ–‡ï¼ˆã¯ã¦ãªãƒ–ãƒ­ã‚°ç”¨Markdownå½¢å¼ï¼‰",
  "description": "ãƒ¡ã‚¿ãƒ‡ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆ120æ–‡å­—ä»¥å†…ã®ç‰©èªæ¦‚è¦ï¼‰",
  "categories": ["çŸ­ç·¨å°èª¬", "ã‚«ãƒ†ã‚´ãƒª2"],
  "estimated_read_time": "â—‹åˆ†"
}

# é‡è¦ãªæ³¨æ„äº‹é …
- è‰æ¡ˆã®æ ¸å¿ƒãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚„ãƒ†ãƒ¼ãƒã¯å¿…ãšç‰©èªã«åæ˜ ã•ã›ã‚‹ã€‚ãŸã ã—åæ˜ ã®ä»•æ–¹ã«æ„å¤–æ€§ã‚’æŒãŸã›ã‚‹ã€‚
- è¨˜äº‹ã®é•·ã•ã¯2000ã€œ4000æ–‡å­—ç¨‹åº¦ã€‚èª­ã¿åˆ‡ã‚Œã‚‹é•·ã•ã€‚
- è¦‹å‡ºã—ã¯æœ¬æ–‡ä¸­ã«2ã€œ3å€‹ã€‚å¤šã™ããªã„ã€‚
- ã¯ã¦ãªãƒ–ãƒ­ã‚°ã®Markdownå½¢å¼ã«å¾“ã†ã€‚

è¨€èª: æ—¥æœ¬èªã€‚
JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def call_gemini_api(prompt: str, max_retries: int = 3) -> str:
    """Gemini APIã‚’å‘¼ã³å‡ºã™ã€‚"""
    if not API_KEY:
        raise ValueError("GOOGLE_API_KEY is not set.")

    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent"
    params = {"key": API_KEY}
    headers = {"Content-Type": "application/json"}

    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"response_mime_type": "application/json"}
    }

    for attempt in range(max_retries + 1):
        response = requests.post(url, headers=headers, json=data, params=params)

        if response.status_code == 200:
            break
        elif response.status_code == 429 and attempt < max_retries:
            wait_time = 30 * (2 ** attempt)
            print(f"â³ ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆåˆ°é”ã€‚{wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... ({attempt + 1}/{max_retries})")
            import time
            time.sleep(wait_time)
        else:
            raise Exception(f"API Error: {response.status_code} - {response.text}")

    result = response.json()
    try:
        if "candidates" in result and result["candidates"]:
            return result["candidates"][0]["content"]["parts"][0]["text"]
        else:
            raise Exception(f"Empty candidates in response: {result}")
    except (KeyError, IndexError):
        raise Exception(f"Unexpected API response format: {result}")


def load_knowledge_graph() -> Optional[Dict[str, Any]]:
    """ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    if not os.path.exists(MASTER_GRAPH_PATH):
        print("âš ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è‰æ¡ˆã®ã¿ã§è¨˜äº‹ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
        return None
    
    try:
        with open(MASTER_GRAPH_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        return None


def extract_relevant_context(graph: Dict[str, Any]) -> str:
    """ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰åŒ¿ååŒ–ã•ã‚ŒãŸãƒ†ãƒ¼ãƒãƒ’ãƒ³ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚
    
    å€‹äººã‚’ç‰¹å®šã§ãã‚‹æƒ…å ±ã¯é™¤å¤–ã—ã€ãƒ†ãƒ¼ãƒãƒ»æ„Ÿæƒ…ãƒ»æ•™è¨“ãƒ¬ãƒ™ãƒ«ã«æŠ½è±¡åŒ–ã™ã‚‹ã€‚
    """
    nodes = graph.get("nodes", [])
    
    # çŸ¥è¦‹ã‹ã‚‰ãƒ†ãƒ¼ãƒã‚’æŠ½å‡ºï¼ˆãƒ©ãƒ™ãƒ«ã®ã¿ã€è©³ç´°ã¯é™¤å¤–ï¼‰
    insights = [n for n in nodes if n.get("type") == "çŸ¥è¦‹"]
    insights_sorted = sorted(insights, key=lambda x: x.get("last_seen", ""), reverse=True)[:5]
    
    # æ„Ÿæƒ…ã®å‚¾å‘ã‚’æŠ½å‡ºï¼ˆå…·ä½“çš„ãªå†…å®¹ã¯é™¤å¤–ï¼‰
    emotions = [n for n in nodes if n.get("type") == "æ„Ÿæƒ…"]
    emotions_sorted = sorted(emotions, key=lambda x: x.get("last_seen", ""), reverse=True)[:3]
    
    context = "### ãƒ†ãƒ¼ãƒã®ãƒ’ãƒ³ãƒˆï¼ˆç€æƒ³ã®ç´ æã€‚äº‹å®Ÿã¨ã—ã¦ãã®ã¾ã¾ä½¿ã‚ãªã„ã“ã¨ï¼‰\n"
    
    if insights_sorted:
        context += "\n**æœ€è¿‘ã®æ°—ã¥ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:**\n"
        for i in insights_sorted:
            # ãƒ©ãƒ™ãƒ«ã®ã¿æ¸¡ã™ã€‚è©³ç´°ã‚„å›ºæœ‰åè©ã¯å«ã‚ãªã„
            label = i.get('label', '')
            context += f"- {label}\n"
    
    if emotions_sorted:
        context += "\n**æœ€è¿‘ã®æ„Ÿæƒ…ãƒˆãƒ¼ãƒ³:**\n"
        for e in emotions_sorted:
            sentiment = e.get('sentiment', 0)
            tone = "ãƒã‚¸ãƒ†ã‚£ãƒ–" if sentiment > 0 else "ãƒã‚¬ãƒ†ã‚£ãƒ–" if sentiment < 0 else "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«"
            context += f"- {tone}ãªæ„Ÿæƒ…\n"
    
    return context


def parse_draft_template(draft_text: str) -> Dict[str, str]:
    """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå½¢å¼ã®è‰æ¡ˆã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ã€‚
    
    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:
    - ãƒ†ãƒ¼ãƒ
    - ã‚¸ãƒ£ãƒ³ãƒ«
    - ãƒˆãƒ¼ãƒ³
    - ä¼ãˆãŸã„ã“ã¨
    - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
    - èª­å¾Œæ„Ÿ
    
    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ²¿ã£ã¦ã„ãªã„å ´åˆã¯ã€å…¨æ–‡ã‚’ã€Œã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€ã¨ã—ã¦æ‰±ã†ã€‚
    """
    fields = {
        "ãƒ†ãƒ¼ãƒ": "",
        "ã‚¸ãƒ£ãƒ³ãƒ«": "",
        "ãƒˆãƒ¼ãƒ³": "",
        "ä¼ãˆãŸã„ã“ã¨": "",
        "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰": "",
        "èª­å¾Œæ„Ÿ": ""
    }
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå½¢å¼ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    known_keys = list(fields.keys())
    has_template = any(f"{key}:" in draft_text or f"{key}ï¼š" in draft_text for key in known_keys[:3])
    
    if not has_template:
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãªã—: å…¨æ–‡ã‚’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã†
        fields["ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰"] = draft_text.strip()
        return fields
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹
    current_key = None
    current_lines = []
    
    for line in draft_text.split("\n"):
        stripped = line.strip()
        if not stripped:
            if current_key:
                current_lines.append("")
            continue
        
        # æ–°ã—ã„ã‚­ãƒ¼ã®æ¤œå‡º
        matched_key = None
        for key in known_keys:
            if stripped.startswith(f"{key}:") or stripped.startswith(f"{key}ï¼š"):
                matched_key = key
                break
        
        if matched_key:
            # å‰ã®ã‚­ãƒ¼ã®å†…å®¹ã‚’ä¿å­˜
            if current_key:
                fields[current_key] = "\n".join(current_lines).strip()
            current_key = matched_key
            # ã‚³ãƒ­ãƒ³ä»¥é™ã®å€¤ã‚’å–å¾—
            sep = "ï¼š" if f"{matched_key}ï¼š" in stripped else ":"
            value = stripped.split(sep, 1)[1].strip()
            current_lines = [value] if value else []
        else:
            current_lines.append(stripped)
    
    # æœ€å¾Œã®ã‚­ãƒ¼ã®å†…å®¹ã‚’ä¿å­˜
    if current_key:
        fields[current_key] = "\n".join(current_lines).strip()
    
    return fields


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def generate_fiction(draft_text: str, context: str) -> Dict[str, Any]:
    """è‰æ¡ˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    
    # è‰æ¡ˆã‚’ãƒ‘ãƒ¼ã‚¹
    fields = parse_draft_template(draft_text)
    
    # ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿ç«‹ã¦
    draft_section = "### ç‰©èªã®ç´ æ\n\n"
    
    if fields["ãƒ†ãƒ¼ãƒ"]:
        draft_section += f"**ãƒ†ãƒ¼ãƒ:** {fields['ãƒ†ãƒ¼ãƒ']}\n"
    if fields["ã‚¸ãƒ£ãƒ³ãƒ«"]:
        draft_section += f"**ã‚¸ãƒ£ãƒ³ãƒ«:** {fields['ã‚¸ãƒ£ãƒ³ãƒ«']}\n"
    if fields["ãƒˆãƒ¼ãƒ³"]:
        draft_section += f"**ãƒˆãƒ¼ãƒ³:** {fields['ãƒˆãƒ¼ãƒ³']}\n"
    if fields["ä¼ãˆãŸã„ã“ã¨"]:
        draft_section += f"\n**ç‰©èªã‚’é€šã˜ã¦ä¼ãˆãŸã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:**\n{fields['ä¼ãˆãŸã„ã“ã¨']}\n"
    if fields["ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰"]:
        draft_section += f"\n**ç‰©èªã®ç¨®ã«ãªã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã¯ã‚ãã¾ã§ç€æƒ³ã€‚ãã®ã¾ã¾ä½¿ã‚ãšã€æ¶ç©ºã®ç‰©èªã«å¤‰æ›ã™ã‚‹ã“ã¨ï¼‰:**\n{fields['ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰']}\n"
    if fields["èª­å¾Œæ„Ÿ"]:
        draft_section += f"\n**èª­å¾Œã«æ®‹ã—ãŸã„æ„Ÿæƒ…:** {fields['èª­å¾Œæ„Ÿ']}\n"
    
    prompt = f"""
{FICTION_SYSTEM_PROMPT}

{context}

{draft_section}

### æŒ‡ç¤º
ä¸Šè¨˜ã®ç´ æã‚’ã‚‚ã¨ã«ã€å®Œå…¨ãªãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨å°èª¬ã‚’1æœ¬åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
ç™»å ´äººç‰©ã¯å…¨å“¡æ¶ç©ºã®äººç‰©ã«ã—ã€å®Ÿåœ¨ã®äººç‰©ãƒ»ä¼æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚
è‰æ¡ˆã®ãƒ†ãƒ¼ãƒã‚„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ç‰©èªã«åæ˜ ã•ã›ã¤ã¤ã‚‚ã€å€‹äººã‚’ç‰¹å®šã§ããªã„å‰µä½œç‰©ã«ã—ã¦ãã ã•ã„ã€‚
"""
    
    print("ğŸ“ ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ã‚’ç”Ÿæˆä¸­...")
    json_text = call_gemini_api(prompt)
    return json.loads(json_text)


def save_essay(essay_data: Dict[str, Any], source_file: str) -> tuple:
    """ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒƒã‚»ã‚¤ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚"""
    if not os.path.exists(BLOG_READY_DIR):
        os.makedirs(BLOG_READY_DIR)
    
    date_str = datetime.now().strftime('%Y%m%d')
    title = essay_data.get("title", "ç„¡é¡Œ")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    import re
    safe_title = re.sub(r'[\\/*?:"<>|]', '', title)[:50]
    
    # Markdownè¨˜äº‹ã®ä¿å­˜
    md_filename = f"{date_str}_{safe_title}.md"
    md_path = os.path.join(BLOG_READY_DIR, md_filename)
    
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(essay_data.get("body", ""))
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    meta_filename = f"{date_str}_{safe_title}.json"
    meta_path = os.path.join(BLOG_READY_DIR, meta_filename)
    
    meta = {
        "title": title,
        "description": essay_data.get("description", ""),
        "categories": essay_data.get("categories", []),
        "estimated_read_time": essay_data.get("estimated_read_time", ""),
        "source_file": source_file,
        "generated_at": datetime.now().isoformat()
    }
    
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ã‚¨ãƒƒã‚»ã‚¤ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {md_path}")
    print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {meta_path}")
    
    return md_path, meta_path


def publish_to_hatena(md_path: str, meta_path: str):
    """ã¯ã¦ãªãƒ–ãƒ­ã‚°ã«ä¸‹æ›¸ãæŠ•ç¨¿ã™ã‚‹ã€‚"""
    cmd = ["python3", HATENA_PUBLISHER_SCRIPT, md_path, "--meta", meta_path]
    print("ğŸš€ ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã‚’é–‹å§‹...")
    result = subprocess.run(cmd)
    if result.returncode != 0:
        print(f"âš ï¸ ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ (returncode={result.returncode})")
    else:
        print("âœ… ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ãŒå®Œäº†ã—ã¾ã—ãŸ")


def main():
    parser = argparse.ArgumentParser(description="ãƒãƒ¡ãƒ©è‰æ¡ˆ â†’ 1è©±å®Œçµã‚¨ãƒƒã‚»ã‚¤ â†’ ã¯ã¦ãªãƒ–ãƒ­ã‚°æŠ•ç¨¿")
    parser.add_argument("input_file", help="è‰æ¡ˆãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--skip-publish", action="store_true", help="ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--master-graph", default=MASTER_GRAPH_PATH, help="ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®ãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    # 1. è‰æ¡ˆã®èª­ã¿è¾¼ã¿
    try:
        import unicodedata
        args.input_file = unicodedata.normalize('NFC', args.input_file)
        with open(args.input_file, "r", encoding="utf-8") as f:
            draft_text = f.read()
    except FileNotFoundError:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.input_file}")
        return
    
    if not draft_text.strip():
        print("âŒ è‰æ¡ˆãŒç©ºã§ã™ã€‚")
        return
    
    print(f"ğŸ“„ è‰æ¡ˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {args.input_file}")
    print(f"   æ–‡å­—æ•°: {len(draft_text)}")
    
    # 2. ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    context = ""
    graph = load_knowledge_graph()
    if graph:
        context = extract_relevant_context(graph)
        print(f"ğŸ“Š ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    
    # 3. ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨ã‚’ç”Ÿæˆ
    try:
        essay_data = generate_fiction(draft_text, context)
        print(f"âœ¨ çŸ­ç·¨ç”Ÿæˆå®Œäº†: ã€Œ{essay_data.get('title', 'ç„¡é¡Œ')}ã€")
    except Exception as e:
        print(f"âŒ çŸ­ç·¨ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # 4. ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    md_path, meta_path = save_essay(essay_data, args.input_file)
    
    # 5. ã¯ã¦ãªãƒ–ãƒ­ã‚°ã«æŠ•ç¨¿
    if not args.skip_publish:
        publish_to_hatena(md_path, meta_path)
    else:
        print("â­ï¸ ã¯ã¦ãªãƒ–ãƒ­ã‚°ã¸ã®æŠ•ç¨¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")


if __name__ == "__main__":
    main()
