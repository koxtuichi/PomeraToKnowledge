import json
import os
import sys
import argparse
from datetime import datetime, timedelta
import google.generativeai as genai
from typing import Dict, Any

# Local Import
try:
    import graph_merger
except ImportError:
    print("âš ï¸  graph_merger module not found. Persistence features will be limited.")
    sys.exit(1)

# Load environment variables
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    # Try alternate name just in case
    api_key = os.getenv("GEMINI_API_KEY")

if not api_key:
    print("âŒ GOOGLE_API_KEY not found in environment variables.")
    print("Please export GOOGLE_API_KEY='your_key_here'")
    sys.exit(1)

genai.configure(api_key=api_key)

MASTER_GRAPH_FILE = "master_graph.json"
WEEKLY_REPORT_FILE = "weekly_review_{date}.md"
HTML_FILE = "index.html"

SYSTEM_PROMPT = """
ã‚ãªãŸã¯ç†Ÿç·´ã—ãŸãƒ©ã‚¤ãƒ•ã‚³ãƒ¼ãƒå…¼ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ã‚ãªãŸã®ä»»å‹™ã¯ã€éå»1é€±é–“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ—¥è¨˜ã¨ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€ã€Œé€±æ¬¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆWeekly Reviewï¼‰ã€ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ã€‚

## å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
1. **æ—¥æ¬¡ã‚µãƒãƒªãƒ¼**: éå»1é€±é–“ã®æ—¥ã€…ã®åˆ†æã®è¦ç´„ã€‚
2. **ä¸»è¦ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£**: ã“ã®æœŸé–“ã«è¨€åŠã•ã‚ŒãŸé‡è¦ãªäººç‰©ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ç›®æ¨™ã€‚
3. **æ„Ÿæƒ…ãƒˆãƒ¬ãƒ³ãƒ‰**: æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®æ¨ç§»ã€‚

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (Markdown)
ä»¥ä¸‹ã®æ§‹æˆã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼ˆã™ã¹ã¦æ—¥æœ¬èªã§ï¼‰ã€‚

# é€±æ¬¡ãƒ¬ãƒ“ãƒ¥ãƒ¼: [é–‹å§‹æ—¥] - [çµ‚äº†æ—¥]

## 1. ğŸ“ˆ ãƒã‚¤ãƒ©ã‚¤ãƒˆã¨ãƒˆãƒ¬ãƒ³ãƒ‰
- **æ„Ÿæƒ…ã®æ¨ç§»**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ°—åˆ†ã¯ã©ã†å¤‰åŒ–ã—ã¾ã—ãŸã‹ï¼Ÿï¼ˆä¾‹ï¼šã€Œé€±ã®åˆã‚ã¯é«˜ã‹ã£ãŸãŒã€ä¸­é ƒã«Xã®å½±éŸ¿ã§è½ã¡è¾¼ã¿ã€é€±æœ«ã«å›å¾©ã—ãŸã€ï¼‰
- **ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆ**: ä»Šé€±æœ€ã‚‚å½±éŸ¿åŠ›ã®ã‚ã£ãŸ1ã€œ3ã¤ã®å‡ºæ¥äº‹ã€‚
- **æœ€å¤§ã®æˆæœ (Big Wins)**: ä½•ãŒæœ€å¤§ã®é”æˆã§ã—ãŸã‹ï¼Ÿ

## 2. ğŸ¯ ã‚´ãƒ¼ãƒ«é€²æ—åˆ†æ
- ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæˆ¦ç•¥ã¨ãã®çµæœã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¾ã™ã€‚
- **ç‰¹å®šã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚«ãƒ¼**: é€²æ—ã‚’å¦¨ã’ãŸç¹°ã‚Šè¿”ã•ã‚Œã‚‹èª²é¡Œã¯ä½•ã§ã™ã‹ï¼Ÿ
- **ç‰¹å®šã•ã‚ŒãŸãƒ–ãƒ¼ã‚¹ã‚¿ãƒ¼**: ä¸€è²«ã—ã¦é«˜ã„æ„Ÿæƒ…ã‚„ç”Ÿç”£æ€§ã‚’ã‚‚ãŸã‚‰ã—ãŸæ´»å‹•ã¯ä½•ã§ã™ã‹ï¼Ÿ

## 3. ğŸ’¡ ã‚¤ãƒ³ã‚µã‚¤ãƒˆã¨ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
- ä¸€è¦‹ç„¡é–¢ä¿‚ã«è¦‹ãˆã‚‹å‡ºæ¥äº‹ã®é–“ã®ç‚¹ã¨ç‚¹ã‚’ç¹‹ã’ã¦ãã ã•ã„ã€‚
- é »ç¹ã«ç¾ã‚ŒãŸã€ŒèªçŸ¥ã®æ­ªã¿ã€ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚

## 4. ğŸš€ æ¥é€±ã®æˆ¦ç•¥ (Future Action)
- æ¬¡é€±ã«å‘ã‘ãŸå…·ä½“çš„ãªãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã‚¨ãƒªã‚¢ã‚’3ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚
- è©¦ã™ã¹ãå…·ä½“çš„ãªã€Œè¡Œå‹•å¤‰å®¹ã€ã‚’1ã¤ææ¡ˆã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šã€Œå¤œã§ã¯ãªãæœã«æ›¸ãã€ãªã©ï¼‰ã€‚

## ãƒˆãƒ¼ãƒ³
æ”¯æŒçš„ã§ã€åˆ†æçš„ã‹ã¤å®¢è¦³çš„ã§ã‚ã‚‹ã“ã¨ã€‚èª­ã¿ã‚„ã™ãã™ã‚‹ãŸã‚ã«ç®‡æ¡æ›¸ãï¼ˆBullet Pointsï¼‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
"""

def load_master_graph():
    if not os.path.exists(MASTER_GRAPH_FILE):
        print(f"âŒ Master graph not found: {MASTER_GRAPH_FILE}")
        sys.exit(1)
    with open(MASTER_GRAPH_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def get_weekly_context(graph, target_date_str, days=7):
    target_date = datetime.strptime(target_date_str, "%Y-%m-%d")
    start_date = target_date - timedelta(days=days-1)
    
    print(f"ğŸ“… Analyzing range: {start_date.strftime('%Y-%m-%d')} to {target_date_str}")

    daily_nodes = []
    key_entities = []
    sentiments = []

    # 1. Filter Diary Nodes
    for node in graph.get("nodes", []):
        if node["type"] == "diary" and node.get("date"):
            try:
                node_date = datetime.strptime(node["date"], "%Y-%m-%d")
                if start_date <= node_date <= target_date:
                    daily_nodes.append(node)
                    if node.get("sentiment"):
                        sentiments.append(f"{node['date']}: {node['sentiment']}")
            except ValueError:
                continue

    # 2. Find Associated Entities (mentions in this period)
    # We look for edges connected to these diary nodes
    diary_ids = set(n["id"] for n in daily_nodes)
    related_node_ids = set()
    
    for edge in graph.get("edges", []):
        if edge["source"] in diary_ids:
            related_node_ids.add(edge["target"])
        elif edge["target"] in diary_ids:
            related_node_ids.add(edge["source"])

    for node in graph.get("nodes", []):
        if node["id"] in related_node_ids and node["type"] not in ["diary", "self"]:
            key_entities.append(f"- {node['label']} ({node['type']}): {node.get('detail', '')}")

    # Sort daily nodes by date
    daily_nodes.sort(key=lambda x: x["date"])

    context = f"## Period: {start_date.strftime('%Y-%m-%d')} to {target_date_str}\n\n"
    
    context += "## Daily Summaries\n"
    if not daily_nodes:
        context += "(No diary entries found for this period)\n"
    else:
        for n in daily_nodes:
            summary = n.get("analysis_content", "No analysis available").split("\n")[0:5] # Take first few lines
            context += f"### {n['date']} (Sentiment: {n.get('sentiment', 0)})\n"
            context += "Summary: " + "\n".join(summary) + "\n...\n\n"

    context += "## Key Entities Mentioned\n"
    context += "\n".join(key_entities[:20]) # Limit to top 20 to avoid context overflow
    
    context += "\n## Sentiment History\n"
    context += "\n".join(sentiments)

    return context, daily_nodes

def generate_report(context):
    model = genai.GenerativeModel('gemini-3-flash-preview')
    try:
        response = model.generate_content(SYSTEM_PROMPT + "\n\n" + context)
        return response.text
    except Exception as e:
        return f"Error generating report: {e}"

def update_html_visualization(html_path: str, graph_data: Dict[str, Any]):
    """graph_data.js ã® GRAPH_DATA ã‚’æ›´æ–°ã™ã‚‹ã€‚

    index.html æœ¬ä½“ã§ã¯ãªãã€åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® graph_data.js ã‚’æ›¸ãæ›ãˆã‚‹ã€‚
    """
    import re as _re
    js_path = os.path.join(os.path.dirname(os.path.abspath(html_path)), "graph_data.js")
    try:
        # æ›¸ãè¾¼ã¿å‰ã«åŸºæœ¬çš„ãªæ•´åˆæ€§ã‚’ç¢ºèª
        if not isinstance(graph_data.get("nodes"), list):
            raise ValueError("graph_data.nodes ãŒãƒªã‚¹ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“")

        new_content = (
            "// GRAPH_DATA_START\n"
            f"const GRAPH_DATA = {json.dumps(graph_data, ensure_ascii=False, indent=2)};\n"
            "// GRAPH_DATA_END\n"
        )

        with open(js_path, "w", encoding="utf-8") as f:
            f.write(new_content)

        # æ›¸ãè¾¼ã¿å¾Œã«å†èª­ã¿ã—ã¦ JSON ãƒ‘ãƒ¼ã‚¹ã‚’ç¢ºèª
        with open(js_path, "r", encoding="utf-8") as f:
            written = f.read()
        m = _re.search(r"const GRAPH_DATA = (\{.*\});", written, _re.DOTALL)
        if not m:
            raise ValueError("æ›¸ãè¾¼ã¿å¾Œã® graph_data.js ã‹ã‚‰ GRAPH_DATA ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“")
        json.loads(m.group(1))

        print(f"âœ… graph_data.js ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {len(graph_data['nodes'])} nodes")
    except Exception as e:
        print(f"âŒ graph_data.js æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")


def save_to_graph_and_visualize(report_text, target_date_str, daily_nodes, full_graph):
    # 1. Create Weekly Review Node
    review_node_id = f"review:weekly:{target_date_str}"
    
    # Check if exists (to update or create)
    # Actually graph_merger handles updates, so we just construct a partial graph
    
    weekly_graph = {
        "nodes": [{
            "id": review_node_id,
            "label": f"é€±æ¬¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ ({target_date_str})",
            "type": "review",
            "detail": "LLMã«ã‚ˆã‚‹é€±æ¬¡æŒ¯ã‚Šè¿”ã‚Šã¨æ¬¡é€±ã®ææ¡ˆ",
            "analysis_content": report_text,
            "date": target_date_str,
            "sentiment": 0.0, # Neutral container
            "community": 10, # Special community for reviews?
            "first_seen": datetime.now().isoformat(),
            "last_seen": datetime.now().isoformat(),
            "weight": 5 # Important node
        }],
        "edges": []
    }
    
    # 2. Link to Diary Nodes
    for diary in daily_nodes:
        weekly_graph["edges"].append({
            "source": review_node_id,
            "target": diary["id"],
            "type": "ABOUT",
            "label": "å¯¾è±¡æœŸé–“ã¨ã—ã¦é›†è¨ˆ",
            "weight": 1
        })
        
    # 3. Merge into Master
    print("ğŸ”„ Merging Weekly Review into Master Graph...")
    updated_master = graph_merger.merge_graphs(full_graph, weekly_graph)
    
    # 4. Save Master
    with open(MASTER_GRAPH_FILE, "w", encoding="utf-8") as f:
        json.dump(updated_master, f, indent=2, ensure_ascii=False)
        
    # 5. Update HTML
    update_html_visualization(HTML_FILE, updated_master)

def main():
    parser = argparse.ArgumentParser(description="Generate Weekly Review")
    parser.add_argument("--date", help="Target date (YYYY-MM-DD), defaults to today", default=datetime.now().strftime("%Y-%m-%d"))
    parser.add_argument("--days", type=int, default=7, help="Number of days to look back")
    parser.add_argument("--no-save", action="store_true", help="Do not save to master graph")
    args = parser.parse_args()

    graph = load_master_graph()
    context, daily_nodes = get_weekly_context(graph, args.date, args.days)
    
    print("ğŸ§  Generating Weekly Review...")
    report = generate_report(context)
    
    filename = WEEKLY_REPORT_FILE.format(date=args.date)
    with open(filename, "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"âœ… Report saved to: {filename}")
    
    if not args.no_save:
        save_to_graph_and_visualize(report, args.date, daily_nodes, graph)

if __name__ == "__main__":
    main()
