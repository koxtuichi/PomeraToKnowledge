import os
import json
import argparse
from datetime import datetime
import requests
from typing import Dict, Any, List

try:
    import graph_merger
except ImportError:
    print("âš ï¸  graph_merger module not found. Persistence features will be limited.")

# â”€â”€ è¨­å®š â”€â”€
API_KEY = os.getenv("GOOGLE_API_KEY")
ROLE_DEF_FILE = "role_definition.txt"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXTRACTION_SYSTEM_PROMPT = """
# å½¹å‰²
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œåˆ†èº«ã€ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒŠãƒ¬ãƒƒã‚¸ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚
æ—¥è¨˜ã‹ã‚‰ã€ã‚¿ã‚¹ã‚¯ç®¡ç†ã‚’é«˜åº¦åŒ–ã™ã‚‹ãŸã‚ã®ã€ŒçŸ¥è­˜ã‚°ãƒ©ãƒ•ã®å·®åˆ†ã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

# æŠ½å‡ºå¯¾è±¡ï¼ˆãƒãƒ¼ãƒ‰ã®ç¨®é¡ï¼‰

1. **ã‚¿ã‚¹ã‚¯**: å…·ä½“çš„ã€ã¾ãŸã¯æŠ½è±¡çš„ãªã€Œã‚„ã‚‹ã¹ãã“ã¨ã€ã€‚
   - status: "æœªç€æ‰‹" / "é€²è¡Œä¸­" / "å®Œäº†" / "ä¿ç•™"
2. **åˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰**: ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œã‚’å¦¨ã’ã‚‹è¦å› ã€‚
   - ç¨®é¡: "æ™‚é–“ä¸è¶³" / "ç–²åŠ´" / "æŠ€è¡“çš„èª²é¡Œ" / "æ„Ÿæƒ…çš„ãƒ–ãƒ¬ãƒ¼ã‚­" / "ç‰©ç†çš„éšœå®³" / "ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³" / "ãã®ä»–"
3. **çŸ¥è¦‹**: è©¦è¡ŒéŒ¯èª¤ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæ•™è¨“ã‚„ã€å°†æ¥ã®è³‡ç”£ã«ç¹‹ãŒã‚Šãã†ãªæ°—ã¥ãã€‚
4. **æ„Ÿæƒ…**: ãã®æ™‚ã®æ„Ÿæƒ…ã€‚ã‚¿ã‚¹ã‚¯ã®åŸå‹•åŠ›ã€ã¾ãŸã¯é˜»å®³è¦å› ã«ãªã‚‹ã€‚
   - sentiment: -1.0 ã‹ã‚‰ 1.0 ã®æ•°å€¤
5. **äººç‰©**: æ—¥è¨˜ã«ç™»å ´ã™ã‚‹äººã€‚
6. **å‡ºæ¥äº‹**: èµ·ããŸå…·ä½“çš„ãªã‚¤ãƒ™ãƒ³ãƒˆã€‚
   - status: "äºˆå®š" / "å®Œäº†" / "ä¸­æ­¢"
7. **ç›®æ¨™**: é•·æœŸçš„ã«ç›®æŒ‡ã™ã‚‚ã®ã€‚
   - status: "é€²è¡Œä¸­" / "é”æˆ" / "æ–­å¿µ"
8. **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: ç¶™ç¶šçš„ãªå–ã‚Šçµ„ã¿ã€‚
9. **æ¦‚å¿µ**: æŠ½è±¡çš„ãªæ¦‚å¿µã‚„çŠ¶æ…‹ã€‚
10. **å ´æ‰€**: å ´æ‰€ã€‚
11. **æ—¥è¨˜**: æ—¥è¨˜ã‚¨ãƒ³ãƒˆãƒªãã®ã‚‚ã®ã€‚

# é–¢ä¿‚æ€§ï¼ˆã‚¨ãƒƒã‚¸ã®ç¨®é¡ï¼‰

| é–¢ä¿‚å | æ–¹å‘ | æ„å‘³ |
|--------|------|------|
| é˜»å®³ã™ã‚‹ | åˆ¶ç´„ â†’ ã‚¿ã‚¹ã‚¯ | ã“ã®åˆ¶ç´„ãŒã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œã‚’å¦¨ã’ã¦ã„ã‚‹ |
| åŸå‹•åŠ›ã«ãªã‚‹ | æ„Ÿæƒ…/çŸ¥è¦‹ â†’ ã‚¿ã‚¹ã‚¯ | ã“ã®æ„Ÿæƒ…ã‚„çŸ¥è¦‹ãŒã‚¿ã‚¹ã‚¯ã‚’æ¨é€²ã™ã‚‹åŠ›ã«ãªã‚‹ |
| ä¸€éƒ¨ã§ã‚ã‚‹ | çŸ¥è¦‹ â†’ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ | ã“ã®çŸ¥è¦‹ãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€éƒ¨ã‚’æ§‹æˆã™ã‚‹ |
| è¨€åŠã™ã‚‹ | æ—¥è¨˜ â†’ å„ãƒãƒ¼ãƒ‰ | æ—¥è¨˜å†…ã§è§¦ã‚ŒãŸã‚‚ã® |
| å¼•ãèµ·ã“ã™ | å‡ºæ¥äº‹ â†’ æ„Ÿæƒ…/çŸ¥è¦‹ | å› æœé–¢ä¿‚ |
| å‚åŠ ã™ã‚‹ | äººç‰© â†’ å‡ºæ¥äº‹ | äººç‰©ãŒå‡ºæ¥äº‹ã«å‚åŠ  |
| å ´æ‰€ã§ | å‡ºæ¥äº‹ â†’ å ´æ‰€ | å‡ºæ¥äº‹ãŒèµ·ããŸå ´æ‰€ |
| å¯¾è±¡ã«ã™ã‚‹ | ã‚¿ã‚¹ã‚¯ â†’ äººç‰©/æ¦‚å¿µ | ã‚¿ã‚¹ã‚¯ã®å¯¾è±¡ |
| è¨ˆç”»ã™ã‚‹ | ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ â†’ ã‚¿ã‚¹ã‚¯ | ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å±ã™ã‚‹ã‚¿ã‚¹ã‚¯ |
| è§£æ±ºã™ã‚‹ | çŸ¥è¦‹/ã‚¿ã‚¹ã‚¯ â†’ åˆ¶ç´„ | åˆ¶ç´„ã‚’è§£æ¶ˆã™ã‚‹æ‰‹æ®µ |
| é–¢é€£ã™ã‚‹ | ä»»æ„ â†’ ä»»æ„ | ãã®ä»–ã®é–¢ä¿‚ |

# æŠ½å‡ºãƒ«ãƒ¼ãƒ«
1. **åˆ¶ç´„ã®æŠ½å‡ºã‚’é‡è¦–**: æ—¥è¨˜ã‹ã‚‰ã‚¿ã‚¹ã‚¯ã ã‘ã§ãªãã€ãã®ã‚¿ã‚¹ã‚¯ã‚’é˜»å®³ã—ã¦ã„ã‚‹ã€Œé‡åŠ›ã€ã‚’ç©æ¥µçš„ã«è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚
   - æ™‚é–“ãŒãªã„ â†’ åˆ¶ç´„ã€Œæ™‚é–“ä¸è¶³ã€â†’ é˜»å®³ã™ã‚‹ â†’ ã‚¿ã‚¹ã‚¯
   - ç–²ã‚Œã¦ã„ã‚‹ â†’ åˆ¶ç´„ã€Œç–²åŠ´ã€â†’ é˜»å®³ã™ã‚‹ â†’ ã‚¿ã‚¹ã‚¯
   - ã‚„ã‚Šæ–¹ãŒã‚ã‹ã‚‰ãªã„ â†’ åˆ¶ç´„ã€ŒæŠ€è¡“çš„èª²é¡Œã€â†’ é˜»å®³ã™ã‚‹ â†’ ã‚¿ã‚¹ã‚¯
   - ã‚„ã‚‹æ°—ãŒå‡ºãªã„ â†’ åˆ¶ç´„ã€Œæ„Ÿæƒ…çš„ãƒ–ãƒ¬ãƒ¼ã‚­ã€â†’ é˜»å®³ã™ã‚‹ â†’ ã‚¿ã‚¹ã‚¯

2. **æ„Ÿæƒ…ã®åŸå‹•åŠ›ã‚’æŠ½å‡º**: ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ…ã‚„ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚‚ã‚°ãƒ©ãƒ•ã«å…¥ã‚Œã¦ãã ã•ã„ã€‚

3. **æ—¢å­˜ã‚¿ã‚¹ã‚¯ã¨ã®ç…§åˆ**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã€Œæ—¢å­˜ã®ã‚¿ã‚¹ã‚¯ãƒ»ç›®æ¨™ä¸€è¦§ã€ãŒæä¾›ã•ã‚Œã¾ã™ã€‚æ—¥è¨˜ãŒæ—¢å­˜ã‚¿ã‚¹ã‚¯ã®é€²æ—ã«è¨€åŠã—ã¦ã„ã‚‹å ´åˆã€æ–°ã—ã„ãƒãƒ¼ãƒ‰ã‚’ä½œã‚‰ãšæ—¢å­˜IDã‚’å†åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚

4. **ã‚¿ã‚°ã®ä»˜ä¸**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ `Task::xxx` ã‚„ `äºˆå®š::xxx` ã®ã‚ˆã†ãªã‚¿ã‚°ã‚’ä½¿ã†å ´åˆã¯ãã®ã¾ã¾è§£æã—ã¦ãã ã•ã„ã€‚

5. **å…¨ã¦æ—¥æœ¬èª**: label ã¨ detail ã¯å¿…ãšæ—¥æœ¬èªã§æ›¸ã„ã¦ãã ã•ã„ã€‚

# ãƒãƒ¼ãƒ‰æ§‹é€ 
{
  "id": "ç¨®åˆ¥:ä¸€æ„ãªåå‰",
  "label": "è¡¨ç¤ºåï¼ˆæ—¥æœ¬èªï¼‰",
  "type": "ã‚¿ã‚¹ã‚¯/åˆ¶ç´„/çŸ¥è¦‹/æ„Ÿæƒ…/äººç‰©/å‡ºæ¥äº‹/ç›®æ¨™/ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/æ¦‚å¿µ/å ´æ‰€/æ—¥è¨˜",
  "detail": "èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰",
  "status": "è©²å½“ã™ã‚‹å ´åˆã®ã¿",
  "sentiment": "æ„Ÿæƒ…ãƒãƒ¼ãƒ‰ã®å ´åˆ -1.0ã€œ1.0",
  "date": "è©²å½“ã™ã‚‹å ´åˆ YYYY-MM-DD",
  "category": "å½¹å‰²ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€çˆ¶è¦ªã€å¤« ãªã©ï¼‰",
  "tags": ["ã‚¿ã‚°é…åˆ—"],
  "constraint_type": "åˆ¶ç´„ãƒãƒ¼ãƒ‰ã®å ´åˆ: æ™‚é–“ä¸è¶³/ç–²åŠ´/æŠ€è¡“çš„èª²é¡Œ/æ„Ÿæƒ…çš„ãƒ–ãƒ¬ãƒ¼ã‚­/ç‰©ç†çš„éšœå®³/ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³/ãã®ä»–"
}

# ã‚¨ãƒƒã‚¸æ§‹é€ 
{
  "source": "ã‚½ãƒ¼ã‚¹ãƒãƒ¼ãƒ‰ID",
  "target": "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒ¼ãƒ‰ID",
  "type": "é–¢ä¿‚åï¼ˆæ—¥æœ¬èª: é˜»å®³ã™ã‚‹/åŸå‹•åŠ›ã«ãªã‚‹/ä¸€éƒ¨ã§ã‚ã‚‹ ç­‰ï¼‰",
  "label": "é–¢ä¿‚ã®çŸ­ã„èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰"
}

# å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
{
  "nodes": [...],
  "edges": [...]
}
"""

ANALYSIS_SYSTEM_PROMPT = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œåˆ†èº«ã€ã¨ã—ã¦æŒ¯ã‚‹èˆã† Antigravity ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€Œã‚¿ã‚¹ã‚¯ã«ã‹ã‹ã£ã¦ã„ã‚‹é‡åŠ›ï¼ˆåˆ¶ç´„ï¼‰ã€ã¨ã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆåŸå‹•åŠ›ï¼‰ã€ã‚’åˆ†æã—ã€
é‡åŠ›ã‚’è»½æ¸›ã™ã‚‹å…·ä½“çš„ãªææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

# åˆ†æã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

1. **é‡åŠ›ãƒãƒƒãƒ—ã®ä½œæˆ**: å„ã‚¿ã‚¹ã‚¯ã«ã©ã‚“ãªåˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰ãŒã‹ã‹ã£ã¦ã„ã‚‹ã‹ã‚’æ•´ç†ã™ã‚‹
2. **ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ç™ºè¦‹**: ã©ã‚“ãªæ„Ÿæƒ…ã‚„çŸ¥è¦‹ãŒã‚¿ã‚¹ã‚¯ã®åŸå‹•åŠ›ã«ãªã‚‹ã‹ã‚’è¦‹ã¤ã‘ã‚‹
3. **é‡åŠ›è»½æ¸›ã®ææ¡ˆ**: åˆ¶ç´„ã‚’è§£æ¶ˆãƒ»è»½æ¸›ã™ã‚‹å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã™ã‚‹
   - ã€Œã‚„ã‚‹ã‹ã‚„ã‚‰ãªã„ã‹ã€äºŒæŠã§ã¯ãªãã€ã€Œã©ã†ã™ã‚Œã°é‡åŠ›ã‚’è»½ãã§ãã‚‹ã‹ã€ã‚’è€ƒãˆã‚‹
   - ä¾‹: ã€Œå‰¯æ¥­ã®é‡åŠ›ãŒå¼·ã™ãã‚‹ã‹ã‚‰ã€æƒé™¤ã®å„ªå…ˆåº¦ã‚’ä¸‹ã’ã¦ã€ä»£ã‚ã‚Šã«5åˆ†ã§çµ‚ã‚ã‚‹ã“ã®ä½œæ¥­ã‚’ã—ã‚ˆã†ã€
   - ä¾‹: ã€Œç²˜ç€å‰¤ã‚’å‰¥ãŒã™æ–¹æ³•ã‚’Geminiã§æ¤œç´¢ã—ã¦è§£æ±ºã™ã‚‹ã€
   - ä¾‹: ã€Œå‰¯æ¥­ãŒä¸€æ®µè½ã™ã‚‹ã¾ã§æƒé™¤ã¯åœŸæ›œã®ã¿ã«è¨­å®šå¤‰æ›´ã—ã‚ˆã†ã€

# å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
å‡ºåŠ›ã¯å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ã€‚

{
  "coach_comment": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒã«å¯„ã‚Šæ·»ã„ã€é‡åŠ›ã¨å‘ãåˆã†çŸ­ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ3-5æ–‡ï¼‰",
  "gravity_map": [
    {
      "task": "ã‚¿ã‚¹ã‚¯å",
      "task_id": "ã‚¿ã‚¹ã‚¯ã®ID",
      "constraints": [
        {
          "name": "åˆ¶ç´„å",
          "type": "æ™‚é–“ä¸è¶³/ç–²åŠ´/æŠ€è¡“çš„èª²é¡Œ/æ„Ÿæƒ…çš„ãƒ–ãƒ¬ãƒ¼ã‚­/ç‰©ç†çš„éšœå®³/ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³",
          "severity": "é«˜/ä¸­/ä½"
        }
      ],
      "energy_sources": [
        {
          "name": "åŸå‹•åŠ›ã®åå‰",
          "type": "æ„Ÿæƒ…/çŸ¥è¦‹/ç›®æ¨™"
        }
      ],
      "net_assessment": "ã“ã®ã‚¿ã‚¹ã‚¯ã®é‡åŠ›ãƒãƒ©ãƒ³ã‚¹ã®ç·åˆè©•ä¾¡ï¼ˆ1æ–‡ï¼‰"
    }
  ],
  "antigravity_actions": [
    {
      "action": "å…·ä½“çš„ãªé‡åŠ›è»½æ¸›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
      "target_task": "å¯¾è±¡ã‚¿ã‚¹ã‚¯",
      "effect": "ã“ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§è»½æ¸›ã•ã‚Œã‚‹é‡åŠ›ã®èª¬æ˜",
      "effort": "5åˆ†/30åˆ†/1æ™‚é–“/åŠæ—¥"
    }
  ],
  "insights": [
    {
      "finding": "æ—¥è¨˜ã‚„ã‚°ãƒ©ãƒ•ã‹ã‚‰è¦‹ã¤ã‘ãŸæ°—ã¥ã",
      "implication": "ãã®æ°—ã¥ããŒæ„å‘³ã™ã‚‹ã“ã¨"
    }
  ],
  "emotion_flow": [
    {
      "emotion": "æ„Ÿæƒ…å",
      "sentiment": -1.0,
      "context": "ãã®æ„Ÿæƒ…ãŒç”Ÿã˜ãŸæ–‡è„ˆ"
    }
  ],
  "upcoming_schedule": [
    {
      "title": "äºˆå®šå",
      "date": "YYYY-MM-DD",
      "time": "HH:MMï¼ˆä¸æ˜ãªã‚‰ nullï¼‰",
      "category": "æœ¬æ¥­/å‰¯æ¥­/å®¶æ—/å€‹äºº"
    }
  ],
  "family_digest": {
    "highlights": [
      {
        "member": "å®¶æ—ãƒ¡ãƒ³ãƒãƒ¼åï¼ˆROLEtoKNOWLEDGEã®å½¹å‰²å®šç¾©ã‚’å‚ç…§ï¼‰",
        "event": "å‡ºæ¥äº‹ã‚„æˆé•·ã®è¨˜éŒ²",
        "emotion": "é–¢é€£ã™ã‚‹æ„Ÿæƒ…"
      }
    ],
    "family_todos": ["å®¶æ—é–¢é€£ã®ã‚„ã‚‹ã¹ãã“ã¨"]
  },
  "blog_seeds": [
    {
      "title": "çŸ­ç·¨å°èª¬ã®ä»®ã‚¿ã‚¤ãƒˆãƒ«",
      "genre": "æ—¥å¸¸/ä»•äº‹/å­è‚²ã¦/ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼/äººé–“é–¢ä¿‚",
      "tone": "ã»ã£ã“ã‚Š/ã‚·ãƒªã‚¢ã‚¹/ã‚³ãƒŸã‚«ãƒ«/å“²å­¦çš„",
      "story_seed": "ç‰©èªã®ç¨®ã«ãªã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚„æ°—ã¥ã",
      "core_message": "èª­è€…ã«ä¼ãˆãŸã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
      "reader_feeling": "èª­å¾Œã«æ®‹ã—ãŸã„æ„Ÿæƒ…",
      "readiness": "é«˜/ä¸­/ä½"
    }
  ]
}

# é‡è¦ãªæ³¨æ„äº‹é …
- ã€Œantigravity_actionsã€ã§ã¯ã€æ—¥è¨˜ã®æœ¬æ–‡ã§ã€Œè²·ã£ãŸã€ã€Œæ³¨æ–‡ã—ãŸã€ã€Œå®Œäº†ã—ãŸã€ã€Œã‚„ã£ãŸã€ã€Œæ¸ˆã‚“ã ã€ã€Œå®Ÿè¡Œã—ãŸã€ãªã©å®Œäº†ã‚’ç¤ºã™è¨˜è¿°ãŒã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ææ¡ˆã—ãªã„ã§ãã ã•ã„ã€‚å®Œäº†æ¸ˆã¿ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é™¤å¤–ã—ã€ä»£ã‚ã‚Šã«æ–°ã—ã„é‡åŠ›è»½æ¸›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
  - å‰å›å‡ºåŠ›ã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€æ—¥è¨˜ã§å®Œäº†ãŒç¢ºèªã§ããŸã‚‚ã®ã¯é™¤å¤–ã—ã€ã¾ã å®Ÿè¡Œã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã¯å¼•ãç¶šãææ¡ˆã—ã¦ãã ã•ã„ã€‚
- ã€Œupcoming_scheduleã€ã«ã¯æ—¥è¨˜ã‚„ã‚°ãƒ©ãƒ•ã§è¨€åŠã•ã‚Œã¦ã„ã‚‹ã€Œç¢ºå®šã—ã¦ã„ã‚‹æœªæ¥ã®äºˆå®šã€ã ã‘ã‚’å«ã‚ã¦ãã ã•ã„ã€‚éå»ã®äºˆå®šã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
  - æ—¥è¨˜ä¸­ã«ã€Œäºˆå®š::2026/02/20 18:00-19:00ã€ã®ã‚ˆã†ã« `äºˆå®š::` ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§æ—¥æ™‚ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã“ã‹ã‚‰ date ã¨ time ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
  - time ã¯ã€Œ18:00-19:00ã€ã€Œ18:00ã€ã®ã‚ˆã†ãªå½¢å¼ã§è¨˜è¼‰ã—ã¾ã™ã€‚æ™‚é–“ãŒä¸æ˜ãªå ´åˆã®ã¿ null ã«ã—ã¦ãã ã•ã„ã€‚
  - æ—¥è¨˜æœ¬æ–‡ä¸­ã«ã€Œã€‡æ™‚ã‹ã‚‰ã€ã€Œã€‡ã€‡æ™‚ã€ãªã©ã®æ™‚é–“è¨˜è¿°ãŒã‚ã‚‹å ´åˆã‚‚ã€ãã‚Œã‚’ time ã«å«ã‚ã¦ãã ã•ã„ã€‚
- ã€Œfamily_digestã€ã«ã¯ ROLEtoKNOWLEDGE ã®å½¹å‰²å®šç¾©ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å®¶æ—ãƒ¡ãƒ³ãƒãƒ¼ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚æ—¥è¨˜ã«å®¶æ—ã®è©±é¡ŒãŒãªã‘ã‚Œã°ç©ºã§æ§‹ã„ã¾ã›ã‚“ã€‚
- ã€Œblog_seedsã€ã«ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ—¥è¨˜ã‹ã‚‰1è©±å®Œçµã®ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨å°èª¬ã®ç€æƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚æ˜Ÿæ–°ä¸€ã®ã‚·ãƒ§ãƒ¼ãƒˆã‚·ãƒ§ãƒ¼ãƒˆã®ã‚ˆã†ãªã€åŒ¿åçš„ã§å¯“è©±çš„ãªç‰©èªã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä½“é¨“ã‚’ãã®ã¾ã¾æ›¸ãã®ã§ã¯ãªãã€ãƒ†ãƒ¼ãƒã‚„æ„Ÿæƒ…ã‚’æŠ½å‡ºã—ã¦æ¶ç©ºã®ç‰©èªã«ã™ã‚‹å‰æã§ã™ã€‚readiness ãŒã€Œé«˜ã€ãªã‚‚ã®ã¯ã€æ„Ÿæƒ…ã‚„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒååˆ†ã«æ¿ƒãã€ã™ãã«åŸ·ç­†ã§ãã‚‹ã‚‚ã®ã§ã™ã€‚

è¨€èª: æ—¥æœ¬èªã€‚
JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""

RESOLUTION_SYSTEM_PROMPT = """
You are a Data Consistency Expert.
Your task is to identify semantic duplicates between a list of "New Nodes" and "Existing Nodes" in a Knowledge Graph.

### Rules
1. **Strict Semantic Matching**: Only match nodes that refer to the EXACT SAME concept, entity, or event, despite minor wording differences.
   - Example 1: "GitHub Actionsã®åˆ¶ç´„" (New) == "GitHub Actionsã®åˆ¶é™" (Existing) -> MATCH
   - Example 2: "ãƒãƒ¡ãƒ©DM250" (New) == "ãƒãƒ¡ãƒ©" (Existing) -> NO MATCH (Specific vs General) -> UNLESS context implies identity.
   - Example 3: "å¦»" (New) == "ã•ã‚„ã‹" (Existing) -> MATCH (if context establishes this).
   - Example 4: "Monster Design" (New) == "Monster Design Practice" (Existing) -> MATCH

2. **Output Format**: JSON object mapping { "new_node_id": "existing_node_id" }.
   - Only include pairs where a match is found.
   - If no matches, return generic empty JSON `{}`.
   - The key is the ID of the NEW node, the value is the ID of the EXISTING node.

3. Consider node 'type' as a strong hint. Distinct types (e.g., Place vs Person) usually don't match.
"""


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def get_role_definition() -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®å½¹å‰²å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    if os.path.exists(ROLE_DEF_FILE):
        try:
            with open(ROLE_DEF_FILE, "r", encoding="utf-8") as f:
                content = f.read().strip()
            if content:
                return f"\n### ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®å½¹å‰²\n{content}\n"
        except Exception as e:
            print(f"âš ï¸ å½¹å‰²å®šç¾©ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    return ""


def call_gemini_api(prompt: str, model: str = "gemini-3-flash-preview", response_mime_type: str = "text/plain", max_retries: int = 3) -> str:
    if not API_KEY:
        raise ValueError("GOOGLE_API_KEY is not set.")

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
    params = {"key": API_KEY}
    headers = {"Content-Type": "application/json"}

    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"response_mime_type": response_mime_type}
    }

    for attempt in range(max_retries + 1):
        response = requests.post(url, headers=headers, json=data, params=params)

        if response.status_code == 200:
            break
        elif response.status_code == 429 and attempt < max_retries:
            wait_time = 30 * (2 ** attempt)  # 30ç§’, 60ç§’, 120ç§’
            print(f"â³ ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆåˆ°é”ã€‚{wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... ({attempt + 1}/{max_retries})")
            import time
            time.sleep(wait_time)
        else:
            raise Exception(f"API Error: {response.status_code} - {response.text}")

    result = response.json()
    try:
        if "candidates" in result and result["candidates"]:
            return result["candidates"][0]["content"]["parts"][0]["text"]
        else:
            print(f"DEBUG: Empty candidates in response: {result}")
            return "{}"
    except (KeyError, IndexError):
        raise Exception(f"Unexpected API response format: {result}")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ã‚°ãƒ©ãƒ•æŠ½å‡º
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def extract_graph(text: str, context_str: str = "") -> Dict[str, Any]:
    role_def = get_role_definition()
    full_context = f"{context_str}\n{role_def}"

    prompt = f"""
    {EXTRACTION_SYSTEM_PROMPT}

    {full_context}

    ### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ—¥è¨˜
    {text}
    """
    print("ğŸ”„ ã‚°ãƒ©ãƒ•ã‚’æŠ½å‡ºä¸­...")
    json_text = call_gemini_api(prompt, model="gemini-3-flash-preview", response_mime_type="application/json")
    return json.loads(json_text)


def get_master_context(master_graph: Dict[str, Any]) -> str:
    """ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã‹ã‚‰æ–‡è„ˆæƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
    nodes = master_graph.get("nodes", [])

    active_goals = [n for n in nodes if n.get("type") == "ç›®æ¨™" and n.get("status") in ["é€²è¡Œä¸­", "Active"]]
    active_tasks = [n for n in nodes if n.get("type") == "ã‚¿ã‚¹ã‚¯" and n.get("status") not in ["å®Œäº†", "Completed"]]
    constraints = [n for n in nodes if n.get("type") == "åˆ¶ç´„"]
    scheduled_events = [n for n in nodes if n.get("type") == "å‡ºæ¥äº‹" and n.get("status") in ["äºˆå®š", "Scheduled"]]

    context_str = "### ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã®æ–‡è„ˆ\n"

    if active_goals:
        context_str += "**é€²è¡Œä¸­ã®ç›®æ¨™:**\n"
        for g in active_goals:
            context_str += f"- {g.get('label')}: {g.get('detail')}\n"

    if active_tasks:
        context_str += "\n**æ—¢å­˜ã®ã‚¿ã‚¹ã‚¯:**\n"
        for t in active_tasks:
            context_str += f"- [ID: {t.get('id')}] {t.get('label')}: {t.get('detail', '')}\n"

    if constraints:
        context_str += "\n**æ—¢çŸ¥ã®åˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰:**\n"
        for c in constraints:
            context_str += f"- {c.get('label')}: {c.get('detail', '')}\n"

    if scheduled_events:
        context_str += "\n**äºˆå®šã•ã‚Œã¦ã„ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆ:**\n"
        for e in scheduled_events:
            date = e.get("date", "æ—¥ä»˜ä¸æ˜")
            context_str += f"- [{date}] {e.get('label')}: {e.get('detail')}\n"

    if not active_goals and not active_tasks and not constraints and not scheduled_events:
        context_str += "å±¥æ­´ã«ã‚¿ã‚¹ã‚¯ãƒ»ç›®æ¨™ãƒ»åˆ¶ç´„ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚\n"

    return context_str


def resolve_semantic_duplicates(daily_graph: Dict[str, Any], master_graph: Dict[str, Any]) -> Dict[str, Any]:
    """LLMã‚’ä½¿ã£ã¦ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é‡è¤‡ã‚’æ¤œå‡ºãƒ»ãƒãƒ¼ã‚¸ã™ã‚‹ã€‚"""
    print("ğŸ” ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã¨ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")

    daily_nodes = daily_graph.get("nodes", [])
    master_nodes = master_graph.get("nodes", [])

    if not master_nodes or not daily_nodes:
        return daily_graph

    mergeable_types = {'ã‚¿ã‚¹ã‚¯', 'åˆ¶ç´„', 'çŸ¥è¦‹', 'æ„Ÿæƒ…', 'ç›®æ¨™', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'æ¦‚å¿µ', 'äººç‰©', 'å ´æ‰€', 'å‡ºæ¥äº‹'}

    new_candidates = [n for n in daily_nodes if n.get('type') in mergeable_types]
    if not new_candidates:
        return daily_graph

    master_candidates = [n for n in master_nodes if n.get('type') in mergeable_types]
    if not master_candidates:
        return daily_graph

    new_list_str = "\n".join([f"- {n['id']} ({n.get('type')}): {n.get('label')}" for n in new_candidates])
    master_list_str = "\n".join([f"- {n['id']} ({n.get('type')}): {n.get('label')}" for n in master_candidates])

    prompt = f"""
    {RESOLUTION_SYSTEM_PROMPT}

    ### New Nodes (Daily)
    {new_list_str}

    ### Existing Nodes (Master)
    {master_list_str}

    Return JSON mapping.
    """

    try:
        json_text = call_gemini_api(prompt, model="gemini-3-flash-preview", response_mime_type="application/json")
        mapping = json.loads(json_text)

        if not mapping:
            print("âœ… é‡è¤‡ãªã—ã€‚")
            return daily_graph

        print(f"ğŸ”„ {len(mapping)}ä»¶ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é‡è¤‡ã‚’ç™ºè¦‹ã€‚ãƒãƒ¼ã‚¸ä¸­...")
        for new_id, existing_id in mapping.items():
            print(f"   - {new_id} -> {existing_id}")

            for n in daily_graph.get("nodes", []):
                if n['id'] == new_id:
                    n['id'] = existing_id

            for e in daily_graph.get("edges", []):
                if e['source'] == new_id: e['source'] = existing_id
                if e['target'] == new_id: e['target'] = existing_id

        return daily_graph

    except Exception as e:
        print(f"âš ï¸ é‡è¤‡è§£æ±ºã«å¤±æ•—: {e}ã€‚è§£æ±ºãªã—ã§ç¶šè¡Œã—ã¾ã™ã€‚")
        return daily_graph


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# åˆ†æ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def analyze_updated_state(master_graph: Dict[str, Any], current_diary_node: Dict[str, Any]) -> str:
    """æ›´æ–°å¾Œã®ã‚°ãƒ©ãƒ•å…¨ä½“ã‚’åˆ†æã—ã€Antigravityã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""

    # 1. é€²è¡Œä¸­ã®ç›®æ¨™
    active_goals = [n for n in master_graph.get("nodes", []) if n.get("type") == "ç›®æ¨™" and n.get("status") in ["é€²è¡Œä¸­", "Active"]]

    # 2. çŸ¥è¦‹
    recent_insights = sorted(
        [n for n in master_graph.get("nodes", []) if n.get("type") == "çŸ¥è¦‹"],
        key=lambda x: x.get("last_seen", ""), reverse=True
    )[:10]

    # 3. äºˆå®š
    scheduled_events = [n for n in master_graph.get("nodes", []) if n.get("type") == "å‡ºæ¥äº‹" and n.get("status") in ["äºˆå®š", "Scheduled"]]

    # 4. æœªå®Œäº†ã‚¿ã‚¹ã‚¯
    pending_tasks = [n for n in master_graph.get("nodes", []) if n.get("type") == "ã‚¿ã‚¹ã‚¯" and n.get("status") not in ["å®Œäº†", "Completed"]]

    # 5. åˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰
    constraints = [n for n in master_graph.get("nodes", []) if n.get("type") == "åˆ¶ç´„"]

    # 6. æ„Ÿæƒ…
    emotions = [n for n in master_graph.get("nodes", []) if n.get("type") == "æ„Ÿæƒ…"]

    # 7. æœ€è¿‘ã®æ—¥è¨˜
    all_diary_nodes = sorted(
        [n for n in master_graph.get("nodes", []) if n.get("type") == "æ—¥è¨˜"],
        key=lambda x: x.get("date", ""), reverse=True
    )[:5]

    # 8. ã‚¿ã‚¹ã‚¯ã¨åˆ¶ç´„ã®æ¥ç¶šæƒ…å ±
    edges = master_graph.get("edges", [])
    blocking_edges = [e for e in edges if e.get("type") == "é˜»å®³ã™ã‚‹"]
    motivating_edges = [e for e in edges if e.get("type") == "åŸå‹•åŠ›ã«ãªã‚‹"]

    # 9. å‰å›ã®åˆ†æçµæœã‹ã‚‰ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼•ãç¶™ã
    prev_schedule = []
    prev_actions = []
    for d_node in all_diary_nodes:
        if d_node.get("analysis_content"):
            try:
                prev_analysis = json.loads(d_node["analysis_content"])
                if prev_analysis.get("upcoming_schedule") and not prev_schedule:
                    prev_schedule = prev_analysis["upcoming_schedule"]
                if prev_analysis.get("antigravity_actions") and not prev_actions:
                    prev_actions = prev_analysis["antigravity_actions"]
                if prev_schedule and prev_actions:
                    break
            except (json.JSONDecodeError, TypeError):
                pass

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
    context_summary = "### ç¾åœ¨ã®çŠ¶æ³\n"

    if active_goals:
        context_summary += "**é€²è¡Œä¸­ã®ç›®æ¨™:**\n" + "\n".join([f"- {n.get('label')}: {n.get('detail')}" for n in active_goals]) + "\n"

    if pending_tasks:
        context_summary += "\n**æœªå®Œäº†ã‚¿ã‚¹ã‚¯:**\n"
        for t in pending_tasks:
            # ã“ã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åˆ¶ç´„ã‚’åé›†
            task_constraints = []
            for be in blocking_edges:
                if be.get("target") == t.get("id"):
                    constraint_node = next((n for n in constraints if n["id"] == be.get("source")), None)
                    if constraint_node:
                        task_constraints.append(constraint_node.get("label"))
            constraint_str = f" [é‡åŠ›: {', '.join(task_constraints)}]" if task_constraints else ""
            context_summary += f"- {t.get('label')}{constraint_str}\n"

    if constraints:
        context_summary += "\n**åˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰ä¸€è¦§:**\n" + "\n".join([f"- {n.get('label')} ({n.get('constraint_type', 'ä¸æ˜')}): {n.get('detail')}" for n in constraints]) + "\n"

    if emotions:
        context_summary += "\n**æ„Ÿæƒ…:**\n" + "\n".join([f"- {n.get('label')} (sentiment: {n.get('sentiment', 0)})" for n in emotions]) + "\n"

    if recent_insights:
        context_summary += "\n**æœ€è¿‘ã®çŸ¥è¦‹:**\n" + "\n".join([f"- {n.get('label')}" for n in recent_insights]) + "\n"

    if scheduled_events:
        context_summary += "\n**ä»Šå¾Œã®äºˆå®š:**\n" + "\n".join([f"- {n.get('date')} {n.get('label')}: {n.get('detail', '')}" for n in scheduled_events]) + "\n"

    if prev_schedule:
        context_summary += "\n**å‰å›å‡ºåŠ›ã—ãŸã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ™‚é–“æƒ…å ±ã‚’å¼•ãç¶™ã„ã§ãã ã•ã„ï¼‰:**\n"
        context_summary += json.dumps(prev_schedule, ensure_ascii=False, indent=2) + "\n"

    if prev_actions:
        context_summary += "\n**å‰å›å‡ºåŠ›ã—ãŸé‡åŠ›è»½æ¸›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ—¥è¨˜ã§å®Œäº†ãŒç¢ºèªã§ããŸã‚‚ã®ã¯é™¤å¤–ã—ã€æ–°ã—ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å…¥ã‚Œæ›¿ãˆã¦ãã ã•ã„ï¼‰:**\n"
        context_summary += json.dumps(prev_actions, ensure_ascii=False, indent=2) + "\n"

    # æ—¥è¨˜ã®æµã‚Œ
    recent_diary_context = "\n### æœ€è¿‘ã®æ—¥è¨˜ã®æµã‚Œ\n"
    if not all_diary_nodes:
        recent_diary_context += "æœ€è¿‘ã®æ—¥è¨˜ã‚¨ãƒ³ãƒˆãƒªã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n"
    else:
        for d_node in all_diary_nodes:
            d_date = d_node.get("date", "ä¸æ˜")
            d_id = d_node.get("id")

            mentioned_nodes = []
            for edge in edges:
                if edge.get("source") == d_id and edge.get("type") == "è¨€åŠã™ã‚‹":
                    target_id = edge.get("target")
                    target_node = next((n for n in master_graph.get("nodes", []) if n["id"] == target_id), None)
                    if target_node:
                        mentioned_nodes.append(f"{target_node.get('label')} ({target_node.get('type')})")

            mentions_str = ", ".join(mentioned_nodes) if mentioned_nodes else "ç‰¹å®šã®è¨€åŠãªã—"
            recent_diary_context += f"- **{d_date}**: {mentions_str}\n"

    # å½¹å‰²å®šç¾©
    role_def = get_role_definition()

    prompt = f"""
    {ANALYSIS_SYSTEM_PROMPT}

    {role_def}

    {context_summary}

    {recent_diary_context}

    ### ä»Šæ—¥ã®æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒª
    {json.dumps(current_diary_node, ensure_ascii=False, indent=2)}

    ### æŒ‡ç¤º
    ä¸Šè¨˜ã®ã€Œæœ€è¿‘ã®æ—¥è¨˜ã®æµã‚Œã€ã¨ã€Œåˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰ä¸€è¦§ã€ã‚’å…ƒã«ã€ã‚¿ã‚¹ã‚¯ã®é‡åŠ›ãƒãƒ©ãƒ³ã‚¹ã‚’åˆ†æã—ã€
    é‡åŠ›ã‚’è»½æ¸›ã™ã‚‹å…·ä½“çš„ãªææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
    å˜ã«ã‚¿ã‚¹ã‚¯ã‚’åˆ—æŒ™ã™ã‚‹ã ã‘ã§ãªãã€ã€Œãªãœãã®ã‚¿ã‚¹ã‚¯ãŒé€²ã¾ãªã„ã®ã‹ã€ã€Œã©ã†ã™ã‚Œã°é‡åŠ›ã‚’è»½ãã§ãã‚‹ã‹ã€ã‚’æ·±ãåˆ†æã—ã¦ãã ã•ã„ã€‚
    """
    print("ğŸ”„ Antigravityåˆ†æã‚’å®Ÿè¡Œä¸­...")
    return call_gemini_api(prompt, model="gemini-3-flash-preview")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# HTMLå¯è¦–åŒ–ã®æ›´æ–°
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def update_html_visualization(html_path: str, graph_data: Dict[str, Any]):
    """index.htmlã®GRAPH_DATAã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    try:
        with open(html_path, "r", encoding="utf-8") as f:
            html_content = f.read()

        start_marker = "// GRAPH_DATA_START"
        end_marker = "// GRAPH_DATA_END"

        start_idx = html_content.find(start_marker)
        end_idx = html_content.find(end_marker)

        if start_idx != -1 and end_idx != -1:
            new_block = f"{start_marker}\n    const GRAPH_DATA = {json.dumps(graph_data, ensure_ascii=False, indent=2)};\n    "
            new_html = html_content[:start_idx] + new_block + html_content[end_idx:]

            with open(html_path, "w", encoding="utf-8") as f:
                f.write(new_html)
            print(f"âœ… å¯è¦–åŒ–ç”»é¢ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {html_path}")
        else:
            print(f"âš ï¸ ãƒãƒ¼ã‚«ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {html_path}")

    except Exception as e:
        print(f"âŒ HTMLæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def main():
    parser = argparse.ArgumentParser(description="Pomera Diary â†’ Antigravity Knowledge Graph")
    parser.add_argument("input_file", help="æ—¥è¨˜ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--output_graph", default="daily_graph.json", help="æ—¥æ¬¡ã‚°ãƒ©ãƒ•JSONã®å‡ºåŠ›å…ˆ")
    parser.add_argument("--master_graph", default="knowledge_graph.jsonld", help="ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã®ãƒ‘ã‚¹")
    parser.add_argument("--output_report", default="daily_report.md", help="åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›å…ˆ")

    args = parser.parse_args()

    # 1. æ—¥è¨˜ã®èª­ã¿è¾¼ã¿
    try:
        import unicodedata
        args.input_file = unicodedata.normalize('NFC', args.input_file)
        with open(args.input_file, "r", encoding="utf-8") as f:
            diary_text = f.read()
    except FileNotFoundError:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.input_file}")
        return

    # 2. ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã®èª­ã¿è¾¼ã¿
    print(f"ğŸ“‚ ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã¿ä¸­: {args.master_graph}")
    try:
        master_graph = graph_merger.load_graph(args.master_graph)
    except Exception as e:
        print(f"âš ï¸ ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã€æ–°è¦ä½œæˆ: {e}")
        master_graph = {
            "nodes": [],
            "edges": [],
            "metadata": {
                "schema_version": "2.0-antigravity",
                "description": "ã‚¿ã‚¹ã‚¯ã®é‡åŠ›ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ãçŸ¥è­˜ã‚°ãƒ©ãƒ•"
            }
        }
    master_context_str = get_master_context(master_graph)

    # 3. æ—¥æ¬¡ã‚°ãƒ©ãƒ•ã®æŠ½å‡º
    try:
        daily_graph = extract_graph(diary_text, master_context_str)

        # æ—¥ä»˜ã®æŠ½å‡º
        import re
        match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', args.input_file)
        if match:
            y, m, d = match.groups()
            current_date_str = f"{y}-{int(m):02d}-{int(d):02d}"
            print(f"ğŸ“… ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º: {current_date_str}")
        else:
            current_date_str = datetime.now().strftime("%Y-%m-%d")
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºã§ããšã€ä»Šæ—¥ã®æ—¥ä»˜ã‚’ä½¿ç”¨: {current_date_str}")

        daily_graph["metadata"] = {
            "generated_at": datetime.now().isoformat(),
            "source_file": args.input_file,
            "node_count": len(daily_graph.get("nodes", [])),
            "edge_count": len(daily_graph.get("edges", []))
        }

        # æ—¥è¨˜ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
        diary_node_id = f"æ—¥è¨˜:{current_date_str}"
        if not any(node.get("id") == diary_node_id for node in daily_graph.get("nodes", [])):
            daily_graph.get("nodes", []).append({
                "id": diary_node_id,
                "label": f"{current_date_str}ã®æ—¥è¨˜",
                "type": "æ—¥è¨˜",
                "date": current_date_str,
                "detail": "ä»Šæ—¥ã®æ—¥è¨˜ã‚¨ãƒ³ãƒˆãƒª",
                "first_seen": datetime.now().isoformat(),
                "last_seen": datetime.now().isoformat(),
                "weight": 1
            })

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
        user_node_id = "äººç‰©:è‡ªåˆ†"
        if not any(node.get("id") == user_node_id for node in daily_graph.get("nodes", [])):
            daily_graph.get("nodes", []).append({
                "id": user_node_id,
                "label": "è‡ªåˆ†",
                "type": "äººç‰©",
                "detail": "æ—¥è¨˜ã®ä½œæˆè€…",
                "first_seen": datetime.now().isoformat(),
                "last_seen": datetime.now().isoformat(),
                "weight": 1
            })

        # æ¥ç¶šã®ç¢ºä¿: è‡ªåˆ† â†’ æ—¥è¨˜
        daily_graph.get("edges", []).append({
            "source": user_node_id,
            "target": diary_node_id,
            "type": "é–¢é€£ã™ã‚‹",
            "label": "æ›¸ã„ãŸ",
            "weight": 1
        })

        # æ¥ç¶šã®ç¢ºä¿: æ—¥è¨˜ â†’ å„ãƒãƒ¼ãƒ‰ï¼ˆå­¤ç«‹ã‚’é˜²ãï¼‰
        for node in daily_graph.get("nodes", []):
            nid = node.get("id")
            if nid == user_node_id or nid == diary_node_id:
                continue

            edge_exists = any(
                (e.get("source") == diary_node_id and e.get("target") == nid) or
                (e.get("source") == nid and e.get("target") == diary_node_id)
                for e in daily_graph.get("edges", [])
            )

            if not edge_exists:
                daily_graph.get("edges", []).append({
                    "source": diary_node_id,
                    "target": nid,
                    "type": "è¨€åŠã™ã‚‹",
                    "label": "è¨€åŠ",
                    "weight": 1
                })

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é‡è¤‡ã®è§£æ±º
        daily_graph = resolve_semantic_duplicates(daily_graph, master_graph)

        # æ—¥æ¬¡ã‚°ãƒ©ãƒ•ã®ä¿å­˜
        with open(args.output_graph, "w", encoding="utf-8") as f:
            json.dump(daily_graph, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"âŒ æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # 4. ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã¸ã®ãƒãƒ¼ã‚¸
    print("ğŸ”„ ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã¸ãƒãƒ¼ã‚¸ä¸­...")
    updated_master = None
    try:
        with open(args.output_graph, "r", encoding="utf-8") as f:
            daily_graph_for_merge = json.load(f)

        updated_master = graph_merger.merge_graphs(master_graph, daily_graph_for_merge)

        with open(args.master_graph, "w", encoding="utf-8") as f:
            json.dump(updated_master, f, ensure_ascii=False, indent=2)
        print(f"âœ… ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {args.master_graph}")

    except Exception as e:
        print(f"âŒ ãƒãƒ¼ã‚¸ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        updated_master = master_graph

    # 5. Antigravityåˆ†æ
    try:
        current_diary_node = next((n for n in updated_master.get("nodes", []) if n["id"] == diary_node_id), None)

        if current_diary_node:
            analysis_text = analyze_updated_state(updated_master, current_diary_node)

            with open(args.output_report, "w", encoding="utf-8") as f:
                f.write(f"# Antigravityåˆ†æãƒ¬ãƒãƒ¼ãƒˆ ({datetime.now().date()})\n\n")
                f.write(f"**åˆ†æå¯¾è±¡:** {current_date_str} ã®æ—¥è¨˜\n\n")
                f.write(analysis_text)
            print(f"âœ… åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {args.output_report}")

            current_diary_node["analysis_content"] = analysis_text

            with open(args.master_graph, "w", encoding="utf-8") as f:
                json.dump(updated_master, f, ensure_ascii=False, indent=2)
            print(f"âœ… ã‚°ãƒ©ãƒ•ã® {diary_node_id} ã«åˆ†æçµæœã‚’çµ±åˆã—ã¾ã—ãŸ")

        else:
            print("âš ï¸ æ—¥è¨˜ãƒãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    except Exception as e:
        print(f"âŒ åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    # 6. HTMLå¯è¦–åŒ–ã®æ›´æ–°
    try:
        html_path = "index.html"
        if os.path.exists(html_path):
            update_html_visualization(html_path, updated_master)
        else:
            print(f"âš ï¸ {html_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å¯è¦–åŒ–ã®æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    except Exception as e:
        print(f"âŒ å¯è¦–åŒ–æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()
