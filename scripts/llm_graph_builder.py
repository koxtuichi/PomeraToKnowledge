import os
import json
import argparse
from datetime import datetime
import requests
from typing import Dict, Any, List, Optional

try:
    import graph_merger
except ImportError:
    print("âš ï¸  graph_merger module not found. Persistence features will be limited.")

# â”€â”€ è¨­å®š â”€â”€
API_KEY = os.getenv("GOOGLE_API_KEY")
ROLE_DEF_FILE = "role_definition.txt"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXTRACTION_SYSTEM_PROMPT = """
# å½¹å‰²
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œåˆ†èº«ã€ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒŠãƒ¬ãƒƒã‚¸ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚
æ—¥è¨˜ã‹ã‚‰ã€ã‚¿ã‚¹ã‚¯ç®¡ç†ã‚’é«˜åº¦åŒ–ã™ã‚‹ãŸã‚ã®ã€ŒçŸ¥è­˜ã‚°ãƒ©ãƒ•ã®å·®åˆ†ã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

# æŠ½å‡ºå¯¾è±¡ï¼ˆãƒãƒ¼ãƒ‰ã®ç¨®é¡ï¼‰

1. **ã‚¿ã‚¹ã‚¯**: å…·ä½“çš„ã€ã¾ãŸã¯æŠ½è±¡çš„ãªã€Œã‚„ã‚‹ã¹ãã“ã¨ã€ã€‚
   - status: "æœªç€æ‰‹" / "é€²è¡Œä¸­" / "å®Œäº†" / "ä¿ç•™"
2. **åˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰**: ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œã‚’å¦¨ã’ã‚‹è¦å› ã€‚
   - ç¨®é¡: "æ™‚é–“ä¸è¶³" / "ç–²åŠ´" / "æŠ€è¡“çš„èª²é¡Œ" / "æ„Ÿæƒ…çš„ãƒ–ãƒ¬ãƒ¼ã‚­" / "ç‰©ç†çš„éšœå®³" / "ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³" / "ãã®ä»–"
3. **çŸ¥è¦‹**: è©¦è¡ŒéŒ¯èª¤ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæ•™è¨“ã‚„ã€å°†æ¥ã®è³‡ç”£ã«ç¹‹ãŒã‚Šãã†ãªæ°—ã¥ãã€‚
4. **æ„Ÿæƒ…**: ãã®æ™‚ã®æ„Ÿæƒ…ã€‚ã‚¿ã‚¹ã‚¯ã®åŸå‹•åŠ›ã€ã¾ãŸã¯é˜»å®³è¦å› ã«ãªã‚‹ã€‚
   - sentiment: -1.0 ã‹ã‚‰ 1.0 ã®æ•°å€¤
   - emotion_category: ã€Œå–œã³ã€ã€Œé”æˆæ„Ÿã€ã€Œä¸å®‰ã€ã€Œæ€’ã‚Šã€ã€Œãã®ä»–ã€ã®5åˆ†é¡ã‹ã‚‰å¿…ãšé¸æŠ
   - trigger: ãã®æ„Ÿæƒ…ãŒç”Ÿã¾ã‚ŒãŸå…·ä½“çš„ãªãã£ã‹ã‘ï¼ˆ1æ–‡ã§è¨˜è¿°ï¼‰
5. **äººç‰©**: æ—¥è¨˜ã«ç™»å ´ã™ã‚‹äººã€‚
6. **å‡ºæ¥äº‹**: èµ·ããŸå…·ä½“çš„ãªã‚¤ãƒ™ãƒ³ãƒˆã€‚
   - status: "äºˆå®š" / "å®Œäº†" / "ä¸­æ­¢"
7. **ç›®æ¨™**: é•·æœŸçš„ã«ç›®æŒ‡ã™ã‚‚ã®ã€‚
   - status: "é€²è¡Œä¸­" / "é”æˆ" / "æ–­å¿µ"
8. **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: ç¶™ç¶šçš„ãªå–ã‚Šçµ„ã¿ã€‚
9. **æ¦‚å¿µ**: æŠ½è±¡çš„ãªæ¦‚å¿µã‚„çŠ¶æ…‹ã€‚
10. **å ´æ‰€**: å ´æ‰€ã€‚
11. **æ—¥è¨˜**: æ—¥è¨˜ã‚¨ãƒ³ãƒˆãƒªãã®ã‚‚ã®ã€‚
12. **è³¼å…¥å¸Œæœ›**: ã€Œã‚‚ã£ã¦ã„ã‚‹ã®ãŒãã‚ŒãŒãã¦ã„ã‚‹ã€ã€Œè²·ã„ãŸã„ã€ã€Œæ¬²ã—ã„ã€ã€Œæ¤ãˆæ›¿ãˆãŸã„ã€ã€Œæ”¹è¾ºã—ãŸã„ã€ãªã©ã€è²·ã„ãŸã„ã‚‚ã®ã‚„ã‚„ã‚ŠãŸã„äº‹ã«é–¢ã™ã‚‹è¨˜è¿°ã€‚
    - cost: é‡‘é¡ã®ç›®å®‰ï¼ˆè¨€åŠã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ã€æ•´æ•°å††ï¼‰
    - priority: ã€Œé«˜ã€ï¼ˆç”Ÿæ´»ä¸Šå¿…è¦ä¸å¯æ¬ ï¼‰ã€Œä¸­ã€ï¼ˆè¿‘ã„å°†æ¥ã«æ¬²ã—ã„ï¼‰ã€Œä½ã€ï¼ˆã„ã¤ã‹æ¬²ã—ã„ï¼‰ã®3åˆ†é¡
    - status: ã€Œæ¤œè¨ä¸­ã€ / ã€Œè³¼å…¥æ¸ˆã€ / ã€Œã‚­ãƒ£ãƒ³ã‚»ãƒ«ã€

# é–¢ä¿‚æ€§ï¼ˆã‚¨ãƒƒã‚¸ã®ç¨®é¡ï¼‰

| é–¢ä¿‚å | æ–¹å‘ | æ„å‘³ |
|--------|------|------|
| é˜»å®³ã™ã‚‹ | åˆ¶ç´„ â†’ ã‚¿ã‚¹ã‚¯ | ã“ã®åˆ¶ç´„ãŒã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œã‚’å¦¨ã’ã¦ã„ã‚‹ |
| åŸå‹•åŠ›ã«ãªã‚‹ | æ„Ÿæƒ…/çŸ¥è¦‹ â†’ ã‚¿ã‚¹ã‚¯ | ã“ã®æ„Ÿæƒ…ã‚„çŸ¥è¦‹ãŒã‚¿ã‚¹ã‚¯ã‚’æ¨é€²ã™ã‚‹åŠ›ã«ãªã‚‹ |
| ä¸€éƒ¨ã§ã‚ã‚‹ | çŸ¥è¦‹ â†’ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ | ã“ã®çŸ¥è¦‹ãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€éƒ¨ã‚’æ§‹æˆã™ã‚‹ |
| è¨€åŠã™ã‚‹ | æ—¥è¨˜ â†’ å„ãƒãƒ¼ãƒ‰ | æ—¥è¨˜å†…ã§è§¦ã‚ŒãŸã‚‚ã® |
| å¼•ãèµ·ã“ã™ | å‡ºæ¥äº‹ â†’ æ„Ÿæƒ…/çŸ¥è¦‹ | å› æœé–¢ä¿‚ |
| å‚åŠ ã™ã‚‹ | äººç‰© â†’ å‡ºæ¥äº‹ | äººç‰©ãŒå‡ºæ¥äº‹ã«å‚åŠ  |
| å ´æ‰€ã§ | å‡ºæ¥äº‹ â†’ å ´æ‰€ | å‡ºæ¥äº‹ãŒèµ·ããŸå ´æ‰€ |
| å¯¾è±¡ã«ã™ã‚‹ | ã‚¿ã‚¹ã‚¯ â†’ äººç‰©/æ¦‚å¿µ | ã‚¿ã‚¹ã‚¯ã®å¯¾è±¡ |
| è¨ˆç”»ã™ã‚‹ | ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ â†’ ã‚¿ã‚¹ã‚¯ | ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å±ã™ã‚‹ã‚¿ã‚¹ã‚¯ |
| è§£æ±ºã™ã‚‹ | çŸ¥è¦‹/ã‚¿ã‚¹ã‚¯ â†’ åˆ¶ç´„ | åˆ¶ç´„ã‚’è§£æ¶ˆã™ã‚‹æ‰‹æ®µ |
| é–¢é€£ã™ã‚‹ | ä»»æ„ â†’ ä»»æ„ | ãã®ä»–ã®é–¢ä¿‚ |
| æ¬²ã—ãŒã‚‹ | äººç‰©/æ—¥è¨˜ â†’ è³¼å…¥å¸Œæœ› | è³¼å…¥å¸Œæœ›ã¨ã®é–¢é€£ |

# æŠ½å‡ºãƒ«ãƒ¼ãƒ«
1. **åˆ¶ç´„ã®æŠ½å‡ºã‚’é‡è¦–**: æ—¥è¨˜ã‹ã‚‰ã‚¿ã‚¹ã‚¯ã ã‘ã§ãªãã€ãã®ã‚¿ã‚¹ã‚¯ã‚’é˜»å®³ã—ã¦ã„ã‚‹ã€Œé‡åŠ›ã€ã‚’ç©æ¥µçš„ã«è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚
   - æ™‚é–“ãŒãªã„ â†’ åˆ¶ç´„ã€Œæ™‚é–“ä¸è¶³ã€â†’ é˜»å®³ã™ã‚‹ â†’ ã‚¿ã‚¹ã‚¯
   - ç–²ã‚Œã¦ã„ã‚‹ â†’ åˆ¶ç´„ã€Œç–²åŠ´ã€â†’ é˜»å®³ã™ã‚‹ â†’ ã‚¿ã‚¹ã‚¯
   - ã‚„ã‚Šæ–¹ãŒã‚ã‹ã‚‰ãªã„ â†’ åˆ¶ç´„ã€ŒæŠ€è¡“çš„èª²é¡Œã€â†’ é˜»å®³ã™ã‚‹ â†’ ã‚¿ã‚¹ã‚¯
   - ã‚„ã‚‹æ°—ãŒå‡ºãªã„ â†’ åˆ¶ç´„ã€Œæ„Ÿæƒ…çš„ãƒ–ãƒ¬ãƒ¼ã‚­ã€â†’ é˜»å®³ã™ã‚‹ â†’ ã‚¿ã‚¹ã‚¯

2. **æ„Ÿæƒ…ã®åŸå‹•åŠ›ã‚’æŠ½å‡º**: ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ…ã‚„ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚‚ã‚°ãƒ©ãƒ•ã«å…¥ã‚Œã¦ãã ã•ã„ã€‚

3. **æ—¢å­˜ã‚¿ã‚¹ã‚¯ã¨ã®ç…§åˆ**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã€Œæ—¢å­˜ã®ã‚¿ã‚¹ã‚¯ãƒ»ç›®æ¨™ä¸€è¦§ã€ãŒæä¾›ã•ã‚Œã¾ã™ã€‚æ—¥è¨˜ãŒæ—¢å­˜ã‚¿ã‚¹ã‚¯ãƒ»ç›®æ¨™ã®é€²æ—ã«è¨€åŠã—ã¦ã„ã‚‹å ´åˆã€æ–°ã—ã„ãƒãƒ¼ãƒ‰ã‚’ä½œã‚‰ãšæ—¢å­˜IDã‚’å†åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚
   - **é‡è¦**: æ—¥è¨˜ãŒæ—¢å­˜ã®ç›®æ¨™ã‚„ç›®æ¨™ã«ã¤ã„ã¦è§¦ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€å¿…ãšã€Œæ—¥è¨˜ãƒãƒ¼ãƒ‰ â†’ ç›®æ¨™ãƒãƒ¼ãƒ‰ã€ã® `è¨€åŠã™ã‚‹` ã‚¨ãƒƒã‚¸ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ãƒ¢ãƒ³ã‚¹ã‚¿ãƒ¼ãƒ‡ã‚¶ã‚¤ãƒ³ã«ã¤ã„ã¦æ›¸ã„ãŸ â†’ `æ—¥è¨˜:XXXX -[è¨€åŠã™ã‚‹]-> ç›®æ¨™:RealisticDesign`ï¼‰ã€‚ç›´æ¥ã®ãƒãƒ¼ãƒ‰æ›´æ–°ãŒãªãã¦ã‚‚è¨€åŠã‚¨ãƒƒã‚¸ã¯å¿…é ˆã§ã™ã€‚

4. **ã‚¿ã‚°ã®ä»˜ä¸**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ `Task::xxx` ã‚„ `äºˆå®š::xxx` ã®ã‚ˆã†ãªã‚¿ã‚°ã‚’ä½¿ã†å ´åˆã¯ãã®ã¾ã¾è§£æã—ã¦ãã ã•ã„ã€‚

5. **å…¨ã¦æ—¥æœ¬èª**: label ã¨ detail ã¯å¿…ãšæ—¥æœ¬èªã§æ›¸ã„ã¦ãã ã•ã„ã€‚

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®š

å„ãƒãƒ¼ãƒ‰ã« `context` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä»˜ä¸ã—ã¦ãã ã•ã„ã€‚
æ—¥è¨˜ã¯1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«è¤‡æ•°ã®æ–‡è„ˆãŒæ··åœ¨ã™ã‚‹ãŸã‚ã€ãƒãƒ¼ãƒ‰ã®å†…å®¹ã”ã¨ã«é©åˆ‡ãªæ–‡è„ˆã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚

- "knowbe"  : Knowbeã§ã®ä»•äº‹ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã€ãƒãƒ¼ãƒ ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã—ã¦ã®æ´»å‹•ï¼‰ã«é–¢ä¿‚ã™ã‚‹ãƒãƒ¼ãƒ‰
- "saiteki" : Saitekiã§ã®å‰¯æ¥­ï¼ˆAIç ”ç©¶ï¼‰ã«é–¢ä¿‚ã™ã‚‹ãƒãƒ¼ãƒ‰
- "private" : ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆï¼ˆå®¶æ—ã€å¥åº·ã€è¶£å‘³ã€å€‹äººçš„ãªç›®æ¨™ãªã©ï¼‰ã«é–¢ä¿‚ã™ã‚‹ãƒãƒ¼ãƒ‰
- "shared"  : ã©ã®æ–‡è„ˆã«ã‚‚å½“ã¦ã¯ã¾ã‚‰ãªã„ã‚‚ã®ã€ã¾ãŸã¯è¤‡æ•°ã«ã¾ãŸãŒã‚‹ã‚‚ã®

åˆ¤æ–­ã®ãƒ’ãƒ³ãƒˆ:
- ã€Œãƒãƒ¼ãƒ ã€ã€Œãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã€ã€Œãƒªãƒ¼ãƒ€ãƒ¼ã€ã€Œãƒ¡ãƒ³ãƒãƒ¼ã€ãªã©ã¯KnowbeãŒå¤šã„
- ã€ŒAIã€ã€Œç ”ç©¶ã€ã€Œå‰¯æ¥­ã€ã€Œã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã€ãªã©ã¯SaitekiãŒå¤šã„
- ã€Œå¦»ã€ã€Œå­ä¾›ã€ã€Œãƒãƒ¡ãƒ©ã€ã€Œå¥åº·ã€ã€Œä½“é‡ã€ã€Œãƒ–ãƒ­ã‚°ã€ãªã©ã¯privateãŒå¤šã„
- ã€Œã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ã€Œãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ãªã©ä¸¡ç¤¾ã«å…±é€šã™ã‚‹ã‚‚ã®ã¯sharedã«ã™ã‚‹

# ãƒãƒ¼ãƒ‰æ§‹é€ 
{
  "id": "ç¨®åˆ¥:ä¸€æ„ãªåå‰",
  "label": "è¡¨ç¤ºåï¼ˆæ—¥æœ¬èªï¼‰",
  "type": "ã‚¿ã‚¹ã‚¯/åˆ¶ç´„/çŸ¥è¦‹/æ„Ÿæƒ…/äººç‰©/å‡ºæ¥äº‹/ç›®æ¨™/ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/æ¦‚å¿µ/å ´æ‰€/æ—¥è¨˜",
  "detail": "èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰",
  "status": "è©²å½“ã™ã‚‹å ´åˆã®ã¿",
  "sentiment": "æ„Ÿæƒ…ãƒãƒ¼ãƒ‰ã®å ´åˆ -1.0ã€œ1.0ï¼ˆåŸç‚¹å€¤ã¨ã—ã¦ä¿æŒï¼‰",
  "emotion_category": "æ„Ÿæƒ…ãƒãƒ¼ãƒ‰ã®å ´åˆ: å–œã³/é”æˆæ„Ÿ/ä¸å®‰/æ€’ã‚Š/ãã®ä»–",
  "trigger": "æ„Ÿæƒ…ãƒãƒ¼ãƒ‰ã®å ´åˆ: ãã®æ„Ÿæƒ…ãŒç”Ÿã¾ã‚ŒãŸãã£ã‹ã‘ï¼ˆ1æ–‡ï¼‰",
  "date": "è©²å½“ã™ã‚‹å ´åˆ YYYY-MM-DD",
  "category": "å½¹å‰²ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€çˆ¶è¦ªã€å¤« ãªã©ï¼‰",
  "context": "knowbe/saiteki/private/shared",
  "tags": ["ã‚¿ã‚°é…åˆ—"],
  "constraint_type": "åˆ¶ç´„ãƒãƒ¼ãƒ‰ã®å ´åˆ: æ™‚é–“ä¸è¶³/ç–²åŠ´/æŠ€è¡“çš„èª²é¡Œ/æ„Ÿæƒ…çš„ãƒ–ãƒ¬ãƒ¼ã‚­/ç‰©ç†çš„éšœå®³/ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³/ãã®ä»–",
  "cost": "è³¼å…¥å¸Œæœ›ãƒãƒ¼ãƒ‰ã®å ´åˆã®ã¿: é‡‘é¡ã®ç›®å®‰ï¼ˆæ•´æ•°å††ï¼‰ã€ä¸æ˜ãªã‚‰ null",
  "priority": "è³¼å…¥å¸Œæœ›ãƒãƒ¼ãƒ‰ã®å ´åˆã®ã¿: é«˜/ä¸­/ä½"
}

# ã‚¨ãƒƒã‚¸æ§‹é€ 
{
  "source": "ã‚½ãƒ¼ã‚¹ãƒãƒ¼ãƒ‰ID",
  "target": "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒ¼ãƒ‰ID",
  "type": "é–¢ä¿‚åï¼ˆæ—¥æœ¬èª: é˜»å®³ã™ã‚‹/åŸå‹•åŠ›ã«ãªã‚‹/ä¸€éƒ¨ã§ã‚ã‚‹ ç­‰ï¼‰",
  "label": "é–¢ä¿‚ã®çŸ­ã„èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰"
}

# å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
{
  "nodes": [...],
  "edges": [...]
}
"""

ANALYSIS_SYSTEM_PROMPT = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œåˆ†èº«ã€ã¨ã—ã¦æŒ¯ã‚‹èˆã† Antigravity ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€Œã‚¿ã‚¹ã‚¯ã«ã‹ã‹ã£ã¦ã„ã‚‹é‡åŠ›ï¼ˆåˆ¶ç´„ï¼‰ã€ã¨ã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆåŸå‹•åŠ›ï¼‰ã€ã‚’åˆ†æã—ã€
é‡åŠ›ã‚’è»½æ¸›ã™ã‚‹å…·ä½“çš„ãªææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

# åˆ†æã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

1. **é‡åŠ›ãƒãƒƒãƒ—ã®ä½œæˆ**: å„ã‚¿ã‚¹ã‚¯ã«ã©ã‚“ãªåˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰ãŒã‹ã‹ã£ã¦ã„ã‚‹ã‹ã‚’æ•´ç†ã™ã‚‹
2. **ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ç™ºè¦‹**: ã©ã‚“ãªæ„Ÿæƒ…ã‚„çŸ¥è¦‹ãŒã‚¿ã‚¹ã‚¯ã®åŸå‹•åŠ›ã«ãªã‚‹ã‹ã‚’è¦‹ã¤ã‘ã‚‹
3. **é‡åŠ›è»½æ¸›ã®ææ¡ˆ**: åˆ¶ç´„ã‚’è§£æ¶ˆãƒ»è»½æ¸›ã™ã‚‹å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã™ã‚‹
   - ã€Œã‚„ã‚‹ã‹ã‚„ã‚‰ãªã„ã‹ã€äºŒæŠã§ã¯ãªãã€ã€Œã©ã†ã™ã‚Œã°é‡åŠ›ã‚’è»½ãã§ãã‚‹ã‹ã€ã‚’è€ƒãˆã‚‹
   - ä¾‹: ã€Œå‰¯æ¥­ã®é‡åŠ›ãŒå¼·ã™ãã‚‹ã‹ã‚‰ã€æƒé™¤ã®å„ªå…ˆåº¦ã‚’ä¸‹ã’ã¦ã€ä»£ã‚ã‚Šã«5åˆ†ã§çµ‚ã‚ã‚‹ã“ã®ä½œæ¥­ã‚’ã—ã‚ˆã†ã€
   - ä¾‹: ã€Œç²˜ç€å‰¤ã‚’å‰¥ãŒã™æ–¹æ³•ã‚’Geminiã§æ¤œç´¢ã—ã¦è§£æ±ºã™ã‚‹ã€
   - ä¾‹: ã€Œå‰¯æ¥­ãŒä¸€æ®µè½ã™ã‚‹ã¾ã§æƒé™¤ã¯åœŸæ›œã®ã¿ã«è¨­å®šå¤‰æ›´ã—ã‚ˆã†ã€

# å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
å‡ºåŠ›ã¯å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ã€‚

{
  "coach_comment": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒã«å¯„ã‚Šæ·»ã„ã€é‡åŠ›ã¨å‘ãåˆã†çŸ­ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ3-5æ–‡ï¼‰",
  "gravity_map": [
    {
      "task": "ã‚¿ã‚¹ã‚¯å",
      "task_id": "ã‚¿ã‚¹ã‚¯ã®ID",
      "constraints": [
        {
          "name": "åˆ¶ç´„å",
          "type": "æ™‚é–“ä¸è¶³/ç–²åŠ´/æŠ€è¡“çš„èª²é¡Œ/æ„Ÿæƒ…çš„ãƒ–ãƒ¬ãƒ¼ã‚­/ç‰©ç†çš„éšœå®³/ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³",
          "severity": "é«˜/ä¸­/ä½"
        }
      ],
      "energy_sources": [
        {
          "name": "åŸå‹•åŠ›ã®åå‰",
          "type": "æ„Ÿæƒ…/çŸ¥è¦‹/ç›®æ¨™"
        }
      ],
      "net_assessment": "ã“ã®ã‚¿ã‚¹ã‚¯ã®é‡åŠ›ãƒãƒ©ãƒ³ã‚¹ã®ç·åˆè©•ä¾¡ï¼ˆ1æ–‡ï¼‰"
    }
  ],
  "antigravity_actions": [
    {
      "action": "å…·ä½“çš„ãªé‡åŠ›è»½æ¸›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
      "target_task": "å¯¾è±¡ã‚¿ã‚¹ã‚¯",
      "effect": "ã“ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§è»½æ¸›ã•ã‚Œã‚‹é‡åŠ›ã®èª¬æ˜",
      "effort": "5åˆ†/30åˆ†/1æ™‚é–“/åŠæ—¥"
    }
  ],
  "insights": [
    {
      "finding": "æ—¥è¨˜ã‚„ã‚°ãƒ©ãƒ•ã‹ã‚‰è¦‹ã¤ã‘ãŸæ°—ã¥ã",
      "implication": "ãã®æ°—ã¥ããŒæ„å‘³ã™ã‚‹ã“ã¨"
    }
  ],
  "emotion_flow": [
    {
      "emotion": "æ„Ÿæƒ…å",
      "sentiment": -1.0,
      "context": "ãã®æ„Ÿæƒ…ãŒç”Ÿã˜ãŸæ–‡è„ˆ"
    }
  ],
  "upcoming_schedule": [
    {
      "title": "äºˆå®šå",
      "date": "YYYY-MM-DD",
      "time": "HH:MMï¼ˆä¸æ˜ãªã‚‰ nullï¼‰",
      "category": "æœ¬æ¥­/å‰¯æ¥­/å®¶æ—/å€‹äºº"
    }
  ],
  "family_digest": {
    "highlights": [
      {
        "member": "å®¶æ—ãƒ¡ãƒ³ãƒãƒ¼åï¼ˆROLEtoKNOWLEDGEã®å½¹å‰²å®šç¾©ã‚’å‚ç…§ï¼‰",
        "event": "å‡ºæ¥äº‹ã‚„æˆé•·ã®è¨˜éŒ²",
        "emotion": "é–¢é€£ã™ã‚‹æ„Ÿæƒ…"
      }
    ],
    "family_todos": ["å®¶æ—é–¢é€£ã®ã‚„ã‚‹ã¹ãã“ã¨"],
    "shopping_list": [
      {
        "item": "å•†å“åï¼ˆä¾‹: ãŠã‚€ã¤ã€ç‰›ä¹³ã€ã‚·ãƒ¼ãƒ«ã¯ãŒã—æ¶²ï¼‰",
        "category": "é£Ÿæ–™å“/æ—¥ç”¨å“/è‚²å…ç”¨å“/åŒ»è–¬å“/ãã®ä»–",
        "urgency": "æ€¥ã/ä»Šé€±ä¸­/ã„ã¤ã‹",
        "note": "è£œè¶³ãƒ¡ãƒ¢ï¼ˆä»»æ„ã€ä¾‹: Mã‚µã‚¤ã‚ºã€Amazonã§æ³¨æ–‡ï¼‰"
      }
    ]
  },
  "blog_seeds": [
    {
      "title": "çŸ­ç·¨å°èª¬ã®ä»®ã‚¿ã‚¤ãƒˆãƒ«",
      "genre": "æ—¥å¸¸/ä»•äº‹/å­è‚²ã¦/ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼/äººé–“é–¢ä¿‚",
      "tone": "ã»ã£ã“ã‚Š/ã‚·ãƒªã‚¢ã‚¹/ã‚³ãƒŸã‚«ãƒ«/å“²å­¦çš„",
      "story_seed": "ç‰©èªã®ç¨®ã«ãªã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚„æ°—ã¥ã",
      "core_message": "èª­è€…ã«ä¼ãˆãŸã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
      "reader_feeling": "èª­å¾Œã«æ®‹ã—ãŸã„æ„Ÿæƒ…",
      "readiness": "é«˜/ä¸­/ä½"
    }
  ],
  "blog_ideas": [
    {
      "title": "èª­è€…ãŒæ€ã‚ãšã‚¯ãƒªãƒƒã‚¯ã—ãŸããªã‚‹ã€å…·ä½“çš„ã§å¼•ãã®ã‚ã‚‹ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«",
      "theme": "ã“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ãŒæ‰±ã†ä¸­å¿ƒãƒ†ãƒ¼ãƒï¼ˆä¾‹: ãƒãƒ¡ãƒ©æ´»ç”¨è¡“ãƒ»AIæ—¥è¨˜ãƒ»è‚²å…ã¨ä»•äº‹ã®ä¸¡ç«‹ãªã©ï¼‰",
      "hook": "è¨˜äº‹ã®å†’é ­ã§èª­è€…ã‚’å¼•ãè¾¼ã‚€ä¸€æ–‡ï¼ˆæ—¥è¨˜ä¸­ã®ãƒªã‚¢ãƒ«ãªä½“é¨“ã‹ã‚‰æŠ½å‡ºï¼‰",
      "readiness": "é«˜/ä¸­/ä½ï¼ˆæ—¥è¨˜ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æ¿ƒã•ãƒ»å…·ä½“æ€§ã‹ã‚‰åˆ¤æ–­ï¼‰"
    }
  ]
}

# é‡è¦ãªæ³¨æ„äº‹é …
- ã€Œantigravity_actionsã€ã§ã¯ã€æ—¥è¨˜ã®æœ¬æ–‡ã§ã€Œè²·ã£ãŸã€ã€Œæ³¨æ–‡ã—ãŸã€ã€Œå®Œäº†ã—ãŸã€ã€Œã‚„ã£ãŸã€ã€Œæ¸ˆã‚“ã ã€ã€Œå®Ÿè¡Œã—ãŸã€ãªã©å®Œäº†ã‚’ç¤ºã™è¨˜è¿°ãŒã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ææ¡ˆã—ãªã„ã§ãã ã•ã„ã€‚å®Œäº†æ¸ˆã¿ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é™¤å¤–ã—ã€ä»£ã‚ã‚Šã«æ–°ã—ã„é‡åŠ›è»½æ¸›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
  - å‰å›å‡ºåŠ›ã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€æ—¥è¨˜ã§å®Œäº†ãŒç¢ºèªã§ããŸã‚‚ã®ã¯é™¤å¤–ã—ã€ã¾ã å®Ÿè¡Œã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã¯å¼•ãç¶šãææ¡ˆã—ã¦ãã ã•ã„ã€‚
- ã€Œupcoming_scheduleã€ã«ã¯æ—¥è¨˜ã‚„ã‚°ãƒ©ãƒ•ã§è¨€åŠã•ã‚Œã¦ã„ã‚‹ã€Œç¢ºå®šã—ã¦ã„ã‚‹æœªæ¥ã®äºˆå®šã€ã ã‘ã‚’å«ã‚ã¦ãã ã•ã„ã€‚éå»ã®äºˆå®šã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
  - æ—¥è¨˜ä¸­ã«ã€Œäºˆå®š::2026/02/20 18:00-19:00ã€ã®ã‚ˆã†ã« `äºˆå®š::` ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§æ—¥æ™‚ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã“ã‹ã‚‰ date ã¨ time ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
  - time ã¯ã€Œ18:00-19:00ã€ã€Œ18:00ã€ã®ã‚ˆã†ãªå½¢å¼ã§è¨˜è¼‰ã—ã¾ã™ã€‚æ™‚é–“ãŒä¸æ˜ãªå ´åˆã®ã¿ null ã«ã—ã¦ãã ã•ã„ã€‚
  - æ—¥è¨˜æœ¬æ–‡ä¸­ã«ã€Œã€‡æ™‚ã‹ã‚‰ã€ã€Œã€‡ã€‡æ™‚ã€ãªã©ã®æ™‚é–“è¨˜è¿°ãŒã‚ã‚‹å ´åˆã‚‚ã€ãã‚Œã‚’ time ã«å«ã‚ã¦ãã ã•ã„ã€‚
- ã€Œfamily_digest.shopping_listã€ã«ã¯æ—¥è¨˜ã‚„ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€Œè²·ã†å¿…è¦ãŒã‚ã‚‹ã‚‚ã®ã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
  - ã€Œã€‡ã€‡ã‚’è²·ã†ã€ã€Œã€‡ã€‡ãŒåˆ‡ã‚ŒãŸ/ãªããªã£ãŸã€ã€Œã€‡ã€‡ã‚’æ³¨æ–‡ã™ã‚‹ã€ã€Œã€‡ã€‡ãŒå¿…è¦ã€ãªã©ã®è¨˜è¿°ã‹ã‚‰å“ç›®ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
  - urgency ã¯æ–‡è„ˆã‹ã‚‰åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚ã€Œæ€¥ã„ã§ã€ã€Œä»Šæ—¥ä¸­ã«ã€ã¯ã€Œæ€¥ãã€ã€ãã‚Œä»¥å¤–ã¯ã€Œä»Šé€±ä¸­ã€ã‹ã€Œã„ã¤ã‹ã€ã¨ã—ã¾ã™ã€‚
  - ã™ã§ã«ã€Œè²·ã£ãŸã€ã€Œå±Šã„ãŸã€ã€Œæ³¨æ–‡æ¸ˆã¿ã€ãªã©å®Œäº†ã—ã¦ã„ã‚‹å“ç›®ã¯ shopping_list ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚
  - å®¶æ—å…¨å“¡ã«å…±é€šã™ã‚‹æ—¥ç”¨å“ãƒ»é£Ÿæ–™å“ã‚‚å«ã‚ã¦ãã ã•ã„ã€‚æ—¥è¨˜ã«è¨€åŠãŒãªã‘ã‚Œã°ç©ºé…åˆ—ã§æ§‹ã„ã¾ã›ã‚“ã€‚
- ã€Œfamily_digestã€ã«ã¯ ROLEtoKNOWLEDGE ã®å½¹å‰²å®šç¾©ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å®¶æ—ãƒ¡ãƒ³ãƒãƒ¼ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚æ—¥è¨˜ã«å®¶æ—ã®è©±é¡ŒãŒãªã‘ã‚Œã°ç©ºã§æ§‹ã„ã¾ã›ã‚“ã€‚
- ã€Œblog_seedsã€ã«ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ—¥è¨˜ã‹ã‚‰1è©±å®Œçµã®ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³çŸ­ç·¨å°èª¬ã®ç€æƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚æ˜Ÿæ–°ä¸€ã®ã‚·ãƒ§ãƒ¼ãƒˆã‚·ãƒ§ãƒ¼ãƒˆã®ã‚ˆã†ãªã€åŒ¿åçš„ã§å¯“è©±çš„ãªç‰©èªã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä½“é¨“ã‚’ãã®ã¾ã¾æ›¸ãã®ã§ã¯ãªãã€ãƒ†ãƒ¼ãƒã‚„æ„Ÿæƒ…ã‚’æŠ½å‡ºã—ã¦æ¶ç©ºã®ç‰©èªã«ã™ã‚‹å‰æã§ã™ã€‚readiness ãŒã€Œé«˜ã€ãªã‚‚ã®ã¯ã€æ„Ÿæƒ…ã‚„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒååˆ†ã«æ¿ƒãã€ã™ãã«åŸ·ç­†ã§ãã‚‹ã‚‚ã®ã§ã™ã€‚
- ã€Œblog_ideasã€ã«ã¯ã“ã®æ—¥è¨˜ã®å†…å®¹ã‹ã‚‰ãƒ–ãƒ­ã‚°è¨˜äº‹ã¨ã—ã¦æ›¸ã‘ãã†ãªã‚¢ã‚¤ãƒ‡ã‚£ã‚¢ã‚’LLMãŒèƒ½å‹•çš„ã«ææ¡ˆã—ã¦ãã ã•ã„ã€‚
  - ã€Œãƒ–ãƒ­ã‚°ã‚¢ã‚¤ãƒ‡ã‚£ã‚¢::ã€ã¨ã„ã†è¨˜æ³•ãŒãªãã¦ã‚‚ã€æ—¥è¨˜ã®ä½“é¨“ãƒ»æ°—ã¥ããƒ»æ„Ÿæƒ…ãƒ»å‡ºæ¥äº‹ã‹ã‚‰ç©æ¥µçš„ã«2ã€œ3ä»¶ææ¡ˆã—ã¦ãã ã•ã„ã€‚
  - ã€Œæ‰‹æ›¸ãã§æ›¸ãæ‰‹é–“ãŒæ¸›ã£ãŸã€ã€Œå­è‚²ã¦ã¨å‰¯æ¥­ã‚’ä¸¡ç«‹ã™ã‚‹å·¥å¤«ã€ã€ŒAIã«é ¼ã‚“ã§æ„å¤–ã¨ã‚ˆã‹ã£ãŸã“ã¨ã€ãªã©ã€èª­è€…ãŒå…±æ„Ÿã§ãã‚‹å…·ä½“çš„ãªãƒ†ãƒ¼ãƒã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
  - readiness ãŒã€Œé«˜ã€ãªã‚‚ã®ã¯ã€ä»Šã™ãæ›¸ã‘ã‚‹ã»ã©æƒ…å ±ãŒæƒã£ã¦ã„ã‚‹ã‚‚ã®ã§ã™ã€‚
  - ã€Œãƒ–ãƒ­ã‚°ã‚¢ã‚¤ãƒ‡ã‚£ã‚¢::ãƒ†ãƒ¼ãƒã€ã¨ã„ã†è¨˜æ³•ãŒæ—¥è¨˜ã«ã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚‚å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

è¨€èª: æ—¥æœ¬èªã€‚
JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""

RESOLUTION_SYSTEM_PROMPT = """
You are a Data Consistency Expert.
Your task is to identify semantic duplicates between a list of "New Nodes" and "Existing Nodes" in a Knowledge Graph.

### Rules
1. **Strict Semantic Matching**: Only match nodes that refer to the EXACT SAME concept, entity, or event, despite minor wording differences.
   - Example 1: "GitHub Actionsã®åˆ¶ç´„" (New) == "GitHub Actionsã®åˆ¶é™" (Existing) -> MATCH
   - Example 2: "ãƒãƒ¡ãƒ©DM250" (New) == "ãƒãƒ¡ãƒ©" (Existing) -> NO MATCH (Specific vs General) -> UNLESS context implies identity.
   - Example 3: "å¦»" (New) == "ã•ã‚„ã‹" (Existing) -> MATCH (if context establishes this).
   - Example 4: "Monster Design" (New) == "Monster Design Practice" (Existing) -> MATCH

2. **Output Format**: JSON object mapping { "new_node_id": "existing_node_id" }.
   - Only include pairs where a match is found.
   - If no matches, return generic empty JSON `{}`.
   - The key is the ID of the NEW node, the value is the ID of the EXISTING node.

3. Consider node 'type' as a strong hint. Distinct types (e.g., Place vs Person) usually don't match.
"""


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def get_role_definition() -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®å½¹å‰²å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    if os.path.exists(ROLE_DEF_FILE):
        try:
            with open(ROLE_DEF_FILE, "r", encoding="utf-8") as f:
                content = f.read().strip()
            if content:
                return f"\n### ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®å½¹å‰²\n{content}\n"
        except Exception as e:
            print(f"âš ï¸ å½¹å‰²å®šç¾©ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    return ""


def call_gemini_api(prompt: str, model: str = "gemini-3-flash-preview", response_mime_type: str = "text/plain", max_retries: int = 3) -> str:
    if not API_KEY:
        raise ValueError("GOOGLE_API_KEY is not set.")

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
    params = {"key": API_KEY}
    headers = {"Content-Type": "application/json"}

    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"response_mime_type": response_mime_type}
    }

    for attempt in range(max_retries + 1):
        response = requests.post(url, headers=headers, json=data, params=params)

        if response.status_code == 200:
            break
        elif response.status_code == 429 and attempt < max_retries:
            wait_time = 30 * (2 ** attempt)  # 30ç§’, 60ç§’, 120ç§’
            print(f"â³ ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆåˆ°é”ã€‚{wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... ({attempt + 1}/{max_retries})")
            import time
            time.sleep(wait_time)
        else:
            raise Exception(f"API Error: {response.status_code} - {response.text}")

    result = response.json()
    try:
        if "candidates" in result and result["candidates"]:
            return result["candidates"][0]["content"]["parts"][0]["text"]
        else:
            print(f"DEBUG: Empty candidates in response: {result}")
            return "{}"
    except (KeyError, IndexError):
        raise Exception(f"Unexpected API response format: {result}")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ã‚°ãƒ©ãƒ•æŠ½å‡º
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def extract_graph(text: str, context_str: str = "") -> Dict[str, Any]:
    role_def = get_role_definition()
    full_context = f"{context_str}\n{role_def}"

    prompt = f"""
    {EXTRACTION_SYSTEM_PROMPT}

    {full_context}

    ### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ—¥è¨˜
    {text}
    """
    print("ğŸ”„ ã‚°ãƒ©ãƒ•ã‚’æŠ½å‡ºä¸­...")
    json_text = call_gemini_api(prompt, model="gemini-3-flash-preview", response_mime_type="application/json")
    return json.loads(json_text)


def get_master_context(master_graph: Dict[str, Any]) -> str:
    """ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã‹ã‚‰æ–‡è„ˆæƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
    nodes = master_graph.get("nodes", [])

    active_goals = [n for n in nodes if n.get("type") == "ç›®æ¨™" and n.get("status") in ["é€²è¡Œä¸­", "Active"]]
    active_tasks = [n for n in nodes if n.get("type") == "ã‚¿ã‚¹ã‚¯" and n.get("status") not in ["å®Œäº†", "Completed"]]
    constraints = [n for n in nodes if n.get("type") == "åˆ¶ç´„"]
    scheduled_events = [n for n in nodes if n.get("type") == "å‡ºæ¥äº‹" and n.get("status") in ["äºˆå®š", "Scheduled"]]

    context_str = "### ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã®æ–‡è„ˆ\n"

    if active_goals:
        context_str += "**é€²è¡Œä¸­ã®ç›®æ¨™:**\n"
        for g in active_goals:
            context_str += f"- {g.get('label')}: {g.get('detail')}\n"

    if active_tasks:
        context_str += "\n**æ—¢å­˜ã®ã‚¿ã‚¹ã‚¯:**\n"
        for t in active_tasks:
            context_str += f"- [ID: {t.get('id')}] {t.get('label')}: {t.get('detail', '')}\n"

    if constraints:
        context_str += "\n**æ—¢çŸ¥ã®åˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰:**\n"
        for c in constraints:
            context_str += f"- {c.get('label')}: {c.get('detail', '')}\n"

    if scheduled_events:
        context_str += "\n**äºˆå®šã•ã‚Œã¦ã„ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆ:**\n"
        for e in scheduled_events:
            date = e.get("date", "æ—¥ä»˜ä¸æ˜")
            context_str += f"- [{date}] {e.get('label')}: {e.get('detail')}\n"

    if not active_goals and not active_tasks and not constraints and not scheduled_events:
        context_str += "å±¥æ­´ã«ã‚¿ã‚¹ã‚¯ãƒ»ç›®æ¨™ãƒ»åˆ¶ç´„ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚\n"

    return context_str


def resolve_semantic_duplicates(daily_graph: Dict[str, Any], master_graph: Dict[str, Any]) -> Dict[str, Any]:
    """LLMã‚’ä½¿ã£ã¦ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é‡è¤‡ã‚’æ¤œå‡ºãƒ»ãƒãƒ¼ã‚¸ã™ã‚‹ã€‚"""
    print("ğŸ” ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã¨ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")

    daily_nodes = daily_graph.get("nodes", [])
    master_nodes = master_graph.get("nodes", [])

    if not master_nodes or not daily_nodes:
        return daily_graph

    mergeable_types = {'ã‚¿ã‚¹ã‚¯', 'åˆ¶ç´„', 'çŸ¥è¦‹', 'æ„Ÿæƒ…', 'ç›®æ¨™', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'æ¦‚å¿µ', 'äººç‰©', 'å ´æ‰€', 'å‡ºæ¥äº‹'}

    new_candidates = [n for n in daily_nodes if n.get('type') in mergeable_types]
    if not new_candidates:
        return daily_graph

    master_candidates = [n for n in master_nodes if n.get('type') in mergeable_types]
    if not master_candidates:
        return daily_graph

    new_list_str = "\n".join([f"- {n['id']} ({n.get('type')}): {n.get('label')}" for n in new_candidates])
    master_list_str = "\n".join([f"- {n['id']} ({n.get('type')}): {n.get('label')}" for n in master_candidates])

    prompt = f"""
    {RESOLUTION_SYSTEM_PROMPT}

    ### New Nodes (Daily)
    {new_list_str}

    ### Existing Nodes (Master)
    {master_list_str}

    Return JSON mapping.
    """

    try:
        json_text = call_gemini_api(prompt, model="gemini-3-flash-preview", response_mime_type="application/json")
        mapping = json.loads(json_text)

        if not mapping:
            print("âœ… é‡è¤‡ãªã—ã€‚")
            return daily_graph

        print(f"ğŸ”„ {len(mapping)}ä»¶ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é‡è¤‡ã‚’ç™ºè¦‹ã€‚ãƒãƒ¼ã‚¸ä¸­...")
        for new_id, existing_id in mapping.items():
            print(f"   - {new_id} -> {existing_id}")

            for n in daily_graph.get("nodes", []):
                if n['id'] == new_id:
                    n['id'] = existing_id

            for e in daily_graph.get("edges", []):
                if e['source'] == new_id: e['source'] = existing_id
                if e['target'] == new_id: e['target'] = existing_id

        return daily_graph

    except Exception as e:
        print(f"âš ï¸ é‡è¤‡è§£æ±ºã«å¤±æ•—: {e}ã€‚è§£æ±ºãªã—ã§ç¶šè¡Œã—ã¾ã™ã€‚")
        return daily_graph


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥LLMå‘¼ã³å‡ºã—åŸºç›¤
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def build_graph_context(master_graph: Dict[str, Any], category_filter: Optional[str] = None) -> str:
    """ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰ã‚’LLMå‘ã‘ã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚

    Args:
        master_graph: knowledge_graph.jsonld å…¨ä½“
        category_filter: 'knowbe' / 'saiteki' / 'å®¶æ—' / 'å€‹äºº' ãªã©ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚Noneãªã‚‰å…¨ä»¶ã€‚
    """
    nodes = master_graph.get("nodes", [])
    if category_filter:
        nodes = [n for n in nodes if n.get("category") == category_filter or category_filter in (n.get("tags") or [])]

    lines = ["### ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•: ç¾åœ¨ã®çŠ¶æ…‹"]

    # ã‚¿ã‚¹ã‚¯ãƒãƒ¼ãƒ‰ï¼ˆstatusä»˜ãï¼‰
    tasks = [n for n in nodes if n.get("type") == "ã‚¿ã‚¹ã‚¯"]
    if tasks:
        lines.append("\n**ã‚¿ã‚¹ã‚¯ä¸€è¦§ï¼ˆstatusã¯å¿…ãšå‚ç…§ã—ã¦ãã ã•ã„ï¼‰:**")
        for t in tasks:
            status = t.get("status", "é€²è¡Œä¸­")
            detail = t.get("detail", "")
            lines.append(f"- [{status}] {t.get('label', '')}: {detail[:100]}")

    # ç›®æ¨™ãƒãƒ¼ãƒ‰
    goals = [n for n in nodes if n.get("type") == "ç›®æ¨™"]
    if goals:
        lines.append("\n**ç›®æ¨™ä¸€è¦§:**")
        for g in goals:
            status = f"[{g['status']}] " if g.get("status") else ""
            lines.append(f"- {status}{g.get('label', '')}: {g.get('detail', '')[:100]}")

    # æ¬²ã—ã„ã‚‚ã® / è²·ã„ç‰©
    wants = [n for n in nodes if n.get("type") in ["æ¬²ã—ã„ã‚‚ã®", "è²·ã„ç‰©", "è³¼å…¥å¸Œæœ›"]]
    if wants:
        lines.append("\n**æ¬²ã—ã„ã‚‚ã® / è²·ã„ç‰©ï¼ˆstatusä»˜ãï¼‰:**")
        for w in wants:
            status = w.get("status", "æœªè³¼å…¥")
            lines.append(f"- [{status}] {w.get('label', '')}: {w.get('detail', '')[:80]}")

    # åˆ¶ç´„ãƒãƒ¼ãƒ‰ï¼ˆcategory_filteræŒ‡å®šæ™‚ã¯ç‰¹ã«é‡è¦ï¼‰
    constraints = [n for n in nodes if n.get("type") == "åˆ¶ç´„"]
    if constraints:
        lines.append("\n**åˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰:**")
        for c in constraints:
            lines.append(f"- {c.get('label', '')}: {c.get('detail', '')[:100]}")

    return "\n".join(lines)


_DEFAULT_SECTION_MODEL = "gemini-2.0-flash-lite"

def call_section_llm(section_name: str, prompt: str, expect_json: bool = True) -> Any:
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ã®ç‹¬ç«‹ã—ãŸLLMå‘¼ã³å‡ºã—ã€‚JSONé…åˆ—ã¾ãŸã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™ã€‚

    Args:
        section_name: ãƒ­ã‚°è¡¨ç¤ºç”¨ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å
        prompt: å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        expect_json: Trueãªã‚‰application/jsonã§å‘¼ã³å‡ºã™
    Returns:
        ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸPythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆãƒªã‚¹ãƒˆã¾ãŸã¯è¾æ›¸ï¼‰ã€‚å¤±æ•—æ™‚ã¯ç©ºãƒªã‚¹ãƒˆã€‚
    """
    print(f"   ğŸ¤– [{section_name}] LLMå‘¼ã³å‡ºã—ä¸­...")
    try:
        mime = "application/json" if expect_json else "text/plain"
        raw = call_gemini_api(prompt, model=_DEFAULT_SECTION_MODEL, response_mime_type=mime)
        if not expect_json:
            return raw.strip()
        cleaned = raw.strip()
        if cleaned.startswith("```"):
            cleaned = re.sub(r'^```(?:json)?\s*', '', cleaned)
            cleaned = re.sub(r'```\s*$', '', cleaned).strip()
        parsed = json.loads(cleaned)
        print(f"   âœ… [{section_name}] å–å¾—å®Œäº†")
        return parsed
    except Exception as e:
        print(f"   âš ï¸ [{section_name}] LLMå‘¼ã³å‡ºã—å¤±æ•—: {e}")
        return [] if expect_json else ""


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# åˆ†æ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def analyze_updated_state(master_graph: Dict[str, Any], current_diary_node: Dict[str, Any], diary_text: str = "") -> str:
    """æ›´æ–°å¾Œã®ã‚°ãƒ©ãƒ•å…¨ä½“ã‚’åˆ†æã—ã€Antigravityã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""

    # 1. é€²è¡Œä¸­ã®ç›®æ¨™
    active_goals = [n for n in master_graph.get("nodes", []) if n.get("type") == "ç›®æ¨™" and n.get("status") in ["é€²è¡Œä¸­", "Active"]]

    # 2. çŸ¥è¦‹
    recent_insights = sorted(
        [n for n in master_graph.get("nodes", []) if n.get("type") == "çŸ¥è¦‹"],
        key=lambda x: x.get("last_seen", ""), reverse=True
    )[:10]

    # 3. äºˆå®š
    scheduled_events = [n for n in master_graph.get("nodes", []) if n.get("type") == "å‡ºæ¥äº‹" and n.get("status") in ["äºˆå®š", "Scheduled"]]

    # 4. æœªå®Œäº†ã‚¿ã‚¹ã‚¯
    pending_tasks = [n for n in master_graph.get("nodes", []) if n.get("type") == "ã‚¿ã‚¹ã‚¯" and n.get("status") not in ["å®Œäº†", "Completed"]]

    # 5. åˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰
    constraints = [n for n in master_graph.get("nodes", []) if n.get("type") == "åˆ¶ç´„"]

    # 6. æ„Ÿæƒ…
    emotions = [n for n in master_graph.get("nodes", []) if n.get("type") == "æ„Ÿæƒ…"]

    # 7. æœ€è¿‘ã®æ—¥è¨˜
    all_diary_nodes = sorted(
        [n for n in master_graph.get("nodes", []) if n.get("type") == "æ—¥è¨˜"],
        key=lambda x: x.get("date", ""), reverse=True
    )[:5]

    # 8. ã‚¿ã‚¹ã‚¯ã¨åˆ¶ç´„ã®æ¥ç¶šæƒ…å ±
    edges = master_graph.get("edges", [])
    blocking_edges = [e for e in edges if e.get("type") == "é˜»å®³ã™ã‚‹"]
    motivating_edges = [e for e in edges if e.get("type") == "åŸå‹•åŠ›ã«ãªã‚‹"]

    # 9. å‰å›ã®åˆ†æçµæœã‹ã‚‰ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€è²·ã„ç‰©ãƒªã‚¹ãƒˆã‚’å¼•ãç¶™ã
    prev_schedule = []
    prev_actions = []
    prev_shopping_list = []
    for d_node in all_diary_nodes:
        if d_node.get("analysis_content"):
            try:
                raw_ac = d_node["analysis_content"]
                # Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é™¤å»
                cleaned_ac = raw_ac.strip()
                if cleaned_ac.startswith("```"):
                    cleaned_ac = re.sub(r'^```(?:json)?\s*', '', cleaned_ac)
                    cleaned_ac = re.sub(r'```\s*$', '', cleaned_ac).strip()
                prev_analysis = json.loads(cleaned_ac)

                if prev_analysis.get("upcoming_schedule") and not prev_schedule:
                    prev_schedule = prev_analysis["upcoming_schedule"]
                if prev_analysis.get("antigravity_actions") and not prev_actions:
                    prev_actions = prev_analysis["antigravity_actions"]
                if prev_analysis.get("family_digest", {}).get("shopping_list") and not prev_shopping_list:
                    prev_shopping_list = prev_analysis["family_digest"]["shopping_list"]
                if prev_schedule and prev_actions and prev_shopping_list:
                    break
            except (json.JSONDecodeError, TypeError):
                pass

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
    context_summary = "### ç¾åœ¨ã®çŠ¶æ³\n"

    if active_goals:
        context_summary += "**é€²è¡Œä¸­ã®ç›®æ¨™:**\n" + "\n".join([f"- {n.get('label')}: {n.get('detail')}" for n in active_goals]) + "\n"

    if pending_tasks:
        context_summary += "\n**æœªå®Œäº†ã‚¿ã‚¹ã‚¯:**\n"
        for t in pending_tasks:
            # ã“ã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åˆ¶ç´„ã‚’åé›†
            task_constraints = []
            for be in blocking_edges:
                if be.get("target") == t.get("id"):
                    constraint_node = next((n for n in constraints if n["id"] == be.get("source")), None)
                    if constraint_node:
                        task_constraints.append(constraint_node.get("label"))
            constraint_str = f" [é‡åŠ›: {', '.join(task_constraints)}]" if task_constraints else ""
            context_summary += f"- {t.get('label')}{constraint_str}\n"

    if constraints:
        context_summary += "\n**åˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰ä¸€è¦§:**\n" + "\n".join([f"- {n.get('label')} ({n.get('constraint_type', 'ä¸æ˜')}): {n.get('detail')}" for n in constraints]) + "\n"

    if emotions:
        context_summary += "\n**æ„Ÿæƒ…:**\n" + "\n".join([f"- {n.get('label')} (sentiment: {n.get('sentiment', 0)})" for n in emotions]) + "\n"

    if recent_insights:
        context_summary += "\n**æœ€è¿‘ã®çŸ¥è¦‹:**\n" + "\n".join([f"- {n.get('label')}" for n in recent_insights]) + "\n"

    if scheduled_events:
        context_summary += "\n**ä»Šå¾Œã®äºˆå®š:**\n" + "\n".join([f"- {n.get('date')} {n.get('label')}: {n.get('detail', '')}" for n in scheduled_events]) + "\n"

    if prev_schedule:
        context_summary += "\n**å‰å›å‡ºåŠ›ã—ãŸã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ™‚é–“æƒ…å ±ã‚’å¼•ãç¶™ã„ã§ãã ã•ã„ï¼‰:**\n"
        context_summary += json.dumps(prev_schedule, ensure_ascii=False, indent=2) + "\n"

    if prev_actions:
        context_summary += "\n**å‰å›å‡ºåŠ›ã—ãŸé‡åŠ›è»½æ¸›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ—¥è¨˜ã§å®Œäº†ãŒç¢ºèªã§ããŸã‚‚ã®ã¯é™¤å¤–ã—ã€æ–°ã—ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å…¥ã‚Œæ›¿ãˆã¦ãã ã•ã„ï¼‰:**\n"
        context_summary += json.dumps(prev_actions, ensure_ascii=False, indent=2) + "\n"

    if prev_shopping_list:
        context_summary += "\n**å‰å›å‡ºåŠ›ã—ãŸè²·ã„ç‰©ãƒªã‚¹ãƒˆï¼ˆä»Šæ—¥ã®æ—¥è¨˜ã§ã€è²·ã£ãŸã€ã€å±Šã„ãŸã€ã€æ³¨æ–‡æ¸ˆã¿ã€ãªã©ã®å®Œäº†è¡¨ç¾ãŒã‚ã‚‹ã‚‚ã®ã¯å¿…ãšé™¤å¤–ã—ã¦ãã ã•ã„ã€‚å‘¨æœŸçš„ãªæ¶ˆè€—å“ã§ã‚‚ã€ä»Šæ—¥ã®æ—¥è¨˜ã§è³¼å…¥ã—ãŸã¨æ˜ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ï¼‰:**\n"
        context_summary += json.dumps(prev_shopping_list, ensure_ascii=False, indent=2) + "\n"

    # æ—¥è¨˜ã®æµã‚Œ
    recent_diary_context = "\n### æœ€è¿‘ã®æ—¥è¨˜ã®æµã‚Œï¼ˆå®Œäº†åˆ¤å®šã«ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼‰\n"
    recent_diary_context += "â€» æ—¥è¨˜ã®æœ¬æ–‡ã«ã€Œè²·ã£ãŸã€ã€Œæ³¨æ–‡ã—ãŸã€ã€Œå®Œäº†ã—ãŸã€ã€Œã‚„ã£ãŸã€ã€Œæ¸ˆã‚“ã ã€ãªã©ã®è¡¨ç¾ãŒã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€å‰å›ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆã‹ã‚‰å¿…ãšé™¤å¤–ã—ã¦ãã ã•ã„ã€‚\n"
    if not all_diary_nodes:
        recent_diary_context += "æœ€è¿‘ã®æ—¥è¨˜ã‚¨ãƒ³ãƒˆãƒªã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n"
    else:
        for d_node in all_diary_nodes:
            d_date = d_node.get("date", "ä¸æ˜")
            d_id = d_node.get("id")

            mentioned_nodes = []
            for edge in edges:
                if edge.get("source") == d_id and edge.get("type") == "è¨€åŠã™ã‚‹":
                    target_id = edge.get("target")
                    target_node = next((n for n in master_graph.get("nodes", []) if n["id"] == target_id), None)
                    if target_node:
                        mentioned_nodes.append(f"{target_node.get('label')} ({target_node.get('type')})")

            mentions_str = ", ".join(mentioned_nodes) if mentioned_nodes else "ç‰¹å®šã®è¨€åŠãªã—"

            # æ—¥è¨˜æœ¬æ–‡ï¼ˆdetailï¼‰ã‚’å«ã‚ã‚‹ â€” å®Œäº†åˆ¤å®šã®ãŸã‚æœ€é‡è¦
            diary_body = d_node.get("detail", "").strip()
            if diary_body:
                # é•·ã™ãã‚‹å ´åˆã¯å…ˆé ­800æ–‡å­—ã«åˆ¶é™ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¯€ç´„
                if len(diary_body) > 800:
                    diary_body = diary_body[:800] + "â€¦ï¼ˆçœç•¥ï¼‰"
                recent_diary_context += f"\n#### {d_date} ã®æ—¥è¨˜\n**è¨€åŠãƒãƒ¼ãƒ‰:** {mentions_str}\n**æœ¬æ–‡:**\n{diary_body}\n"
            else:
                recent_diary_context += f"- **{d_date}**: {mentions_str}\n"


    # å½¹å‰²å®šç¾©
    role_def = get_role_definition()

    prompt = f"""
    {ANALYSIS_SYSTEM_PROMPT}

    {role_def}

    {context_summary}

    {recent_diary_context}

    ### ä»Šæ—¥ã®æ—¥è¨˜ï¼ˆç”Ÿãƒ†ã‚­ã‚¹ãƒˆï¼‰
    ä»¥ä¸‹ãŒä»Šæ—¥ã®æ—¥è¨˜ã®å…¨æ–‡ã§ã™ã€‚ã€Œãƒ–ãƒ­ã‚°ã‚¢ã‚¤ãƒ‡ã‚£ã‚¢::ã€ã€Œãƒ–ãƒ­ã‚°ã‚´ãƒ¼ãƒ«::ã€ãªã©ã®ã‚¿ã‚°ã¯å¿…ãšã“ã“ã‹ã‚‰æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    ---
    {diary_text}
    ---

    ### ä»Šæ—¥ã®æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªï¼ˆã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰æƒ…å ±ï¼‰
    {json.dumps(current_diary_node, ensure_ascii=False, indent=2)}

    ### æŒ‡ç¤º
    ä¸Šè¨˜ã®ã€Œæœ€è¿‘ã®æ—¥è¨˜ã®æµã‚Œã€ã¨ã€Œåˆ¶ç´„ï¼ˆé‡åŠ›ï¼‰ä¸€è¦§ã€ã‚’å…ƒã«ã€ã‚¿ã‚¹ã‚¯ã®é‡åŠ›ãƒãƒ©ãƒ³ã‚¹ã‚’åˆ†æã—ã€
    é‡åŠ›ã‚’è»½æ¸›ã™ã‚‹å…·ä½“çš„ãªææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
    å˜ã«ã‚¿ã‚¹ã‚¯ã‚’åˆ—æŒ™ã™ã‚‹ã ã‘ã§ãªãã€ã€Œãªãœãã®ã‚¿ã‚¹ã‚¯ãŒé€²ã¾ãªã„ã®ã‹ã€ã€Œã©ã†ã™ã‚Œã°é‡åŠ›ã‚’è»½ãã§ãã‚‹ã‹ã€ã‚’æ·±ãåˆ†æã—ã¦ãã ã•ã„ã€‚
    """
    print("ğŸ”„ Antigravityåˆ†æã‚’å®Ÿè¡Œä¸­...")
    raw = call_gemini_api(prompt, model=_DEFAULT_SECTION_MODEL, response_mime_type="application/json")
    # Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒæ··å…¥ã—ãŸå ´åˆã«å‚™ãˆã¦ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    cleaned = raw.strip()
    if cleaned.startswith("```"):
        cleaned = re.sub(r'^```(?:json)?\s*', '', cleaned)
        cleaned = re.sub(r'```\s*$', '', cleaned).strip()

    # â”€â”€ antigravity_actions ã‚’ç‹¬ç«‹ã—ãŸLLMå‘¼ã³å‡ºã—ã§ä¸Šæ›¸ã â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®statusä»˜ããƒãƒ¼ãƒ‰ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ¸¡ã—ã¦LLMã«å®Œäº†åˆ¤å®šã•ã›ã‚‹
    graph_ctx = build_graph_context(master_graph)

    # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¿”ã—ãŸå‰å›ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚è€ƒã¨ã—ã¦æä¾›
    prev_actions_for_section = []
    try:
        base_parsed = json.loads(cleaned)
        prev_actions_for_section = base_parsed.get("antigravity_actions", [])
    except Exception:
        pass
    # ã•ã‚‰ã«ä»¥å‰ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå‰å›ãƒ«ãƒ¼ãƒ—ã‹ã‚‰ç¶™æ‰¿ã—ãŸã‚‚ã®ï¼‰ã‚‚å‚è€ƒã«ã™ã‚‹
    if prev_actions and not prev_actions_for_section:
        prev_actions_for_section = prev_actions

    # å‰å›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’JSONæ–‡å­—åˆ—åŒ–ï¼ˆå‚è€ƒã¨ã—ã¦æ¸¡ã™ï¼‰
    prev_actions_str = json.dumps(prev_actions_for_section, ensure_ascii=False, indent=2) if prev_actions_for_section else "ãªã—"

    actions_prompt = f"""ã‚ãªãŸã¯ã€Œåé‡åŠ›ã‚³ãƒ¼ãƒã€ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»Šæ—¥ã®æ—¥è¨˜ã¨ã€ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®ç¾åœ¨ã®çŠ¶æ…‹ï¼ˆå„ãƒãƒ¼ãƒ‰ã®statusãŒä»˜ã„ã¦ã„ã¾ã™ï¼‰ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚

{graph_ctx}

### ä»Šæ—¥ã®æ—¥è¨˜
{diary_text[:1500]}

### å‰å›ã®é‡åŠ›è»½æ¸›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå‚è€ƒï¼‰
{prev_actions_str}

ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã€Œé‡åŠ›è»½æ¸›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ã‚’3ã€œ5ä»¶ææ¡ˆã—ã¦ãã ã•ã„ã€‚

ãƒ«ãƒ¼ãƒ«:
1. ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã§ status ãŒã€Œå®Œäº†ã€ã€Œè³¼å…¥æ¸ˆã¿ã€ã€Œæ³¨æ–‡æ¸ˆã¿ã€ã€Œdoneã€ã€Œcompletedã€ã®ãƒãƒ¼ãƒ‰ã«é–¢ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€çµ¶å¯¾ã«ææ¡ˆã—ãªã„ã“ã¨ã€‘
2. å‰å›ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ã®ä¸­ã§ã€æ—¥è¨˜ã¾ãŸã¯æœ€æ–°ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰ã€Œå®Ÿè¡Œæ¸ˆã¿ã€ã€Œå®Œäº†ã€ã¨èª­ã¿å–ã‚Œã‚‹ã‚‚ã®ã¯é™¤å¤–ã™ã‚‹ã“ã¨
3. ä»Šæ—¥ã®æ—¥è¨˜ã«æ›¸ã‹ã‚ŒãŸæ‚©ã¿ã‚„åœæ»æ„Ÿã€é‡åŠ›ã‚’è§£æ¶ˆã™ã‚‹æ–°ã—ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã™ã‚‹ã“ã¨
4. effort ã¯ã€Œ5åˆ†ã€ã€Œ30åˆ†ã€ã€Œ1æ™‚é–“ã€ã®ã„ãšã‚Œã‹ã«ã™ã‚‹ã“ã¨

JSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆãã‚Œä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆç¦æ­¢ï¼‰:
[{{"action": "å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³", "target_task": "å¯¾è±¡ã‚¿ã‚¹ã‚¯å", "effect": "ã“ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§è»½æ¸›ã•ã‚Œã‚‹é‡åŠ›ã®èª¬æ˜", "effort": "5åˆ†"}}]
"""
    new_actions = call_section_llm("antigravity_actions", actions_prompt)
    if isinstance(new_actions, list) and new_actions:
        # ãƒ¡ã‚¤ãƒ³çµæœã®JSONã«antigravity_actionsã‚’ä¸Šæ›¸ã
        try:
            base_obj = json.loads(cleaned)
            base_obj["antigravity_actions"] = new_actions
            cleaned = json.dumps(base_obj, ensure_ascii=False)
            print(f"   âœ… antigravity_actions ã‚’ {len(new_actions)} ä»¶ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥LLMã§æ›´æ–°ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"   âš ï¸ antigravity_actions ä¸Šæ›¸ãã«å¤±æ•—ï¼ˆå…ƒã®çµæœã‚’ç¶­æŒï¼‰: {e}")
    else:
        print("   âš ï¸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥LLMã‹ã‚‰antigravity_actionsãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚å…ƒã®çµæœã‚’ç¶­æŒã—ã¾ã™")

    # â”€â”€ family_digest ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥LLMå‘¼ã³å‡ºã— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    family_graph_ctx = build_graph_context(master_graph, category_filter="å®¶æ—")
    diary_short = diary_text[:1200]

    family_highlights_prompt = f"""ã‚ãªãŸã¯å®¶æ—ã®è¨˜éŒ²ä¿‚ã§ã™ã€‚ä»Šæ—¥ã®æ—¥è¨˜ã‹ã‚‰å®¶æ—ãƒ¡ãƒ³ãƒãƒ¼ã®å‡ºæ¥äº‹ãƒ»æˆé•·ãƒ»æ„Ÿæƒ…ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{family_graph_ctx}

### ä»Šæ—¥ã®æ—¥è¨˜
{diary_short}

æ—¥è¨˜ã«å®¶æ—ã®è©±é¡ŒãŒãªã‘ã‚Œã°ç©ºé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿å‡ºåŠ›ï¼ˆä»–ã®ãƒ†ã‚­ã‚¹ãƒˆç¦æ­¢ï¼‰:
[{{"member": "ãƒ¡ãƒ³ãƒãƒ¼åï¼ˆå¦»ãƒ»é•·å¥³ãªã©ï¼‰", "event": "å‡ºæ¥äº‹", "emotion": "é–¢é€£æ„Ÿæƒ…"}}]
"""
    family_todos_prompt = f"""ä»Šæ—¥ã®æ—¥è¨˜ã¨å®¶æ—ã®ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€Œå®¶æ—å…¨å“¡ã§ã‚„ã‚‹ã¹ãã“ã¨ã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{family_graph_ctx}

### ä»Šæ—¥ã®æ—¥è¨˜
{diary_short}

æ—¥è¨˜ã«å®¶æ—ã®ToDoæƒ…å ±ãŒãªã‘ã‚Œã°ç©ºé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿å‡ºåŠ›:
["å®¶æ—ToDoã®ãƒ†ã‚­ã‚¹ãƒˆ"]
"""
    shopping_prompt = f"""ä»Šæ—¥ã®æ—¥è¨˜ã¨å®¶æ—ã®ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€Œè²·ã„ç‰©ãƒªã‚¹ãƒˆã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{family_graph_ctx}

### ä»Šæ—¥ã®æ—¥è¨˜
{diary_short}

ãƒ«ãƒ¼ãƒ«:
- ã€Œè²·ã£ãŸã€ã€Œå±Šã„ãŸã€ã€Œæ³¨æ–‡æ¸ˆã¿ã€ãªã©å®Œäº†ã—ã¦ã„ã‚‹ã‚‚ã®ã¯å«ã‚ãªã„ã“ã¨
- statusãŒã€Œè³¼å…¥æ¸ˆã¿ã€ã€Œæ³¨æ–‡æ¸ˆã¿ã€ã€Œå®Œäº†ã€ã®ãƒãƒ¼ãƒ‰ã«é–¢ã™ã‚‹å“ç›®ã¯å«ã‚ãªã„ã“ã¨
- æ¶ˆè€—å“ï¼ˆãŠã‚€ã¤ãƒ»ç‰›ä¹³ãªã©ï¼‰ã‚‚å«ã‚ã¦ã‚ˆã„

JSONé…åˆ—ã®ã¿å‡ºåŠ›:
[{{"item": "å•†å“å", "category": "é£Ÿæ–™å“/æ—¥ç”¨å“/è‚²å…ç”¨å“", "urgency": "æ€¥ã/ä»Šé€±ä¸­/ã„ã¤ã‹", "note": "è£œè¶³"}}]
"""
    new_highlights = call_section_llm("family_highlights", family_highlights_prompt)
    new_family_todos = call_section_llm("family_todos", family_todos_prompt)
    new_shopping = call_section_llm("shopping_list", shopping_prompt)

    # â”€â”€ knowbe ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥LLMå‘¼ã³å‡ºã— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    knowbe_graph_ctx = build_graph_context(master_graph, category_filter="knowbe")
    knowbe_constraints_prompt = f"""ã‚ãªãŸã¯Knowbeæ¥­å‹™ã®åˆ†æè€…ã§ã™ã€‚ä»Šæ—¥ã®æ—¥è¨˜ã‹ã‚‰Knowbeã®æ¥­å‹™ã«é–¢ã™ã‚‹ã€Œé‡åŠ›ï¼ˆåˆ¶ç´„ãƒ»éšœå®³ï¼‰ã€ã‚’3ä»¶ä»¥å†…ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{knowbe_graph_ctx}

### ä»Šæ—¥ã®æ—¥è¨˜
{diary_short}

Knowbeã«é–¢ã™ã‚‹è¨˜è¿°ãŒãªã‘ã‚Œã°ç©ºé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿å‡ºåŠ›:
[{{"label": "åˆ¶ç´„å", "detail": "è©³ç´°", "constraint_type": "çµ„ç¹”/æ„Ÿæƒ…/ç’°å¢ƒ/æ™‚é–“"}}]
"""
    knowbe_tasks_prompt = f"""ä»Šæ—¥ã®æ—¥è¨˜ã¨Knowbeã®ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€Knowbeæ¥­å‹™ã®ã€Œé€²è¡Œä¸­ãƒ»æœªå®Œäº†ã‚¿ã‚¹ã‚¯ã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{knowbe_graph_ctx}

### ä»Šæ—¥ã®æ—¥è¨˜
{diary_short}

Knowbeã«é–¢ã™ã‚‹ã‚¿ã‚¹ã‚¯æƒ…å ±ãŒãªã‘ã‚Œã°ç©ºé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿å‡ºåŠ›:
[{{"label": "ã‚¿ã‚¹ã‚¯å", "detail": "è©³ç´°", "status": "é€²è¡Œä¸­"}}]
"""
    knowbe_insights_prompt = f"""ä»Šæ—¥ã®æ—¥è¨˜ã¨Knowbeã®ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€Knowbeæ¥­å‹™ã«é–¢ã™ã‚‹ã€ŒçŸ¥è¦‹ãƒ»å­¦ã³ã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{knowbe_graph_ctx}

### ä»Šæ—¥ã®æ—¥è¨˜
{diary_short}

Knowbeã«é–¢ã™ã‚‹çŸ¥è¦‹ãŒãªã‘ã‚Œã°ç©ºé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿å‡ºåŠ›:
[{{"finding": "æ°—ã¥ã", "implication": "ãã‚ŒãŒæ„å‘³ã™ã‚‹ã“ã¨"}}]
"""
    new_knowbe_constraints = call_section_llm("knowbe_constraints", knowbe_constraints_prompt)
    new_knowbe_tasks = call_section_llm("knowbe_tasks", knowbe_tasks_prompt)
    new_knowbe_insights = call_section_llm("knowbe_insights", knowbe_insights_prompt)

    # â”€â”€ saiteki ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥LLMå‘¼ã³å‡ºã— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    saiteki_graph_ctx = build_graph_context(master_graph, category_filter="saiteki")
    saiteki_constraints_prompt = f"""ã‚ãªãŸã¯Saitekiæ¥­å‹™ã®åˆ†æè€…ã§ã™ã€‚ä»Šæ—¥ã®æ—¥è¨˜ã‹ã‚‰Saitekiã®æ¥­å‹™ã«é–¢ã™ã‚‹ã€Œé‡åŠ›ï¼ˆåˆ¶ç´„ãƒ»éšœå®³ï¼‰ã€ã‚’3ä»¶ä»¥å†…ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{saiteki_graph_ctx}

### ä»Šæ—¥ã®æ—¥è¨˜
{diary_short}

Saitekiã«é–¢ã™ã‚‹è¨˜è¿°ãŒãªã‘ã‚Œã°ç©ºé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿å‡ºåŠ›:
[{{"label": "åˆ¶ç´„å", "detail": "è©³ç´°", "constraint_type": "çµ„ç¹”/æ„Ÿæƒ…/ç’°å¢ƒ/æ™‚é–“"}}]
"""
    saiteki_tasks_prompt = f"""ä»Šæ—¥ã®æ—¥è¨˜ã¨Saitekiã®ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€Saitekiæ¥­å‹™ã®ã€Œé€²è¡Œä¸­ãƒ»æœªå®Œäº†ã‚¿ã‚¹ã‚¯ã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{saiteki_graph_ctx}

### ä»Šæ—¥ã®æ—¥è¨˜
{diary_short}

Saitekiã«é–¢ã™ã‚‹ã‚¿ã‚¹ã‚¯æƒ…å ±ãŒãªã‘ã‚Œã°ç©ºé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿å‡ºåŠ›:
[{{"label": "ã‚¿ã‚¹ã‚¯å", "detail": "è©³ç´°", "status": "é€²è¡Œä¸­"}}]
"""
    saiteki_insights_prompt = f"""ä»Šæ—¥ã®æ—¥è¨˜ã¨Saitekiã®ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€Saitekiæ¥­å‹™ã«é–¢ã™ã‚‹ã€ŒçŸ¥è¦‹ãƒ»å­¦ã³ã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{saiteki_graph_ctx}

### ä»Šæ—¥ã®æ—¥è¨˜
{diary_short}

Saitekiã«é–¢ã™ã‚‹çŸ¥è¦‹ãŒãªã‘ã‚Œã°ç©ºé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿å‡ºåŠ›:
[{{"finding": "æ°—ã¥ã", "implication": "ãã‚ŒãŒæ„å‘³ã™ã‚‹ã“ã¨"}}]
"""
    new_saiteki_constraints = call_section_llm("saiteki_constraints", saiteki_constraints_prompt)
    new_saiteki_tasks = call_section_llm("saiteki_tasks", saiteki_tasks_prompt)
    new_saiteki_insights = call_section_llm("saiteki_insights", saiteki_insights_prompt)

    # â”€â”€ å…¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’JSONã«çµ±åˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        base_obj = json.loads(cleaned)

        # family_digest ã‚’ä¸Šæ›¸ã
        base_obj["family_digest"] = {
            "highlights": new_highlights if isinstance(new_highlights, list) else [],
            "family_todos": new_family_todos if isinstance(new_family_todos, list) else [],
            "shopping_list": new_shopping if isinstance(new_shopping, list) else [],
        }

        # knowbe ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        base_obj["knowbe"] = {
            "constraints": new_knowbe_constraints if isinstance(new_knowbe_constraints, list) else [],
            "tasks": new_knowbe_tasks if isinstance(new_knowbe_tasks, list) else [],
            "insights": new_knowbe_insights if isinstance(new_knowbe_insights, list) else [],
        }

        # saiteki ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        base_obj["saiteki"] = {
            "constraints": new_saiteki_constraints if isinstance(new_saiteki_constraints, list) else [],
            "tasks": new_saiteki_tasks if isinstance(new_saiteki_tasks, list) else [],
            "insights": new_saiteki_insights if isinstance(new_saiteki_insights, list) else [],
        }

        cleaned = json.dumps(base_obj, ensure_ascii=False)
        print("   âœ… family/knowbe/saiteki ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥LLMçµæœã‚’çµ±åˆã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"   âš ï¸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆã«å¤±æ•—ï¼ˆå…ƒã®çµæœã‚’ç¶­æŒï¼‰: {e}")

    return cleaned





# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# HTMLå¯è¦–åŒ–ã®æ›´æ–°
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def _validate_graph_data(graph_data: Dict[str, Any]) -> None:
    """GRAPH_DATA ã®æ•´åˆæ€§ã‚’æ¤œè¨¼ã™ã‚‹ã€‚å¤±æ•—ã—ãŸå ´åˆã¯ RuntimeError ã‚’é€å‡ºã€‚"""
    if not isinstance(graph_data, dict):
        raise RuntimeError("GRAPH_DATA ãŒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    if "nodes" not in graph_data or not isinstance(graph_data["nodes"], list):
        raise RuntimeError("GRAPH_DATA.nodes ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ãƒªã‚¹ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    if "edges" not in graph_data or not isinstance(graph_data["edges"], list):
        raise RuntimeError("GRAPH_DATA.edges ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ãƒªã‚¹ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    # JSON ã¨ã—ã¦ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ»ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã§ãã‚‹ã‹ç¢ºèª
    try:
        roundtripped = json.loads(json.dumps(graph_data, ensure_ascii=False))
        assert len(roundtripped["nodes"]) == len(graph_data["nodes"])
    except Exception as e:
        raise RuntimeError(f"GRAPH_DATA ã® JSON ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºæ¤œè¨¼ã«å¤±æ•—: {e}")


def update_html_visualization(html_path: str, graph_data: Dict[str, Any]):
    """graph_data.js ã® GRAPH_DATA ã‚’æ›´æ–°ã™ã‚‹ã€‚

    index.html æœ¬ä½“ã§ã¯ãªãã€åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® graph_data.js ã‚’æ›¸ãæ›ãˆã‚‹ã“ã¨ã§
    index.html ãŒç ´æã™ã‚‹ãƒªã‚¹ã‚¯ã‚’æ ¹æœ¬çš„ã«æ’é™¤ã™ã‚‹ã€‚
    """
    import os
    js_path = os.path.join(os.path.dirname(os.path.abspath(html_path)), "graph_data.js")
    try:
        # â”€â”€â”€ æ›¸ãè¾¼ã¿å‰ã«æ•´åˆæ€§ã‚’æ¤œè¨¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        _validate_graph_data(graph_data)

        # graph_data.js ã®å†…å®¹ã‚’ç”Ÿæˆ
        new_content = (
            "// GRAPH_DATA_START\n"
            f"const GRAPH_DATA = {json.dumps(graph_data, ensure_ascii=False, indent=2)};\n"
            "// GRAPH_DATA_END\n"
        )

        with open(js_path, "w", encoding="utf-8") as f:
            f.write(new_content)

        # â”€â”€â”€ æ›¸ãè¾¼ã¿å¾Œã«å†èª­ã¿è¾¼ã¿ã—ã¦æ¤œè¨¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with open(js_path, "r", encoding="utf-8") as f:
            written = f.read()
        # const GRAPH_DATA = ... ; ã® JSON éƒ¨åˆ†ã‚’æŠ½å‡ºã—ã¦æ¤œè¨¼
        import re
        m = re.search(r"const GRAPH_DATA = (\{.*\});", written, re.DOTALL)
        if not m:
            raise RuntimeError("æ›¸ãè¾¼ã¿å¾Œã® graph_data.js ã‹ã‚‰ GRAPH_DATA ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“")
        json.loads(m.group(1))  # ãƒ‘ãƒ¼ã‚¹ã§ãã‚‹ã‹ç¢ºèª

        node_count = len(graph_data["nodes"])
        edge_count = len(graph_data["edges"])
        print(f"âœ… graph_data.js ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {node_count} nodes, {edge_count} edges")

    except RuntimeError as e:
        print(f"âŒ GRAPH_DATA æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        raise
    except Exception as e:
        print(f"âŒ graph_data.js æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def main():
    parser = argparse.ArgumentParser(description="Pomera Diary â†’ Antigravity Knowledge Graph")
    parser.add_argument("input_file", help="æ—¥è¨˜ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--output_graph", default="daily_graph.json", help="æ—¥æ¬¡ã‚°ãƒ©ãƒ•JSONã®å‡ºåŠ›å…ˆ")
    parser.add_argument("--master_graph", default="knowledge_graph.jsonld", help="ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã®ãƒ‘ã‚¹")
    parser.add_argument("--output_report", default="daily_report.md", help="åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›å…ˆ")

    args = parser.parse_args()

    # 1. æ—¥è¨˜ã®èª­ã¿è¾¼ã¿
    try:
        import unicodedata
        args.input_file = unicodedata.normalize('NFC', args.input_file)
        with open(args.input_file, "r", encoding="utf-8") as f:
            diary_text = f.read()
    except FileNotFoundError:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.input_file}")
        return

    # 2. ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã®èª­ã¿è¾¼ã¿
    print(f"ğŸ“‚ ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã¿ä¸­: {args.master_graph}")
    try:
        master_graph = graph_merger.load_graph(args.master_graph)
    except Exception as e:
        print(f"âš ï¸ ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã€æ–°è¦ä½œæˆ: {e}")
        master_graph = {
            "nodes": [],
            "edges": [],
            "metadata": {
                "schema_version": "2.0-antigravity",
                "description": "ã‚¿ã‚¹ã‚¯ã®é‡åŠ›ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ãçŸ¥è­˜ã‚°ãƒ©ãƒ•"
            }
        }
    master_context_str = get_master_context(master_graph)

    # 3. æ—¥æ¬¡ã‚°ãƒ©ãƒ•ã®æŠ½å‡º
    try:
        daily_graph = extract_graph(diary_text, master_context_str)

        # æ—¥ä»˜ã®æŠ½å‡º
        import re
        match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', args.input_file)
        if match:
            y, m, d = match.groups()
            current_date_str = f"{y}-{int(m):02d}-{int(d):02d}"
            print(f"ğŸ“… ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º: {current_date_str}")
        else:
            current_date_str = datetime.now().strftime("%Y-%m-%d")
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºã§ããšã€ä»Šæ—¥ã®æ—¥ä»˜ã‚’ä½¿ç”¨: {current_date_str}")

        daily_graph["metadata"] = {
            "generated_at": datetime.now().isoformat(),
            "source_file": args.input_file,
            "node_count": len(daily_graph.get("nodes", [])),
            "edge_count": len(daily_graph.get("edges", []))
        }

        # æ—¥è¨˜ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
        diary_node_id = f"æ—¥è¨˜:{current_date_str}"
        if not any(node.get("id") == diary_node_id for node in daily_graph.get("nodes", [])):
            daily_graph.get("nodes", []).append({
                "id": diary_node_id,
                "label": f"{current_date_str}ã®æ—¥è¨˜",
                "type": "æ—¥è¨˜",
                "date": current_date_str,
                "detail": "ä»Šæ—¥ã®æ—¥è¨˜ã‚¨ãƒ³ãƒˆãƒª",
                "first_seen": datetime.now().isoformat(),
                "last_seen": datetime.now().isoformat(),
                "weight": 1
            })

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
        user_node_id = "äººç‰©:è‡ªåˆ†"
        if not any(node.get("id") == user_node_id for node in daily_graph.get("nodes", [])):
            daily_graph.get("nodes", []).append({
                "id": user_node_id,
                "label": "è‡ªåˆ†",
                "type": "äººç‰©",
                "detail": "æ—¥è¨˜ã®ä½œæˆè€…",
                "first_seen": datetime.now().isoformat(),
                "last_seen": datetime.now().isoformat(),
                "weight": 1
            })

        # æ¥ç¶šã®ç¢ºä¿: è‡ªåˆ† â†’ æ—¥è¨˜
        daily_graph.get("edges", []).append({
            "source": user_node_id,
            "target": diary_node_id,
            "type": "é–¢é€£ã™ã‚‹",
            "label": "æ›¸ã„ãŸ",
            "weight": 1
        })

        # æ¥ç¶šã®ç¢ºä¿: æ—¥è¨˜ â†’ å„ãƒãƒ¼ãƒ‰ï¼ˆå­¤ç«‹ã‚’é˜²ãï¼‰
        for node in daily_graph.get("nodes", []):
            nid = node.get("id")
            if nid == user_node_id or nid == diary_node_id:
                continue

            edge_exists = any(
                (e.get("source") == diary_node_id and e.get("target") == nid) or
                (e.get("source") == nid and e.get("target") == diary_node_id)
                for e in daily_graph.get("edges", [])
            )

            if not edge_exists:
                daily_graph.get("edges", []).append({
                    "source": diary_node_id,
                    "target": nid,
                    "type": "è¨€åŠã™ã‚‹",
                    "label": "è¨€åŠ",
                    "weight": 1
                })

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é‡è¤‡ã®è§£æ±º
        daily_graph = resolve_semantic_duplicates(daily_graph, master_graph)

        # æ—¥æ¬¡ã‚°ãƒ©ãƒ•ã®ä¿å­˜
        with open(args.output_graph, "w", encoding="utf-8") as f:
            json.dump(daily_graph, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"âŒ æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # 4. ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã¸ã®ãƒãƒ¼ã‚¸
    print("ğŸ”„ ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã¸ãƒãƒ¼ã‚¸ä¸­...")
    updated_master = None
    try:
        with open(args.output_graph, "r", encoding="utf-8") as f:
            daily_graph_for_merge = json.load(f)

        updated_master = graph_merger.merge_graphs(master_graph, daily_graph_for_merge)

        with open(args.master_graph, "w", encoding="utf-8") as f:
            json.dump(updated_master, f, ensure_ascii=False, indent=2)
        print(f"âœ… ãƒã‚¹ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {args.master_graph}")

    except Exception as e:
        print(f"âŒ ãƒãƒ¼ã‚¸ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        updated_master = master_graph

    # 5. Antigravityåˆ†æ
    try:
        current_diary_node = next((n for n in updated_master.get("nodes", []) if n["id"] == diary_node_id), None)

        if current_diary_node:
            analysis_text = analyze_updated_state(updated_master, current_diary_node, diary_text)

            with open(args.output_report, "w", encoding="utf-8") as f:
                f.write(f"# Antigravityåˆ†æãƒ¬ãƒãƒ¼ãƒˆ ({datetime.now().date()})\n\n")
                f.write(f"**åˆ†æå¯¾è±¡:** {current_date_str} ã®æ—¥è¨˜\n\n")
                f.write(analysis_text)
            print(f"âœ… åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {args.output_report}")

            current_diary_node["analysis_content"] = analysis_text

            with open(args.master_graph, "w", encoding="utf-8") as f:
                json.dump(updated_master, f, ensure_ascii=False, indent=2)
            print(f"âœ… ã‚°ãƒ©ãƒ•ã® {diary_node_id} ã«åˆ†æçµæœã‚’çµ±åˆã—ã¾ã—ãŸ")

        else:
            print("âš ï¸ æ—¥è¨˜ãƒãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    except Exception as e:
        print(f"âŒ åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    # 6. HTMLå¯è¦–åŒ–ã®æ›´æ–°
    try:
        html_path = "index.html"
        if os.path.exists(html_path):
            update_html_visualization(html_path, updated_master)
        else:
            print(f"âš ï¸ {html_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å¯è¦–åŒ–ã®æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    except Exception as e:
        print(f"âŒ å¯è¦–åŒ–æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()
